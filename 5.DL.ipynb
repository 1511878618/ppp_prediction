{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "from pytorch_lightning import LightningModule, LightningDataModule\n",
    "import torch\n",
    "from pytorch_lightning import trainer, LightningModule\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torchmetrics\n",
    "import timm\n",
    "\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    SequentialSampler,\n",
    "    RandomSampler,\n",
    "    WeightedRandomSampler,\n",
    "    Dataset,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TableDataset(Dataset):\n",
    "    def __init__(self, df, features: list, label: list, num_classes=2, y_type=\"bt\"):\n",
    "        super(Dataset, self).__init__()\n",
    "        assert isinstance(df, pd.DataFrame)\n",
    "        assert isinstance(features, list)\n",
    "        assert isinstance(label, list)\n",
    "\n",
    "        for feature in features + label:\n",
    "            assert feature in df.columns\n",
    "\n",
    "        self.df = df.dropna(subset=features + label)\n",
    "        assert len(self.df) > 0\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.num_classes = num_classes\n",
    "        self.y_type = y_type\n",
    "        self._init_dataset()\n",
    "\n",
    "    def _init_dataset(self):\n",
    "        X = torch.tensor(self.df[self.features].values).float()\n",
    "\n",
    "        y = torch.tensor(self.df[self.label].values)\n",
    "        if (self.num_classes != len(self.label)) and self.y_type == \"bt\":\n",
    "            y = F.one_hot(\n",
    "                torch.tensor(y).long(), num_classes=self.num_classes\n",
    "            ).squeeze()\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class DatasetModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train,\n",
    "        test,\n",
    "        batch_size=32,\n",
    "        features: list = None,\n",
    "        label: list = None,\n",
    "        num_classes=2,\n",
    "        y_type=\"bt\",\n",
    "        num_workers=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.num_classes = num_classes\n",
    "        self.y_type = y_type\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self._init_dataset(train, test)\n",
    "\n",
    "    def _init_dataset(self, train, test):\n",
    "        train, val = train_test_split(train, test_size=0.2)\n",
    "        print(\n",
    "            f\"Train : {train[self.label].value_counts()}\\nval : {val[self.label].value_counts()}\\nTest : {test[self.label].value_counts()}\"\n",
    "        )\n",
    "        if self.y_type == \"bt\" and len(self.label) == 1:\n",
    "\n",
    "            class_weights = dict(\n",
    "                enumerate(\n",
    "                    class_weight.compute_class_weight(\n",
    "                        \"balanced\",\n",
    "                        classes=np.arange(self.num_classes),\n",
    "                        y=train[self.label[0]],\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            self.class_weights = class_weights\n",
    "\n",
    "        self.train = TableDataset(\n",
    "            train, self.features, self.label, self.num_classes, self.y_type\n",
    "        )\n",
    "        self.validation = TableDataset(\n",
    "            val, self.features, self.label, self.num_classes, self.y_type\n",
    "        )\n",
    "        self.test = TableDataset(\n",
    "            test, self.features, self.label, self.num_classes, self.y_type\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        if self.y_type == \"bt\":\n",
    "            train_class_weights = [\n",
    "                self.class_weights[torch.argmax(i).item()] for i in self.train.y\n",
    "            ]\n",
    "            sampler = WeightedRandomSampler(\n",
    "                train_class_weights, len(train_class_weights), replacement=True\n",
    "            )\n",
    "        else:\n",
    "            sampler = RandomSampler(self.train)\n",
    "\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler,\n",
    "            drop_last=True,\n",
    "            persistent_workers=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.validation,\n",
    "            batch_size=self.batch_size,\n",
    "            persistent_workers=True,\n",
    "            num_workers=self.num_workers,\n",
    "            sampler=SequentialSampler(self.validation),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            batch_size=self.batch_size,\n",
    "            persistent_workers=True,\n",
    "            num_workers=self.num_workers,\n",
    "            sampler=SequentialSampler(self.test),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed = pd.read_pickle(\"result/part1/train_imputed.pkl\")\n",
    "test_imputed = pd.read_pickle(\"result/part1/test_imputed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C3',\n",
       " 'KLK7',\n",
       " 'GCHFR',\n",
       " 'NHLRC3',\n",
       " 'APOD',\n",
       " 'GAPDH',\n",
       " 'TP53I3',\n",
       " 'CPA4',\n",
       " 'ANXA2',\n",
       " 'GRSF1',\n",
       " 'IL25',\n",
       " 'HMMR',\n",
       " 'MRPL52',\n",
       " 'PAIP2B',\n",
       " 'THAP12',\n",
       " 'FOS',\n",
       " 'FGF9',\n",
       " 'PITHD1',\n",
       " 'THSD1',\n",
       " 'PTGES2',\n",
       " 'DEFB103A_DEFB103B',\n",
       " 'ATP1B4',\n",
       " 'CYB5A',\n",
       " 'UNC79',\n",
       " 'SLC34A3',\n",
       " 'TAGLN3',\n",
       " 'SLIRP',\n",
       " 'CLASP1',\n",
       " 'PSMC3',\n",
       " 'KIR3DL2',\n",
       " 'BEX3',\n",
       " 'PFDN4',\n",
       " 'BCL7A',\n",
       " 'SMC3',\n",
       " 'SLC28A1',\n",
       " 'CDC123',\n",
       " 'GJA8',\n",
       " 'NMRK2',\n",
       " 'GATA3',\n",
       " 'CPLX2',\n",
       " 'RASGRF1',\n",
       " 'FGF7',\n",
       " 'ANKRA2',\n",
       " 'RBM25',\n",
       " 'LYZL2',\n",
       " 'CDK1',\n",
       " 'CREB3',\n",
       " 'CREBZF',\n",
       " 'IGLON5',\n",
       " 'SHC1',\n",
       " 'ZP4',\n",
       " 'TMOD4',\n",
       " 'CEP152',\n",
       " 'MYH7B',\n",
       " 'CEP350',\n",
       " 'CDC25A',\n",
       " 'TRIM26',\n",
       " 'MANEAL',\n",
       " 'MUCL3',\n",
       " 'GIMAP8',\n",
       " 'CYTH3',\n",
       " 'PDXDC1',\n",
       " 'CLINT1',\n",
       " 'MAPRE3',\n",
       " 'EVI2B',\n",
       " 'STAU1',\n",
       " 'PCNA',\n",
       " 'DNAJA1',\n",
       " 'JMJD1C',\n",
       " 'GAGE2A',\n",
       " 'GAD1',\n",
       " 'IZUMO1',\n",
       " 'PDCL2',\n",
       " 'PDE1C',\n",
       " 'STOML2',\n",
       " 'BSND',\n",
       " 'MAPK13',\n",
       " 'PDIA2',\n",
       " 'BTLA',\n",
       " 'MLLT1',\n",
       " 'TPRKB',\n",
       " 'ARHGAP5',\n",
       " 'BTNL10',\n",
       " 'PHLDB2',\n",
       " 'PDIA5',\n",
       " 'ATF4',\n",
       " 'PRAME',\n",
       " 'TOP1MT',\n",
       " 'KHDC3L',\n",
       " 'DCUN1D2',\n",
       " 'IL3',\n",
       " 'DCLRE1C',\n",
       " 'ERCC1',\n",
       " 'DCDC2C',\n",
       " 'VCPKMT',\n",
       " 'SPRING1',\n",
       " 'MORN4',\n",
       " 'ESPL1',\n",
       " 'H2AP',\n",
       " 'MORF4L2',\n",
       " 'SSH3',\n",
       " 'VWA5A',\n",
       " 'PBK',\n",
       " 'REST',\n",
       " 'SHD',\n",
       " 'TXNL1',\n",
       " 'TPM3',\n",
       " 'NEB',\n",
       " 'ATP1B2',\n",
       " 'CEP112',\n",
       " 'SART1',\n",
       " 'ATP6V1G2',\n",
       " 'ATP2B4',\n",
       " 'SAT1',\n",
       " 'ATP1B1',\n",
       " 'NECAP2',\n",
       " 'ATP5F1D',\n",
       " 'ATP1B3',\n",
       " 'ARNTL',\n",
       " 'ARL2BP',\n",
       " 'SCGB2A2',\n",
       " 'GAMT',\n",
       " 'ASS1',\n",
       " 'NFYA',\n",
       " 'GASK1A',\n",
       " 'MANSC4',\n",
       " 'HMGCS1',\n",
       " 'MMUT',\n",
       " 'CBX2',\n",
       " 'BRD3',\n",
       " 'BRDT',\n",
       " 'MAP1LC3B2',\n",
       " 'CASQ2',\n",
       " 'HIP1',\n",
       " 'GSTM4',\n",
       " 'GUK1',\n",
       " 'CALY',\n",
       " 'C1GALT1C1',\n",
       " 'TEF',\n",
       " 'CACNA1H',\n",
       " 'HADH',\n",
       " 'MEGF11',\n",
       " 'MED21',\n",
       " 'THRAP3',\n",
       " 'SPINK8',\n",
       " 'NAA10',\n",
       " 'MRPL24',\n",
       " 'GBP6',\n",
       " 'MYOM2',\n",
       " 'B3GAT3',\n",
       " 'GCLM',\n",
       " 'MYL1',\n",
       " 'HSD17B3',\n",
       " 'MYH4',\n",
       " 'TMED4',\n",
       " 'TMED10',\n",
       " 'SKIV2L',\n",
       " 'SLC12A2',\n",
       " 'SLC51B',\n",
       " 'MTR',\n",
       " 'CD2',\n",
       " 'BHMT2',\n",
       " 'SNU13',\n",
       " 'GP1BB',\n",
       " 'ARL13B',\n",
       " 'HCG22',\n",
       " 'RYR1',\n",
       " 'FDX2',\n",
       " 'ADRA2A',\n",
       " 'ERVV-1',\n",
       " 'EXOSC10',\n",
       " 'EXTL1',\n",
       " 'CYP24A1',\n",
       " 'KIF1C',\n",
       " 'USP47',\n",
       " 'PRKD2',\n",
       " 'PROCR',\n",
       " 'PACS2',\n",
       " 'KIF22',\n",
       " 'NXPE4',\n",
       " 'RTKN2',\n",
       " 'CSRP3',\n",
       " 'NUDT15',\n",
       " 'UHRF2',\n",
       " 'UGDH',\n",
       " 'CSF2',\n",
       " 'KRT17',\n",
       " 'FDX1',\n",
       " 'PYY',\n",
       " 'UBQLN3',\n",
       " 'CSDE1',\n",
       " 'DDA1',\n",
       " 'PALM3',\n",
       " 'VSIG10L',\n",
       " 'PKD2',\n",
       " 'ABCA2',\n",
       " 'EDEM2',\n",
       " 'ABRAXAS2',\n",
       " 'ECI2',\n",
       " 'PGLYRP4',\n",
       " 'PDZD2',\n",
       " 'EIF2AK3',\n",
       " 'EIF5',\n",
       " 'ELOB',\n",
       " 'ITPA',\n",
       " 'ACSL1',\n",
       " 'DENND2B',\n",
       " 'ZCCHC8',\n",
       " 'ACTN2',\n",
       " 'PDE4D',\n",
       " 'ACY3',\n",
       " 'ENOX2',\n",
       " 'YOD1',\n",
       " 'ENPEP',\n",
       " 'PMCH',\n",
       " 'PMM2',\n",
       " 'DHODH',\n",
       " 'KRT6C',\n",
       " 'NUP50',\n",
       " 'LAMA1',\n",
       " 'COPB2',\n",
       " 'LRCH4',\n",
       " 'TSNAX',\n",
       " 'LPP',\n",
       " 'TRPV3',\n",
       " 'IGHMBP2',\n",
       " 'LILRA4',\n",
       " 'FHIP2A',\n",
       " 'NOP56',\n",
       " 'RIPK4',\n",
       " 'TRAF3IP2',\n",
       " 'IGF2BP3',\n",
       " 'NFKB1',\n",
       " 'NFX1',\n",
       " 'REXO2',\n",
       " 'TSPAN15',\n",
       " 'RBM19',\n",
       " 'FRMD4B',\n",
       " 'NOS2',\n",
       " 'TPR',\n",
       " 'NPR1',\n",
       " 'RAB33A',\n",
       " 'RAB39B',\n",
       " 'RPS10',\n",
       " 'ANK2',\n",
       " 'IFNW1',\n",
       " 'CPTP',\n",
       " 'TTN',\n",
       " 'IL36G',\n",
       " 'IL31RA',\n",
       " 'RNASE4',\n",
       " 'LRIG3',\n",
       " 'CACNA1C',\n",
       " 'SCIN',\n",
       " 'DNLZ',\n",
       " 'STEAP4',\n",
       " 'CBLN1',\n",
       " 'CHP1',\n",
       " 'SAG',\n",
       " 'DOCK9',\n",
       " 'RRP15',\n",
       " 'SYNGAP1',\n",
       " 'CNTF',\n",
       " 'ECSCR',\n",
       " 'ELAVL4',\n",
       " 'FZD8',\n",
       " 'SCN2A',\n",
       " 'CNGB3',\n",
       " 'GABRA4',\n",
       " 'CACNB1',\n",
       " 'DEFB118',\n",
       " 'PNMA2',\n",
       " 'SMS',\n",
       " 'CDH4',\n",
       " 'SH3BGRL2',\n",
       " 'RAB3GAP1',\n",
       " 'RANBP2',\n",
       " 'MYOM1',\n",
       " 'CDKL5',\n",
       " 'CSPG5',\n",
       " 'CTNNA1',\n",
       " 'OMP',\n",
       " 'OTOA',\n",
       " 'GLP1R',\n",
       " 'CEND1',\n",
       " 'SNAP25',\n",
       " 'PCARE',\n",
       " 'FH',\n",
       " 'CORO6',\n",
       " 'SCN3B',\n",
       " 'DCUN1D1',\n",
       " 'NLGN2',\n",
       " 'DEFB104A_DEFB104B',\n",
       " 'DEFB116',\n",
       " 'CRYM',\n",
       " 'SPTBN2',\n",
       " 'GPR101',\n",
       " 'DGCR6',\n",
       " 'GRIN2B',\n",
       " 'ZPR1',\n",
       " 'CD3D',\n",
       " 'HTR1A',\n",
       " 'TFAP2A',\n",
       " 'BLOC1S2',\n",
       " 'IMPG1',\n",
       " 'BRME1',\n",
       " 'KLRC1',\n",
       " 'HTR1B',\n",
       " 'IFNL2',\n",
       " 'VAV3',\n",
       " 'ITPRIP',\n",
       " 'KLF4',\n",
       " 'KIF20B',\n",
       " 'ATXN2',\n",
       " 'TSPAN7',\n",
       " 'BCAT2',\n",
       " 'IGDCC3',\n",
       " 'LELP1',\n",
       " 'TMPRSS11B',\n",
       " 'KCNC4',\n",
       " 'MAP1LC3A',\n",
       " 'BRD2',\n",
       " 'LYPLA2',\n",
       " 'BOLA1',\n",
       " 'ART5',\n",
       " 'AGBL2',\n",
       " 'UPK3A',\n",
       " 'IL13RA2',\n",
       " 'HDAC9',\n",
       " 'ARMCX2',\n",
       " 'KIRREL1',\n",
       " 'TJP3',\n",
       " 'TUBB3',\n",
       " 'ARID3A',\n",
       " 'KRT8',\n",
       " 'BHLHE40',\n",
       " 'ARHGEF5',\n",
       " 'ADGRV1',\n",
       " 'LMOD2',\n",
       " 'GFRAL',\n",
       " 'DNAJB6',\n",
       " 'CD7',\n",
       " 'NAGA',\n",
       " 'PTPN9',\n",
       " 'NDUFA5',\n",
       " 'SCPEP1',\n",
       " 'PRR4',\n",
       " 'CSF3R',\n",
       " 'UNC5D',\n",
       " 'TYRP1',\n",
       " 'SHH',\n",
       " 'GLI2',\n",
       " 'GIPR',\n",
       " 'UBE2Z',\n",
       " 'GAD2',\n",
       " 'SLITRK1',\n",
       " 'BCL2L15',\n",
       " 'TLR1',\n",
       " 'EDNRB',\n",
       " 'NUMB',\n",
       " 'ALPI',\n",
       " 'KLRF1',\n",
       " 'SIRT1',\n",
       " 'HS6ST2',\n",
       " 'GIT1',\n",
       " 'CD36',\n",
       " 'TLR4',\n",
       " 'CSNK1D',\n",
       " 'CSF2RB',\n",
       " 'CD3G',\n",
       " 'RNF168',\n",
       " 'RAP1A',\n",
       " 'FGF12',\n",
       " 'REPS1',\n",
       " 'FOLH1',\n",
       " 'RICTOR',\n",
       " 'TRAF3',\n",
       " 'NFAT5',\n",
       " 'FOXJ3',\n",
       " 'CEBPA',\n",
       " 'TPSG1',\n",
       " 'NEDD9',\n",
       " 'RNF31',\n",
       " 'CEMIP2',\n",
       " 'RPA2',\n",
       " 'CLEC12A',\n",
       " 'NEDD4L',\n",
       " 'S100A13',\n",
       " 'NECTIN1',\n",
       " 'TOP2B',\n",
       " 'TP53BP1',\n",
       " 'SEMA6C',\n",
       " 'RELB',\n",
       " 'FGF16',\n",
       " 'NME1',\n",
       " 'NPHS2',\n",
       " 'NPHS1',\n",
       " 'FGF20',\n",
       " 'RALB',\n",
       " 'FGF3',\n",
       " 'IL12RB2',\n",
       " 'ANKMY2',\n",
       " 'FGF6',\n",
       " 'PTP4A3',\n",
       " 'BAG4',\n",
       " 'CPOX',\n",
       " 'TSPYL1',\n",
       " 'BABAM1',\n",
       " 'LATS1',\n",
       " 'TSC1',\n",
       " 'IGFL4',\n",
       " 'RBPMS',\n",
       " 'CD226',\n",
       " 'NXPH3',\n",
       " 'MTDH',\n",
       " 'DGKA',\n",
       " 'STX7',\n",
       " 'STX5',\n",
       " 'HIF1A',\n",
       " 'EIF4E',\n",
       " 'IL36A',\n",
       " 'CASP9',\n",
       " 'PGR',\n",
       " 'DENR',\n",
       " 'ST8SIA1',\n",
       " 'TGFBR1',\n",
       " 'KDM3A',\n",
       " 'PPL',\n",
       " 'DDX4',\n",
       " 'DDX39A',\n",
       " 'ACP1',\n",
       " 'PDZK1',\n",
       " 'SMPD3',\n",
       " 'MKI67',\n",
       " 'POLR2A',\n",
       " 'POF1B',\n",
       " 'PIKFYVE',\n",
       " 'C1QL2',\n",
       " 'ACRV1',\n",
       " 'ZBP1',\n",
       " 'PLCB1',\n",
       " 'YY1',\n",
       " 'ZNF174',\n",
       " 'ADAM12',\n",
       " 'XIAP',\n",
       " 'EP300',\n",
       " 'TERF1',\n",
       " 'ADAMTS1',\n",
       " 'WASL',\n",
       " 'SUMF1',\n",
       " 'ADAMTS4',\n",
       " 'PPM1B',\n",
       " 'STAT2',\n",
       " 'ERMAP',\n",
       " 'HDAC8',\n",
       " 'DAPK2',\n",
       " 'DAND5',\n",
       " 'IL21R',\n",
       " 'IL31',\n",
       " 'VAMP8',\n",
       " 'IL20RB',\n",
       " 'CCNE1',\n",
       " 'EVI5',\n",
       " 'MRPS16',\n",
       " 'PRR5',\n",
       " 'PRSS22',\n",
       " 'PSMG4',\n",
       " 'AKR7L',\n",
       " 'PER3',\n",
       " 'BLNK',\n",
       " 'CA8',\n",
       " 'DBN1',\n",
       " 'SPRED2',\n",
       " 'PALLD',\n",
       " 'SSBP1',\n",
       " 'BNIP3L',\n",
       " 'VEGFB',\n",
       " 'MCEMP1',\n",
       " 'ITGAL',\n",
       " 'INSR',\n",
       " 'ESR1',\n",
       " 'IFI30',\n",
       " 'CNP',\n",
       " 'NAGK',\n",
       " 'LAMP1',\n",
       " 'TP73',\n",
       " 'PGM2',\n",
       " 'DYNLT1',\n",
       " 'CHM',\n",
       " 'PFDN6',\n",
       " 'TPBGL',\n",
       " 'FZD10',\n",
       " 'CLIC5',\n",
       " 'DTX2',\n",
       " 'CLNS1A',\n",
       " 'RRAS',\n",
       " 'CLGN',\n",
       " 'PDRG1',\n",
       " 'RPGR',\n",
       " 'DUSP29',\n",
       " 'CLEC2L',\n",
       " 'EFNB2',\n",
       " 'CHRM1',\n",
       " 'CIT',\n",
       " 'LRFN2',\n",
       " 'AP2B1',\n",
       " 'FRMD7',\n",
       " 'CRTAP',\n",
       " 'PTH',\n",
       " 'FARSA',\n",
       " 'AKR1B10',\n",
       " 'PSMD5',\n",
       " 'FBN2',\n",
       " 'CUZD1',\n",
       " 'OSTN',\n",
       " 'UROS',\n",
       " 'AIDA',\n",
       " 'PRKAG3',\n",
       " 'NRXN3',\n",
       " 'AMIGO1',\n",
       " 'DCC',\n",
       " 'PPT1',\n",
       " 'ERC2',\n",
       " 'DOC2B',\n",
       " 'RAC3',\n",
       " 'DDX25',\n",
       " 'DDX53',\n",
       " 'TTF2',\n",
       " 'KCNH2',\n",
       " 'DIPK1C',\n",
       " 'RBP1',\n",
       " 'TRIM40',\n",
       " 'NLGN1',\n",
       " 'PMS1',\n",
       " 'COL28A1',\n",
       " 'EPB41L5',\n",
       " 'IFT20',\n",
       " 'CNTNAP4',\n",
       " 'LRP2',\n",
       " 'C2orf69',\n",
       " 'LYSMD3',\n",
       " 'MAG',\n",
       " 'MRI1',\n",
       " 'SCT',\n",
       " 'CASC3',\n",
       " 'LRTM1',\n",
       " 'SLC44A4',\n",
       " 'GTPBP2',\n",
       " 'TDO2',\n",
       " 'SLC1A4',\n",
       " 'SV2A',\n",
       " 'MFAP3L',\n",
       " 'GBA',\n",
       " 'SOX9',\n",
       " 'CAMLG',\n",
       " 'MN1',\n",
       " 'CABP2',\n",
       " 'CCDC28A',\n",
       " 'TMCO5A',\n",
       " 'NAA80',\n",
       " 'TEX101',\n",
       " 'STX1B',\n",
       " 'BATF',\n",
       " 'CADPS',\n",
       " 'LRRC38',\n",
       " 'SEZ6',\n",
       " 'MSLNL',\n",
       " 'MYL6B',\n",
       " 'MDM1',\n",
       " 'SOWAHA',\n",
       " 'LRP2BP',\n",
       " 'SCN2B',\n",
       " 'CD164L2',\n",
       " 'TBR1',\n",
       " 'MYLPF',\n",
       " 'CGN',\n",
       " 'TARM1',\n",
       " 'MICALL2',\n",
       " 'GNGT1',\n",
       " 'SCN3A',\n",
       " 'HNF1A',\n",
       " 'ANXA1',\n",
       " 'SUSD5',\n",
       " 'RBPMS2',\n",
       " 'RANBP1',\n",
       " 'COQ7',\n",
       " 'MYBPC2',\n",
       " 'DMP1',\n",
       " 'ANP32C',\n",
       " 'PRRT3',\n",
       " 'PNMA1',\n",
       " 'HSDL2',\n",
       " 'TMEM132A',\n",
       " 'IGSF21',\n",
       " 'MYL4',\n",
       " 'DLL4',\n",
       " 'DMD',\n",
       " 'MYL3',\n",
       " 'EDN1',\n",
       " 'GIP',\n",
       " 'HSBP1',\n",
       " 'BOLA2_BOLA2B',\n",
       " 'AIF1L',\n",
       " 'OXCT1',\n",
       " 'PAGR1',\n",
       " 'SNED1',\n",
       " 'OPLAH',\n",
       " 'GNPDA1',\n",
       " 'SNX5',\n",
       " 'AHNAK2',\n",
       " 'AHNAK',\n",
       " 'BECN1',\n",
       " 'FAM172A',\n",
       " 'VIPR1',\n",
       " 'HRC',\n",
       " 'KHK',\n",
       " 'POMC',\n",
       " 'HS1BP3',\n",
       " 'NUDT10',\n",
       " 'PYDC1',\n",
       " 'SIL1',\n",
       " 'HMGCL',\n",
       " 'SIGLEC8',\n",
       " 'CRYZL1',\n",
       " 'CCER2',\n",
       " 'LAMB1',\n",
       " 'GRP',\n",
       " 'CBS',\n",
       " 'ADAMTSL4',\n",
       " 'EPPK1',\n",
       " 'LIPF',\n",
       " 'B3GNT7',\n",
       " 'RECK',\n",
       " 'SCRIB',\n",
       " 'SEC31A',\n",
       " 'RNF149',\n",
       " 'COMMD1',\n",
       " 'ATP6V1G1',\n",
       " 'RNF5',\n",
       " 'ROBO4',\n",
       " 'FSHB',\n",
       " 'RPL14',\n",
       " 'CEP170',\n",
       " 'AAMDC',\n",
       " 'EIF2S2',\n",
       " 'SCN4B',\n",
       " 'SEL1L',\n",
       " 'INPP5D',\n",
       " 'FSTL1',\n",
       " 'EHD3',\n",
       " 'PECR',\n",
       " 'ECHS1',\n",
       " 'MECR',\n",
       " 'TOR1AIP1',\n",
       " 'ASRGL1',\n",
       " 'IDO1',\n",
       " 'ZP3',\n",
       " 'GADD45GIP1',\n",
       " 'RNASE10',\n",
       " 'MAN1A2',\n",
       " 'COL2A1',\n",
       " 'NIT1',\n",
       " 'ITPR1',\n",
       " 'ENPP6',\n",
       " 'ENO3',\n",
       " 'LONP1',\n",
       " 'DNAJC6',\n",
       " 'NFE2',\n",
       " 'ENTR1',\n",
       " 'GATD3',\n",
       " 'M6PR',\n",
       " 'CALCOCO2',\n",
       " 'APOBR',\n",
       " 'ECM1',\n",
       " 'ACYP1',\n",
       " 'WFDC1',\n",
       " 'GM2A',\n",
       " 'PLG',\n",
       " 'SH3GL3',\n",
       " 'PCBD1',\n",
       " 'RLN2',\n",
       " 'C1QTNF9',\n",
       " 'SERPINI1',\n",
       " 'GLA',\n",
       " 'CACYBP',\n",
       " 'MARS1',\n",
       " 'HMCN2',\n",
       " 'C7',\n",
       " 'LPA',\n",
       " 'FGA',\n",
       " 'CLEC3B',\n",
       " 'PAXX',\n",
       " 'C1QTNF5',\n",
       " 'MENT',\n",
       " 'ADGRD1',\n",
       " 'VTI1A',\n",
       " 'DAAM1',\n",
       " 'GNPDA2',\n",
       " 'PENK',\n",
       " 'SYAP1',\n",
       " 'ADD1',\n",
       " 'PINLYP',\n",
       " 'JAM3',\n",
       " 'PRKG1',\n",
       " 'ITGA2',\n",
       " 'DNAJB2',\n",
       " 'SNX15',\n",
       " 'DIPK2B',\n",
       " 'TBCA',\n",
       " 'GP5',\n",
       " 'YWHAQ',\n",
       " 'PDE5A',\n",
       " 'DTD1',\n",
       " 'DDI2',\n",
       " 'ADH1B',\n",
       " 'ST13',\n",
       " 'INHBB',\n",
       " 'ERP29',\n",
       " 'PHYKPL',\n",
       " 'MOCS2',\n",
       " 'AFAP1',\n",
       " 'SPART',\n",
       " 'HEG1',\n",
       " 'BMPER',\n",
       " 'PDIA3',\n",
       " 'DCTD',\n",
       " 'MFAP4',\n",
       " 'BMP10',\n",
       " 'SPINK2',\n",
       " 'EPHA4',\n",
       " 'ACHE',\n",
       " 'CHAD',\n",
       " 'UBXN1',\n",
       " 'TNFRSF17',\n",
       " 'SLC9A3R1',\n",
       " 'LZTFL1',\n",
       " 'ARHGAP45',\n",
       " 'AMOT',\n",
       " 'CD72',\n",
       " 'CELSR2',\n",
       " 'GIMAP7',\n",
       " 'SDK2',\n",
       " 'GHR',\n",
       " 'RABEP1',\n",
       " 'CD300A',\n",
       " 'SEMA3G',\n",
       " 'CRELD1',\n",
       " 'RIDA',\n",
       " 'SFRP4',\n",
       " 'MXRA8',\n",
       " 'APPL2',\n",
       " 'MYOM3',\n",
       " 'FGFR4',\n",
       " 'TNFAIP8L2',\n",
       " 'PTRHD1',\n",
       " 'COL5A1',\n",
       " 'FUOM',\n",
       " 'AKAP12',\n",
       " 'CTSE',\n",
       " 'SCGB3A1',\n",
       " 'TPD52L2',\n",
       " 'NAGPA',\n",
       " 'UROD',\n",
       " 'GMPR2',\n",
       " 'SNCA',\n",
       " 'GLRX5',\n",
       " 'KCTD5',\n",
       " 'UPK3BL1',\n",
       " 'TRIM24',\n",
       " 'CTAG1A_CTAG1B',\n",
       " 'FUT1',\n",
       " 'HRAS',\n",
       " 'TET2',\n",
       " 'COL4A4',\n",
       " 'TCN1',\n",
       " 'KLKB1',\n",
       " 'QSOX1',\n",
       " 'CEACAM18',\n",
       " 'EFCAB2',\n",
       " 'NEK7',\n",
       " 'NFKB2',\n",
       " 'CEACAM20',\n",
       " 'RGL2',\n",
       " 'SEPTIN7',\n",
       " 'SAP18',\n",
       " 'ARAF',\n",
       " 'GABARAPL1',\n",
       " 'SAT2',\n",
       " 'ARHGAP30',\n",
       " 'TRDMT1',\n",
       " 'ID4',\n",
       " 'PKN3',\n",
       " 'MAPKAPK2',\n",
       " 'TNPO1',\n",
       " 'TAP1',\n",
       " 'TCP11',\n",
       " 'ITGAX',\n",
       " 'IFIT3',\n",
       " 'ACADM',\n",
       " 'CEP290',\n",
       " 'TAB2',\n",
       " 'GAS2',\n",
       " 'RPE',\n",
       " 'ZNF75D',\n",
       " 'LSM8',\n",
       " 'CENPJ',\n",
       " 'CINP',\n",
       " 'RNF43',\n",
       " 'IFIT1',\n",
       " 'CA7',\n",
       " 'RNF4',\n",
       " 'CENPF',\n",
       " 'TPPP2',\n",
       " 'IL9',\n",
       " 'PAFAH2',\n",
       " 'EPN1',\n",
       " 'COL9A2',\n",
       " 'PPIE',\n",
       " 'TLR2',\n",
       " 'MNAT1',\n",
       " 'ERI1',\n",
       " 'CD3E',\n",
       " 'MAGEA3',\n",
       " 'ALMS1',\n",
       " 'PPP1R12B',\n",
       " 'VPS28',\n",
       " 'PTTG1',\n",
       " 'MORF4L1',\n",
       " 'KIAA1549',\n",
       " 'SPRR1B',\n",
       " 'SLK',\n",
       " 'TK1',\n",
       " 'OFD1',\n",
       " 'KIAA1549L',\n",
       " 'MTHFSD',\n",
       " 'EVPL',\n",
       " 'GADD45B',\n",
       " 'TIGIT',\n",
       " 'CCND2',\n",
       " 'BRD1',\n",
       " 'SHPK',\n",
       " 'VSTM2B',\n",
       " 'TEX33',\n",
       " 'GUCY2C',\n",
       " 'CDH22',\n",
       " 'SERPINH1',\n",
       " 'RAPGEF2',\n",
       " 'PRUNE2',\n",
       " 'MTUS1',\n",
       " 'TMED1',\n",
       " 'GTF2IRD1',\n",
       " 'CASP4',\n",
       " 'OGT',\n",
       " 'RAD51',\n",
       " 'TXK',\n",
       " 'PARD3',\n",
       " 'CD82',\n",
       " 'BCHE',\n",
       " 'SERPINF2',\n",
       " 'SERPINA1',\n",
       " 'SERPINA4',\n",
       " 'SGSH',\n",
       " 'CFB',\n",
       " 'NPC2',\n",
       " 'PRDX2',\n",
       " 'TXN',\n",
       " 'CYB5R2',\n",
       " 'MST1',\n",
       " 'CAT',\n",
       " 'CTBS',\n",
       " 'SERPINF1',\n",
       " 'BRAP',\n",
       " 'HPSE',\n",
       " 'SERPINA5',\n",
       " 'F11',\n",
       " 'MBL2',\n",
       " 'CFHR5',\n",
       " 'IST1',\n",
       " 'PGLYRP2',\n",
       " 'CWC15',\n",
       " 'PALM',\n",
       " 'TTR',\n",
       " 'PSAP',\n",
       " 'ASAH1',\n",
       " 'HGFAC',\n",
       " 'AMOTL2',\n",
       " 'CRISP3',\n",
       " 'NMI',\n",
       " 'EIF2AK2',\n",
       " 'APCS',\n",
       " 'SLURP1',\n",
       " 'DTNB',\n",
       " 'LACRT',\n",
       " 'BTN1A1',\n",
       " 'THTPA',\n",
       " 'MPRIP',\n",
       " 'KLK15',\n",
       " 'RNASE6',\n",
       " 'NAP1L4',\n",
       " 'CDC26',\n",
       " 'LMNB1',\n",
       " 'NUDT16',\n",
       " 'PPBP',\n",
       " 'PF4',\n",
       " 'CFHR2',\n",
       " 'GSR',\n",
       " 'MDH1',\n",
       " 'IL2RG',\n",
       " 'REG3G',\n",
       " 'FNTA',\n",
       " 'RFC4',\n",
       " 'CMIP',\n",
       " 'NUBP1',\n",
       " 'FAM171B',\n",
       " 'NENF',\n",
       " 'AHSA1',\n",
       " 'IL22',\n",
       " 'COMMD9',\n",
       " 'VSIG10',\n",
       " 'KIAA2013',\n",
       " 'RCC1',\n",
       " 'ALDH2',\n",
       " 'UNG',\n",
       " 'VPS4B',\n",
       " 'RALY',\n",
       " 'RAB44',\n",
       " 'PXDNL',\n",
       " 'RAB2B',\n",
       " 'VSIG2',\n",
       " 'KIR2DL2',\n",
       " 'USP25',\n",
       " 'UBE2B',\n",
       " 'LARP1',\n",
       " 'S100A3',\n",
       " 'TDP1',\n",
       " 'CAPN3',\n",
       " 'MINDY1',\n",
       " 'SUSD4',\n",
       " 'TADA3',\n",
       " 'TARS1',\n",
       " 'LRRFIP1',\n",
       " 'TG',\n",
       " 'STAM',\n",
       " 'TGFB2',\n",
       " 'LTB',\n",
       " 'LUZP2',\n",
       " 'MAMDC4',\n",
       " 'HSPA2',\n",
       " 'BCL7B',\n",
       " 'CCDC134',\n",
       " 'GPRC5C',\n",
       " 'LAMTOR5',\n",
       " 'MTSS2',\n",
       " 'NDST1',\n",
       " 'GABARAP',\n",
       " 'CHCHD6',\n",
       " 'NACC1',\n",
       " 'AP1G2',\n",
       " 'TRIM58',\n",
       " 'SLC13A1',\n",
       " 'GPD1',\n",
       " 'SMAD2',\n",
       " 'SMAD3',\n",
       " 'GLYR1',\n",
       " 'SMPDL3B',\n",
       " 'SNX2',\n",
       " 'ARG2',\n",
       " 'SDCCAG8',\n",
       " 'TMEM106A',\n",
       " 'ACRBP',\n",
       " 'DUSP13',\n",
       " 'DYNC1H1',\n",
       " 'EDDM3B',\n",
       " 'DYNLT3',\n",
       " 'WDR46',\n",
       " 'PCDHB15',\n",
       " 'EPGN',\n",
       " 'DHPS',\n",
       " 'IMMT',\n",
       " 'PRC1',\n",
       " 'PPP1CC',\n",
       " 'ERN1',\n",
       " 'ZNRF4',\n",
       " 'ZNF830',\n",
       " 'YJU2',\n",
       " 'PCSK7',\n",
       " 'INSL4',\n",
       " 'ENOPH1',\n",
       " 'ITIH5',\n",
       " 'ENSA',\n",
       " 'TMPRSS11D',\n",
       " 'FTCD',\n",
       " 'PLSCR3',\n",
       " 'SEPTIN8',\n",
       " 'PRKAR2A',\n",
       " 'SMTN',\n",
       " 'NFU1',\n",
       " 'PBXIP1',\n",
       " 'HIP1R',\n",
       " 'ZFYVE19',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteomics = test_imputed.columns[test_imputed.columns.tolist().index(\"C3\") :].tolist()\n",
    "risk_factors = [\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"ldl_a\",\n",
    "    \"hdl_a\",\n",
    "    \"tc_a\",\n",
    "    \"tg_a\",\n",
    "    \"sbp_a\",\n",
    "    \"BMI\",\n",
    "    \"smoking\",\n",
    "    \"prevalent_diabetes\",\n",
    "]\n",
    "\n",
    "PRS = [\"PRS\"]\n",
    "proteomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : incident_cad\n",
      "0.0             27209\n",
      "1.0              1596\n",
      "dtype: int64\n",
      "val : incident_cad\n",
      "0.0             6799\n",
      "1.0              403\n",
      "dtype: int64\n",
      "Test : incident_cad\n",
      "0.0             14599\n",
      "1.0               833\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y).long(), num_classes=self.num_classes\n",
      "/tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y).long(), num_classes=self.num_classes\n",
      "/tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y).long(), num_classes=self.num_classes\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetModule(\n",
    "    train=train_imputed,\n",
    "    test=test_imputed,\n",
    "    features=proteomics,\n",
    "    label=[\"incident_cad\"],\n",
    "    num_classes=2,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(134)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(130)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(138)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(117)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(130)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(119)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(119)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(138)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(132)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(120)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(131)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(131)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(129)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(127)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(130)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(128)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(128)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(136)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(135)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(123)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(139)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(122)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(131)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(131)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(131)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(130)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(117)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(124)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(121)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(131)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(122)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(125)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(121)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(125)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(149)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(123)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(125)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(127)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(132)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(139)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(140)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(132)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(134)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(124)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(125)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(134)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(129)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(127)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(139)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(121)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(134)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(123)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(127)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(136)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(129)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(117)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(121)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(131)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(133)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(121)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(114)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(128)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(125)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(143)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(128)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(132)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(118)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(140)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(133)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(125)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(115)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(130)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(127)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(125)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(113)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(122)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(113)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(121)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(124)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(122)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(134)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(113)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(117)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(145)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(125)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(137)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(133)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(113)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(118)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(131)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(112)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(135)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(117)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(125)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(130)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(118)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(130)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(131)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(121)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(129)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(126)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(132)\n",
      "torch.Size([256, 2911]) torch.Size([256, 2])\n",
      "tensor(135)\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.train_dataloader():\n",
    "    print(x.shape, y.shape)\n",
    "    print(torch.argmax(y, dim=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class LinearResBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearResBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "        self.layer_norm = nn.LayerNorm(output_size)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight, nonlinearity=\"relu\")  # <6>\n",
    "        torch.nn.init.constant_(self.layer_norm.weight, 0.5)  # <7>\n",
    "        torch.nn.init.zeros_(self.layer_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        out = self.layer_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class FullyConnectedNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        features,\n",
    "        output_size,\n",
    "        num_resblocks=3,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-2,\n",
    "        weight=[1, 1],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(FullyConnectedNet, self).__init__()\n",
    "        input_size = len(features)\n",
    "        self.features = features\n",
    "        self.norm = nn.LayerNorm(input_size)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[LinearResBlock(hidden_size, hidden_size) for _ in range(num_resblocks)]\n",
    "        )\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.mertic = {\n",
    "            \"train_auc\": torchmetrics.AUROC(num_classes=2, task=\"multiclass\"),\n",
    "            \"val_auc\": torchmetrics.AUROC(num_classes=2, task=\"multiclass\"),\n",
    "        }\n",
    "        self.history = defaultdict(dict)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weight).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        out = torch.relu(self.fc1(x))\n",
    "        out = self.resblocks(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss_fn(outputs, y.squeeze(-1).float())\n",
    "\n",
    "        self.mertic[\"train_auc\"].update(\n",
    "            torch.softmax(outputs, dim=-1), torch.argmax(y, dim=1)\n",
    "        )\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss, on_epoch=True, prog_bar=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss_fn(outputs, y.squeeze(-1).float())\n",
    "\n",
    "        self.mertic[\"val_auc\"].update(\n",
    "            torch.softmax(outputs, dim=-1), torch.argmax(y, dim=1)\n",
    "        )\n",
    "\n",
    "        self.log(\"ptl/val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "\n",
    "        auc = self.mertic[\"train_auc\"].compute()\n",
    "        self.log(\"ptl/train_auc\", auc, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        auc = self.mertic[\"val_auc\"].compute()\n",
    "        self.log(\"ptl/val_auc\", auc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def predict_df(self, df, batch_size=256):\n",
    "\n",
    "        for feature in self.features:\n",
    "            assert feature in df.columns\n",
    "        print(f\"input df have NA: {df[self.features].isna().sum(axis=1).sum()}\")\n",
    "        df = df.copy().dropna(subset=self.features)\n",
    "\n",
    "        predict_dataloader = DataLoader(\n",
    "            torch.tensor(df[self.features].values).float(),\n",
    "            batch_size=batch_size,\n",
    "            persistent_workers=True,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "        self.eval()\n",
    "        pred = []\n",
    "        with torch.no_grad():\n",
    "            for x in predict_dataloader:\n",
    "                y_hat = self.forward(x).cpu().detach()\n",
    "                y_hat = torch.softmax(y_hat, dim=-1)[:, 1]\n",
    "\n",
    "                pred.append(y_hat)\n",
    "        pred = torch.cat(pred).numpy()\n",
    "        df[\"pred\"] = pred\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C3',\n",
       " 'KLK7',\n",
       " 'GCHFR',\n",
       " 'NHLRC3',\n",
       " 'APOD',\n",
       " 'GAPDH',\n",
       " 'TP53I3',\n",
       " 'CPA4',\n",
       " 'ANXA2',\n",
       " 'GRSF1',\n",
       " 'IL25',\n",
       " 'HMMR',\n",
       " 'MRPL52',\n",
       " 'PAIP2B',\n",
       " 'THAP12',\n",
       " 'FOS',\n",
       " 'FGF9',\n",
       " 'PITHD1',\n",
       " 'THSD1',\n",
       " 'PTGES2',\n",
       " 'DEFB103A_DEFB103B',\n",
       " 'ATP1B4',\n",
       " 'CYB5A',\n",
       " 'UNC79',\n",
       " 'SLC34A3',\n",
       " 'TAGLN3',\n",
       " 'SLIRP',\n",
       " 'CLASP1',\n",
       " 'PSMC3',\n",
       " 'KIR3DL2',\n",
       " 'BEX3',\n",
       " 'PFDN4',\n",
       " 'BCL7A',\n",
       " 'SMC3',\n",
       " 'SLC28A1',\n",
       " 'CDC123',\n",
       " 'GJA8',\n",
       " 'NMRK2',\n",
       " 'GATA3',\n",
       " 'CPLX2',\n",
       " 'RASGRF1',\n",
       " 'FGF7',\n",
       " 'ANKRA2',\n",
       " 'RBM25',\n",
       " 'LYZL2',\n",
       " 'CDK1',\n",
       " 'CREB3',\n",
       " 'CREBZF',\n",
       " 'IGLON5',\n",
       " 'SHC1',\n",
       " 'ZP4',\n",
       " 'TMOD4',\n",
       " 'CEP152',\n",
       " 'MYH7B',\n",
       " 'CEP350',\n",
       " 'CDC25A',\n",
       " 'TRIM26',\n",
       " 'MANEAL',\n",
       " 'MUCL3',\n",
       " 'GIMAP8',\n",
       " 'CYTH3',\n",
       " 'PDXDC1',\n",
       " 'CLINT1',\n",
       " 'MAPRE3',\n",
       " 'EVI2B',\n",
       " 'STAU1',\n",
       " 'PCNA',\n",
       " 'DNAJA1',\n",
       " 'JMJD1C',\n",
       " 'GAGE2A',\n",
       " 'GAD1',\n",
       " 'IZUMO1',\n",
       " 'PDCL2',\n",
       " 'PDE1C',\n",
       " 'STOML2',\n",
       " 'BSND',\n",
       " 'MAPK13',\n",
       " 'PDIA2',\n",
       " 'BTLA',\n",
       " 'MLLT1',\n",
       " 'TPRKB',\n",
       " 'ARHGAP5',\n",
       " 'BTNL10',\n",
       " 'PHLDB2',\n",
       " 'PDIA5',\n",
       " 'ATF4',\n",
       " 'PRAME',\n",
       " 'TOP1MT',\n",
       " 'KHDC3L',\n",
       " 'DCUN1D2',\n",
       " 'IL3',\n",
       " 'DCLRE1C',\n",
       " 'ERCC1',\n",
       " 'DCDC2C',\n",
       " 'VCPKMT',\n",
       " 'SPRING1',\n",
       " 'MORN4',\n",
       " 'ESPL1',\n",
       " 'H2AP',\n",
       " 'MORF4L2',\n",
       " 'SSH3',\n",
       " 'VWA5A',\n",
       " 'PBK',\n",
       " 'REST',\n",
       " 'SHD',\n",
       " 'TXNL1',\n",
       " 'TPM3',\n",
       " 'NEB',\n",
       " 'ATP1B2',\n",
       " 'CEP112',\n",
       " 'SART1',\n",
       " 'ATP6V1G2',\n",
       " 'ATP2B4',\n",
       " 'SAT1',\n",
       " 'ATP1B1',\n",
       " 'NECAP2',\n",
       " 'ATP5F1D',\n",
       " 'ATP1B3',\n",
       " 'ARNTL',\n",
       " 'ARL2BP',\n",
       " 'SCGB2A2',\n",
       " 'GAMT',\n",
       " 'ASS1',\n",
       " 'NFYA',\n",
       " 'GASK1A',\n",
       " 'MANSC4',\n",
       " 'HMGCS1',\n",
       " 'MMUT',\n",
       " 'CBX2',\n",
       " 'BRD3',\n",
       " 'BRDT',\n",
       " 'MAP1LC3B2',\n",
       " 'CASQ2',\n",
       " 'HIP1',\n",
       " 'GSTM4',\n",
       " 'GUK1',\n",
       " 'CALY',\n",
       " 'C1GALT1C1',\n",
       " 'TEF',\n",
       " 'CACNA1H',\n",
       " 'HADH',\n",
       " 'MEGF11',\n",
       " 'MED21',\n",
       " 'THRAP3',\n",
       " 'SPINK8',\n",
       " 'NAA10',\n",
       " 'MRPL24',\n",
       " 'GBP6',\n",
       " 'MYOM2',\n",
       " 'B3GAT3',\n",
       " 'GCLM',\n",
       " 'MYL1',\n",
       " 'HSD17B3',\n",
       " 'MYH4',\n",
       " 'TMED4',\n",
       " 'TMED10',\n",
       " 'SKIV2L',\n",
       " 'SLC12A2',\n",
       " 'SLC51B',\n",
       " 'MTR',\n",
       " 'CD2',\n",
       " 'BHMT2',\n",
       " 'SNU13',\n",
       " 'GP1BB',\n",
       " 'ARL13B',\n",
       " 'HCG22',\n",
       " 'RYR1',\n",
       " 'FDX2',\n",
       " 'ADRA2A',\n",
       " 'ERVV-1',\n",
       " 'EXOSC10',\n",
       " 'EXTL1',\n",
       " 'CYP24A1',\n",
       " 'KIF1C',\n",
       " 'USP47',\n",
       " 'PRKD2',\n",
       " 'PROCR',\n",
       " 'PACS2',\n",
       " 'KIF22',\n",
       " 'NXPE4',\n",
       " 'RTKN2',\n",
       " 'CSRP3',\n",
       " 'NUDT15',\n",
       " 'UHRF2',\n",
       " 'UGDH',\n",
       " 'CSF2',\n",
       " 'KRT17',\n",
       " 'FDX1',\n",
       " 'PYY',\n",
       " 'UBQLN3',\n",
       " 'CSDE1',\n",
       " 'DDA1',\n",
       " 'PALM3',\n",
       " 'VSIG10L',\n",
       " 'PKD2',\n",
       " 'ABCA2',\n",
       " 'EDEM2',\n",
       " 'ABRAXAS2',\n",
       " 'ECI2',\n",
       " 'PGLYRP4',\n",
       " 'PDZD2',\n",
       " 'EIF2AK3',\n",
       " 'EIF5',\n",
       " 'ELOB',\n",
       " 'ITPA',\n",
       " 'ACSL1',\n",
       " 'DENND2B',\n",
       " 'ZCCHC8',\n",
       " 'ACTN2',\n",
       " 'PDE4D',\n",
       " 'ACY3',\n",
       " 'ENOX2',\n",
       " 'YOD1',\n",
       " 'ENPEP',\n",
       " 'PMCH',\n",
       " 'PMM2',\n",
       " 'DHODH',\n",
       " 'KRT6C',\n",
       " 'NUP50',\n",
       " 'LAMA1',\n",
       " 'COPB2',\n",
       " 'LRCH4',\n",
       " 'TSNAX',\n",
       " 'LPP',\n",
       " 'TRPV3',\n",
       " 'IGHMBP2',\n",
       " 'LILRA4',\n",
       " 'FHIP2A',\n",
       " 'NOP56',\n",
       " 'RIPK4',\n",
       " 'TRAF3IP2',\n",
       " 'IGF2BP3',\n",
       " 'NFKB1',\n",
       " 'NFX1',\n",
       " 'REXO2',\n",
       " 'TSPAN15',\n",
       " 'RBM19',\n",
       " 'FRMD4B',\n",
       " 'NOS2',\n",
       " 'TPR',\n",
       " 'NPR1',\n",
       " 'RAB33A',\n",
       " 'RAB39B',\n",
       " 'RPS10',\n",
       " 'ANK2',\n",
       " 'IFNW1',\n",
       " 'CPTP',\n",
       " 'TTN',\n",
       " 'IL36G',\n",
       " 'IL31RA',\n",
       " 'RNASE4',\n",
       " 'LRIG3',\n",
       " 'CACNA1C',\n",
       " 'SCIN',\n",
       " 'DNLZ',\n",
       " 'STEAP4',\n",
       " 'CBLN1',\n",
       " 'CHP1',\n",
       " 'SAG',\n",
       " 'DOCK9',\n",
       " 'RRP15',\n",
       " 'SYNGAP1',\n",
       " 'CNTF',\n",
       " 'ECSCR',\n",
       " 'ELAVL4',\n",
       " 'FZD8',\n",
       " 'SCN2A',\n",
       " 'CNGB3',\n",
       " 'GABRA4',\n",
       " 'CACNB1',\n",
       " 'DEFB118',\n",
       " 'PNMA2',\n",
       " 'SMS',\n",
       " 'CDH4',\n",
       " 'SH3BGRL2',\n",
       " 'RAB3GAP1',\n",
       " 'RANBP2',\n",
       " 'MYOM1',\n",
       " 'CDKL5',\n",
       " 'CSPG5',\n",
       " 'CTNNA1',\n",
       " 'OMP',\n",
       " 'OTOA',\n",
       " 'GLP1R',\n",
       " 'CEND1',\n",
       " 'SNAP25',\n",
       " 'PCARE',\n",
       " 'FH',\n",
       " 'CORO6',\n",
       " 'SCN3B',\n",
       " 'DCUN1D1',\n",
       " 'NLGN2',\n",
       " 'DEFB104A_DEFB104B',\n",
       " 'DEFB116',\n",
       " 'CRYM',\n",
       " 'SPTBN2',\n",
       " 'GPR101',\n",
       " 'DGCR6',\n",
       " 'GRIN2B',\n",
       " 'ZPR1',\n",
       " 'CD3D',\n",
       " 'HTR1A',\n",
       " 'TFAP2A',\n",
       " 'BLOC1S2',\n",
       " 'IMPG1',\n",
       " 'BRME1',\n",
       " 'KLRC1',\n",
       " 'HTR1B',\n",
       " 'IFNL2',\n",
       " 'VAV3',\n",
       " 'ITPRIP',\n",
       " 'KLF4',\n",
       " 'KIF20B',\n",
       " 'ATXN2',\n",
       " 'TSPAN7',\n",
       " 'BCAT2',\n",
       " 'IGDCC3',\n",
       " 'LELP1',\n",
       " 'TMPRSS11B',\n",
       " 'KCNC4',\n",
       " 'MAP1LC3A',\n",
       " 'BRD2',\n",
       " 'LYPLA2',\n",
       " 'BOLA1',\n",
       " 'ART5',\n",
       " 'AGBL2',\n",
       " 'UPK3A',\n",
       " 'IL13RA2',\n",
       " 'HDAC9',\n",
       " 'ARMCX2',\n",
       " 'KIRREL1',\n",
       " 'TJP3',\n",
       " 'TUBB3',\n",
       " 'ARID3A',\n",
       " 'KRT8',\n",
       " 'BHLHE40',\n",
       " 'ARHGEF5',\n",
       " 'ADGRV1',\n",
       " 'LMOD2',\n",
       " 'GFRAL',\n",
       " 'DNAJB6',\n",
       " 'CD7',\n",
       " 'NAGA',\n",
       " 'PTPN9',\n",
       " 'NDUFA5',\n",
       " 'SCPEP1',\n",
       " 'PRR4',\n",
       " 'CSF3R',\n",
       " 'UNC5D',\n",
       " 'TYRP1',\n",
       " 'SHH',\n",
       " 'GLI2',\n",
       " 'GIPR',\n",
       " 'UBE2Z',\n",
       " 'GAD2',\n",
       " 'SLITRK1',\n",
       " 'BCL2L15',\n",
       " 'TLR1',\n",
       " 'EDNRB',\n",
       " 'NUMB',\n",
       " 'ALPI',\n",
       " 'KLRF1',\n",
       " 'SIRT1',\n",
       " 'HS6ST2',\n",
       " 'GIT1',\n",
       " 'CD36',\n",
       " 'TLR4',\n",
       " 'CSNK1D',\n",
       " 'CSF2RB',\n",
       " 'CD3G',\n",
       " 'RNF168',\n",
       " 'RAP1A',\n",
       " 'FGF12',\n",
       " 'REPS1',\n",
       " 'FOLH1',\n",
       " 'RICTOR',\n",
       " 'TRAF3',\n",
       " 'NFAT5',\n",
       " 'FOXJ3',\n",
       " 'CEBPA',\n",
       " 'TPSG1',\n",
       " 'NEDD9',\n",
       " 'RNF31',\n",
       " 'CEMIP2',\n",
       " 'RPA2',\n",
       " 'CLEC12A',\n",
       " 'NEDD4L',\n",
       " 'S100A13',\n",
       " 'NECTIN1',\n",
       " 'TOP2B',\n",
       " 'TP53BP1',\n",
       " 'SEMA6C',\n",
       " 'RELB',\n",
       " 'FGF16',\n",
       " 'NME1',\n",
       " 'NPHS2',\n",
       " 'NPHS1',\n",
       " 'FGF20',\n",
       " 'RALB',\n",
       " 'FGF3',\n",
       " 'IL12RB2',\n",
       " 'ANKMY2',\n",
       " 'FGF6',\n",
       " 'PTP4A3',\n",
       " 'BAG4',\n",
       " 'CPOX',\n",
       " 'TSPYL1',\n",
       " 'BABAM1',\n",
       " 'LATS1',\n",
       " 'TSC1',\n",
       " 'IGFL4',\n",
       " 'RBPMS',\n",
       " 'CD226',\n",
       " 'NXPH3',\n",
       " 'MTDH',\n",
       " 'DGKA',\n",
       " 'STX7',\n",
       " 'STX5',\n",
       " 'HIF1A',\n",
       " 'EIF4E',\n",
       " 'IL36A',\n",
       " 'CASP9',\n",
       " 'PGR',\n",
       " 'DENR',\n",
       " 'ST8SIA1',\n",
       " 'TGFBR1',\n",
       " 'KDM3A',\n",
       " 'PPL',\n",
       " 'DDX4',\n",
       " 'DDX39A',\n",
       " 'ACP1',\n",
       " 'PDZK1',\n",
       " 'SMPD3',\n",
       " 'MKI67',\n",
       " 'POLR2A',\n",
       " 'POF1B',\n",
       " 'PIKFYVE',\n",
       " 'C1QL2',\n",
       " 'ACRV1',\n",
       " 'ZBP1',\n",
       " 'PLCB1',\n",
       " 'YY1',\n",
       " 'ZNF174',\n",
       " 'ADAM12',\n",
       " 'XIAP',\n",
       " 'EP300',\n",
       " 'TERF1',\n",
       " 'ADAMTS1',\n",
       " 'WASL',\n",
       " 'SUMF1',\n",
       " 'ADAMTS4',\n",
       " 'PPM1B',\n",
       " 'STAT2',\n",
       " 'ERMAP',\n",
       " 'HDAC8',\n",
       " 'DAPK2',\n",
       " 'DAND5',\n",
       " 'IL21R',\n",
       " 'IL31',\n",
       " 'VAMP8',\n",
       " 'IL20RB',\n",
       " 'CCNE1',\n",
       " 'EVI5',\n",
       " 'MRPS16',\n",
       " 'PRR5',\n",
       " 'PRSS22',\n",
       " 'PSMG4',\n",
       " 'AKR7L',\n",
       " 'PER3',\n",
       " 'BLNK',\n",
       " 'CA8',\n",
       " 'DBN1',\n",
       " 'SPRED2',\n",
       " 'PALLD',\n",
       " 'SSBP1',\n",
       " 'BNIP3L',\n",
       " 'VEGFB',\n",
       " 'MCEMP1',\n",
       " 'ITGAL',\n",
       " 'INSR',\n",
       " 'ESR1',\n",
       " 'IFI30',\n",
       " 'CNP',\n",
       " 'NAGK',\n",
       " 'LAMP1',\n",
       " 'TP73',\n",
       " 'PGM2',\n",
       " 'DYNLT1',\n",
       " 'CHM',\n",
       " 'PFDN6',\n",
       " 'TPBGL',\n",
       " 'FZD10',\n",
       " 'CLIC5',\n",
       " 'DTX2',\n",
       " 'CLNS1A',\n",
       " 'RRAS',\n",
       " 'CLGN',\n",
       " 'PDRG1',\n",
       " 'RPGR',\n",
       " 'DUSP29',\n",
       " 'CLEC2L',\n",
       " 'EFNB2',\n",
       " 'CHRM1',\n",
       " 'CIT',\n",
       " 'LRFN2',\n",
       " 'AP2B1',\n",
       " 'FRMD7',\n",
       " 'CRTAP',\n",
       " 'PTH',\n",
       " 'FARSA',\n",
       " 'AKR1B10',\n",
       " 'PSMD5',\n",
       " 'FBN2',\n",
       " 'CUZD1',\n",
       " 'OSTN',\n",
       " 'UROS',\n",
       " 'AIDA',\n",
       " 'PRKAG3',\n",
       " 'NRXN3',\n",
       " 'AMIGO1',\n",
       " 'DCC',\n",
       " 'PPT1',\n",
       " 'ERC2',\n",
       " 'DOC2B',\n",
       " 'RAC3',\n",
       " 'DDX25',\n",
       " 'DDX53',\n",
       " 'TTF2',\n",
       " 'KCNH2',\n",
       " 'DIPK1C',\n",
       " 'RBP1',\n",
       " 'TRIM40',\n",
       " 'NLGN1',\n",
       " 'PMS1',\n",
       " 'COL28A1',\n",
       " 'EPB41L5',\n",
       " 'IFT20',\n",
       " 'CNTNAP4',\n",
       " 'LRP2',\n",
       " 'C2orf69',\n",
       " 'LYSMD3',\n",
       " 'MAG',\n",
       " 'MRI1',\n",
       " 'SCT',\n",
       " 'CASC3',\n",
       " 'LRTM1',\n",
       " 'SLC44A4',\n",
       " 'GTPBP2',\n",
       " 'TDO2',\n",
       " 'SLC1A4',\n",
       " 'SV2A',\n",
       " 'MFAP3L',\n",
       " 'GBA',\n",
       " 'SOX9',\n",
       " 'CAMLG',\n",
       " 'MN1',\n",
       " 'CABP2',\n",
       " 'CCDC28A',\n",
       " 'TMCO5A',\n",
       " 'NAA80',\n",
       " 'TEX101',\n",
       " 'STX1B',\n",
       " 'BATF',\n",
       " 'CADPS',\n",
       " 'LRRC38',\n",
       " 'SEZ6',\n",
       " 'MSLNL',\n",
       " 'MYL6B',\n",
       " 'MDM1',\n",
       " 'SOWAHA',\n",
       " 'LRP2BP',\n",
       " 'SCN2B',\n",
       " 'CD164L2',\n",
       " 'TBR1',\n",
       " 'MYLPF',\n",
       " 'CGN',\n",
       " 'TARM1',\n",
       " 'MICALL2',\n",
       " 'GNGT1',\n",
       " 'SCN3A',\n",
       " 'HNF1A',\n",
       " 'ANXA1',\n",
       " 'SUSD5',\n",
       " 'RBPMS2',\n",
       " 'RANBP1',\n",
       " 'COQ7',\n",
       " 'MYBPC2',\n",
       " 'DMP1',\n",
       " 'ANP32C',\n",
       " 'PRRT3',\n",
       " 'PNMA1',\n",
       " 'HSDL2',\n",
       " 'TMEM132A',\n",
       " 'IGSF21',\n",
       " 'MYL4',\n",
       " 'DLL4',\n",
       " 'DMD',\n",
       " 'MYL3',\n",
       " 'EDN1',\n",
       " 'GIP',\n",
       " 'HSBP1',\n",
       " 'BOLA2_BOLA2B',\n",
       " 'AIF1L',\n",
       " 'OXCT1',\n",
       " 'PAGR1',\n",
       " 'SNED1',\n",
       " 'OPLAH',\n",
       " 'GNPDA1',\n",
       " 'SNX5',\n",
       " 'AHNAK2',\n",
       " 'AHNAK',\n",
       " 'BECN1',\n",
       " 'FAM172A',\n",
       " 'VIPR1',\n",
       " 'HRC',\n",
       " 'KHK',\n",
       " 'POMC',\n",
       " 'HS1BP3',\n",
       " 'NUDT10',\n",
       " 'PYDC1',\n",
       " 'SIL1',\n",
       " 'HMGCL',\n",
       " 'SIGLEC8',\n",
       " 'CRYZL1',\n",
       " 'CCER2',\n",
       " 'LAMB1',\n",
       " 'GRP',\n",
       " 'CBS',\n",
       " 'ADAMTSL4',\n",
       " 'EPPK1',\n",
       " 'LIPF',\n",
       " 'B3GNT7',\n",
       " 'RECK',\n",
       " 'SCRIB',\n",
       " 'SEC31A',\n",
       " 'RNF149',\n",
       " 'COMMD1',\n",
       " 'ATP6V1G1',\n",
       " 'RNF5',\n",
       " 'ROBO4',\n",
       " 'FSHB',\n",
       " 'RPL14',\n",
       " 'CEP170',\n",
       " 'AAMDC',\n",
       " 'EIF2S2',\n",
       " 'SCN4B',\n",
       " 'SEL1L',\n",
       " 'INPP5D',\n",
       " 'FSTL1',\n",
       " 'EHD3',\n",
       " 'PECR',\n",
       " 'ECHS1',\n",
       " 'MECR',\n",
       " 'TOR1AIP1',\n",
       " 'ASRGL1',\n",
       " 'IDO1',\n",
       " 'ZP3',\n",
       " 'GADD45GIP1',\n",
       " 'RNASE10',\n",
       " 'MAN1A2',\n",
       " 'COL2A1',\n",
       " 'NIT1',\n",
       " 'ITPR1',\n",
       " 'ENPP6',\n",
       " 'ENO3',\n",
       " 'LONP1',\n",
       " 'DNAJC6',\n",
       " 'NFE2',\n",
       " 'ENTR1',\n",
       " 'GATD3',\n",
       " 'M6PR',\n",
       " 'CALCOCO2',\n",
       " 'APOBR',\n",
       " 'ECM1',\n",
       " 'ACYP1',\n",
       " 'WFDC1',\n",
       " 'GM2A',\n",
       " 'PLG',\n",
       " 'SH3GL3',\n",
       " 'PCBD1',\n",
       " 'RLN2',\n",
       " 'C1QTNF9',\n",
       " 'SERPINI1',\n",
       " 'GLA',\n",
       " 'CACYBP',\n",
       " 'MARS1',\n",
       " 'HMCN2',\n",
       " 'C7',\n",
       " 'LPA',\n",
       " 'FGA',\n",
       " 'CLEC3B',\n",
       " 'PAXX',\n",
       " 'C1QTNF5',\n",
       " 'MENT',\n",
       " 'ADGRD1',\n",
       " 'VTI1A',\n",
       " 'DAAM1',\n",
       " 'GNPDA2',\n",
       " 'PENK',\n",
       " 'SYAP1',\n",
       " 'ADD1',\n",
       " 'PINLYP',\n",
       " 'JAM3',\n",
       " 'PRKG1',\n",
       " 'ITGA2',\n",
       " 'DNAJB2',\n",
       " 'SNX15',\n",
       " 'DIPK2B',\n",
       " 'TBCA',\n",
       " 'GP5',\n",
       " 'YWHAQ',\n",
       " 'PDE5A',\n",
       " 'DTD1',\n",
       " 'DDI2',\n",
       " 'ADH1B',\n",
       " 'ST13',\n",
       " 'INHBB',\n",
       " 'ERP29',\n",
       " 'PHYKPL',\n",
       " 'MOCS2',\n",
       " 'AFAP1',\n",
       " 'SPART',\n",
       " 'HEG1',\n",
       " 'BMPER',\n",
       " 'PDIA3',\n",
       " 'DCTD',\n",
       " 'MFAP4',\n",
       " 'BMP10',\n",
       " 'SPINK2',\n",
       " 'EPHA4',\n",
       " 'ACHE',\n",
       " 'CHAD',\n",
       " 'UBXN1',\n",
       " 'TNFRSF17',\n",
       " 'SLC9A3R1',\n",
       " 'LZTFL1',\n",
       " 'ARHGAP45',\n",
       " 'AMOT',\n",
       " 'CD72',\n",
       " 'CELSR2',\n",
       " 'GIMAP7',\n",
       " 'SDK2',\n",
       " 'GHR',\n",
       " 'RABEP1',\n",
       " 'CD300A',\n",
       " 'SEMA3G',\n",
       " 'CRELD1',\n",
       " 'RIDA',\n",
       " 'SFRP4',\n",
       " 'MXRA8',\n",
       " 'APPL2',\n",
       " 'MYOM3',\n",
       " 'FGFR4',\n",
       " 'TNFAIP8L2',\n",
       " 'PTRHD1',\n",
       " 'COL5A1',\n",
       " 'FUOM',\n",
       " 'AKAP12',\n",
       " 'CTSE',\n",
       " 'SCGB3A1',\n",
       " 'TPD52L2',\n",
       " 'NAGPA',\n",
       " 'UROD',\n",
       " 'GMPR2',\n",
       " 'SNCA',\n",
       " 'GLRX5',\n",
       " 'KCTD5',\n",
       " 'UPK3BL1',\n",
       " 'TRIM24',\n",
       " 'CTAG1A_CTAG1B',\n",
       " 'FUT1',\n",
       " 'HRAS',\n",
       " 'TET2',\n",
       " 'COL4A4',\n",
       " 'TCN1',\n",
       " 'KLKB1',\n",
       " 'QSOX1',\n",
       " 'CEACAM18',\n",
       " 'EFCAB2',\n",
       " 'NEK7',\n",
       " 'NFKB2',\n",
       " 'CEACAM20',\n",
       " 'RGL2',\n",
       " 'SEPTIN7',\n",
       " 'SAP18',\n",
       " 'ARAF',\n",
       " 'GABARAPL1',\n",
       " 'SAT2',\n",
       " 'ARHGAP30',\n",
       " 'TRDMT1',\n",
       " 'ID4',\n",
       " 'PKN3',\n",
       " 'MAPKAPK2',\n",
       " 'TNPO1',\n",
       " 'TAP1',\n",
       " 'TCP11',\n",
       " 'ITGAX',\n",
       " 'IFIT3',\n",
       " 'ACADM',\n",
       " 'CEP290',\n",
       " 'TAB2',\n",
       " 'GAS2',\n",
       " 'RPE',\n",
       " 'ZNF75D',\n",
       " 'LSM8',\n",
       " 'CENPJ',\n",
       " 'CINP',\n",
       " 'RNF43',\n",
       " 'IFIT1',\n",
       " 'CA7',\n",
       " 'RNF4',\n",
       " 'CENPF',\n",
       " 'TPPP2',\n",
       " 'IL9',\n",
       " 'PAFAH2',\n",
       " 'EPN1',\n",
       " 'COL9A2',\n",
       " 'PPIE',\n",
       " 'TLR2',\n",
       " 'MNAT1',\n",
       " 'ERI1',\n",
       " 'CD3E',\n",
       " 'MAGEA3',\n",
       " 'ALMS1',\n",
       " 'PPP1R12B',\n",
       " 'VPS28',\n",
       " 'PTTG1',\n",
       " 'MORF4L1',\n",
       " 'KIAA1549',\n",
       " 'SPRR1B',\n",
       " 'SLK',\n",
       " 'TK1',\n",
       " 'OFD1',\n",
       " 'KIAA1549L',\n",
       " 'MTHFSD',\n",
       " 'EVPL',\n",
       " 'GADD45B',\n",
       " 'TIGIT',\n",
       " 'CCND2',\n",
       " 'BRD1',\n",
       " 'SHPK',\n",
       " 'VSTM2B',\n",
       " 'TEX33',\n",
       " 'GUCY2C',\n",
       " 'CDH22',\n",
       " 'SERPINH1',\n",
       " 'RAPGEF2',\n",
       " 'PRUNE2',\n",
       " 'MTUS1',\n",
       " 'TMED1',\n",
       " 'GTF2IRD1',\n",
       " 'CASP4',\n",
       " 'OGT',\n",
       " 'RAD51',\n",
       " 'TXK',\n",
       " 'PARD3',\n",
       " 'CD82',\n",
       " 'BCHE',\n",
       " 'SERPINF2',\n",
       " 'SERPINA1',\n",
       " 'SERPINA4',\n",
       " 'SGSH',\n",
       " 'CFB',\n",
       " 'NPC2',\n",
       " 'PRDX2',\n",
       " 'TXN',\n",
       " 'CYB5R2',\n",
       " 'MST1',\n",
       " 'CAT',\n",
       " 'CTBS',\n",
       " 'SERPINF1',\n",
       " 'BRAP',\n",
       " 'HPSE',\n",
       " 'SERPINA5',\n",
       " 'F11',\n",
       " 'MBL2',\n",
       " 'CFHR5',\n",
       " 'IST1',\n",
       " 'PGLYRP2',\n",
       " 'CWC15',\n",
       " 'PALM',\n",
       " 'TTR',\n",
       " 'PSAP',\n",
       " 'ASAH1',\n",
       " 'HGFAC',\n",
       " 'AMOTL2',\n",
       " 'CRISP3',\n",
       " 'NMI',\n",
       " 'EIF2AK2',\n",
       " 'APCS',\n",
       " 'SLURP1',\n",
       " 'DTNB',\n",
       " 'LACRT',\n",
       " 'BTN1A1',\n",
       " 'THTPA',\n",
       " 'MPRIP',\n",
       " 'KLK15',\n",
       " 'RNASE6',\n",
       " 'NAP1L4',\n",
       " 'CDC26',\n",
       " 'LMNB1',\n",
       " 'NUDT16',\n",
       " 'PPBP',\n",
       " 'PF4',\n",
       " 'CFHR2',\n",
       " 'GSR',\n",
       " 'MDH1',\n",
       " 'IL2RG',\n",
       " 'REG3G',\n",
       " 'FNTA',\n",
       " 'RFC4',\n",
       " 'CMIP',\n",
       " 'NUBP1',\n",
       " 'FAM171B',\n",
       " 'NENF',\n",
       " 'AHSA1',\n",
       " 'IL22',\n",
       " 'COMMD9',\n",
       " 'VSIG10',\n",
       " 'KIAA2013',\n",
       " 'RCC1',\n",
       " 'ALDH2',\n",
       " 'UNG',\n",
       " 'VPS4B',\n",
       " 'RALY',\n",
       " 'RAB44',\n",
       " 'PXDNL',\n",
       " 'RAB2B',\n",
       " 'VSIG2',\n",
       " 'KIR2DL2',\n",
       " 'USP25',\n",
       " 'UBE2B',\n",
       " 'LARP1',\n",
       " 'S100A3',\n",
       " 'TDP1',\n",
       " 'CAPN3',\n",
       " 'MINDY1',\n",
       " 'SUSD4',\n",
       " 'TADA3',\n",
       " 'TARS1',\n",
       " 'LRRFIP1',\n",
       " 'TG',\n",
       " 'STAM',\n",
       " 'TGFB2',\n",
       " 'LTB',\n",
       " 'LUZP2',\n",
       " 'MAMDC4',\n",
       " 'HSPA2',\n",
       " 'BCL7B',\n",
       " 'CCDC134',\n",
       " 'GPRC5C',\n",
       " 'LAMTOR5',\n",
       " 'MTSS2',\n",
       " 'NDST1',\n",
       " 'GABARAP',\n",
       " 'CHCHD6',\n",
       " 'NACC1',\n",
       " 'AP1G2',\n",
       " 'TRIM58',\n",
       " 'SLC13A1',\n",
       " 'GPD1',\n",
       " 'SMAD2',\n",
       " 'SMAD3',\n",
       " 'GLYR1',\n",
       " 'SMPDL3B',\n",
       " 'SNX2',\n",
       " 'ARG2',\n",
       " 'SDCCAG8',\n",
       " 'TMEM106A',\n",
       " 'ACRBP',\n",
       " 'DUSP13',\n",
       " 'DYNC1H1',\n",
       " 'EDDM3B',\n",
       " 'DYNLT3',\n",
       " 'WDR46',\n",
       " 'PCDHB15',\n",
       " 'EPGN',\n",
       " 'DHPS',\n",
       " 'IMMT',\n",
       " 'PRC1',\n",
       " 'PPP1CC',\n",
       " 'ERN1',\n",
       " 'ZNRF4',\n",
       " 'ZNF830',\n",
       " 'YJU2',\n",
       " 'PCSK7',\n",
       " 'INSL4',\n",
       " 'ENOPH1',\n",
       " 'ITIH5',\n",
       " 'ENSA',\n",
       " 'TMPRSS11D',\n",
       " 'FTCD',\n",
       " 'PLSCR3',\n",
       " 'SEPTIN8',\n",
       " 'PRKAR2A',\n",
       " 'SMTN',\n",
       " 'NFU1',\n",
       " 'PBXIP1',\n",
       " 'HIP1R',\n",
       " 'ZFYVE19',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_fatures = proteomics + risk_factors + PRS\n",
    "used_fatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-17 10:31:08</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:13.18        </td></tr>\n",
       "<tr><td>Memory:      </td><td>25.2/50.1 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=23<br>Bracket: Iter 8.000: 0.7877980470657349 | Iter 4.000: 0.7754430770874023 | Iter 2.000: 0.7596098184585571 | Iter 1.000: 0.7493204474449158<br>Logical resource usage: 8.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  ... 30 more trials not shown (16 PENDING, 13 TERMINATED)\n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">     train_loop_config/hi\n",
       "dden_size</th><th style=\"text-align: right;\">  train_loop_config/nu\n",
       "m_resblocks</th><th style=\"text-align: right;\">            train_loop_config/we\n",
       "ight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ptl/val_loss</th><th style=\"text-align: right;\">  ptl/val_auc</th><th style=\"text-align: right;\">  ptl/train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_f5695_00023</td><td>RUNNING   </td><td>172.26.79.196:22643</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.000507831</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00024</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000125219</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00025</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">0.00174492 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00026</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">0.00895851 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00027</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">0.00294267 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00028</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000854914</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00029</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">0.00146755 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00030</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">0.000432065</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00031</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">0.00182457 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00032</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">0.000679674</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00033</td><td>PENDING   </td><td>                   </td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">0.00066463 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00000</td><td>TERMINATED</td><td>172.26.79.196:16464</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">0.00507655 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         32.9575</td><td style=\"text-align: right;\">      0.70128 </td><td style=\"text-align: right;\">     0.79197 </td><td style=\"text-align: right;\">        0.527683</td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00001</td><td>TERMINATED</td><td>172.26.79.196:16465</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">0.00396373 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         22.5703</td><td style=\"text-align: right;\">      0.466172</td><td style=\"text-align: right;\">     0.7794  </td><td style=\"text-align: right;\">        0.523887</td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00002</td><td>TERMINATED</td><td>172.26.79.196:17053</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">0.00569178 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         11.7803</td><td style=\"text-align: right;\">      0.438638</td><td style=\"text-align: right;\">     0.749838</td><td style=\"text-align: right;\">        0.842654</td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00003</td><td>TERMINATED</td><td>172.26.79.196:17192</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000315178</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.339 </td><td style=\"text-align: right;\">      0.374255</td><td style=\"text-align: right;\">     0.74932 </td><td style=\"text-align: right;\">        0.659008</td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00004</td><td>TERMINATED</td><td>172.26.79.196:17433</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.0043784  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         13.5027</td><td style=\"text-align: right;\">      0.777512</td><td style=\"text-align: right;\">     0.753979</td><td style=\"text-align: right;\">        0.562013</td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00005</td><td>TERMINATED</td><td>172.26.79.196:17736</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">0.00504085 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         17.5874</td><td style=\"text-align: right;\">      0.546639</td><td style=\"text-align: right;\">     0.762054</td><td style=\"text-align: right;\">        0.535776</td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00006</td><td>TERMINATED</td><td>172.26.79.196:18005</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">0.00633679 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.7645</td><td style=\"text-align: right;\">      0.964338</td><td style=\"text-align: right;\">     0.733821</td><td style=\"text-align: right;\">        0.630443</td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00007</td><td>TERMINATED</td><td>172.26.79.196:18299</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">0.000201708</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         14.2892</td><td style=\"text-align: right;\">      0.386839</td><td style=\"text-align: right;\">     0.74383 </td><td style=\"text-align: right;\">        0.335793</td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00008</td><td>TERMINATED</td><td>172.26.79.196:18474</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">0.00144212 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.3116</td><td style=\"text-align: right;\">      0.876872</td><td style=\"text-align: right;\">     0.746989</td><td style=\"text-align: right;\">        0.642959</td></tr>\n",
       "<tr><td>TorchTrainer_f5695_00009</td><td>TERMINATED</td><td>172.26.79.196:18909</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000103211</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.4178</td><td style=\"text-align: right;\">      0.525991</td><td style=\"text-align: right;\">     0.746537</td><td style=\"text-align: right;\">        0.520461</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=16464)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=16464)\u001b[0m - (ip=172.26.79.196, pid=16585) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 0.0             27198\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 1.0              1607\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 0.0             6810\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 1.0              392\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 0.0             14599\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 1.0               833\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00000_0_hidden_size=512,num_resblocks=5,weight_decay=0.0051_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(TorchTrainer pid=16465)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=16465)\u001b[0m - (ip=172.26.79.196, pid=16593) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 2 | resblocks | Sequential       | 1.3 M \n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 2.8 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 2.8 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m 11.287    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:03<00:00,  0.54it/s]\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 1.0               833\u001b[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m dtype: int64\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m Test : incident_cad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00001_1_hidden_size=512,num_resblocks=4,weight_decay=0.0040_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:   5%|▍         | 5/111 [00:00<00:02, 37.02it/s, v_num=0]\n",
      "Epoch 0:  11%|█         | 12/111 [00:00<00:01, 50.59it/s, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 18/111 [00:00<00:01, 54.46it/s, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 27/111 [00:00<00:01, 60.92it/s, v_num=0]\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:04<00:04,  0.22it/s]\n",
      "Epoch 0:  32%|███▏      | 36/111 [00:00<00:01, 67.28it/s, v_num=0]\n",
      "                                                                           \n",
      "Epoch 0:  33%|███▎      | 37/111 [00:00<00:01, 67.68it/s, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 44/111 [00:00<00:00, 69.41it/s, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 49/111 [00:00<00:00, 64.07it/s, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:00<00:00, 62.19it/s, v_num=0]\n",
      "Epoch 0:   2%|▏         | 2/111 [00:00<00:06, 16.99it/s, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 59/111 [00:00<00:00, 60.85it/s, v_num=0]\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:  59%|█████▉    | 66/111 [00:01<00:00, 61.21it/s, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 81/111 [00:01<00:00, 63.10it/s, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 93/111 [00:01<00:00, 64.35it/s, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 98/111 [00:01<00:00, 61.85it/s, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 105/111 [00:01<00:00, 62.53it/s, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 106/111 [00:01<00:00, 62.48it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 63.33it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 265.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 210.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 169.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 170.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 161.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 167.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 163.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 157.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 154.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 151.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 152.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 152.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 150.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 151.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 149.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 150.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 150.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 148.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 142.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 140.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 139.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 139.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 139.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 140.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 141.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 139.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 141.36it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 140.26it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 55.65it/s, v_num=0, ptl/val_loss=0.496, ptl/val_auc=0.743]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m -----------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 2 | resblocks | Sequential       | 1.1 M \n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 2.6 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 2.6 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m 10.233    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 52.08it/s, v_num=0, ptl/val_loss=0.496, ptl/val_auc=0.743, ptl/train_loss=0.865]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00000_0_hidden_size=512,num_resblocks=5,weight_decay=0.0051_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 1/111 [00:00<00:06, 16.77it/s, v_num=0, ptl/val_loss=0.496, ptl/val_auc=0.743, ptl/train_loss=0.865, ptl/train_auc=0.701]\n",
      "Epoch 0:  96%|█████████▋| 107/111 [00:01<00:00, 63.80it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 63.35it/s, v_num=0]\n",
      "Epoch 1:   5%|▍         | 5/111 [00:00<00:01, 61.28it/s, v_num=0, ptl/val_loss=0.481, ptl/val_auc=0.761, ptl/train_loss=0.848, ptl/train_auc=0.712]\n",
      "Epoch 1:  55%|█████▍    | 61/111 [00:00<00:00, 68.46it/s, v_num=0, ptl/val_loss=0.496, ptl/val_auc=0.743, ptl/train_loss=0.865, ptl/train_auc=0.701]\n",
      "Epoch 1:  63%|██████▎   | 70/111 [00:00<00:00, 70.26it/s, v_num=0, ptl/val_loss=0.496, ptl/val_auc=0.743, ptl/train_loss=0.865, ptl/train_auc=0.701]\n",
      "Epoch 1:  93%|█████████▎| 103/111 [00:01<00:00, 67.53it/s, v_num=0, ptl/val_loss=0.496, ptl/val_auc=0.743, ptl/train_loss=0.865, ptl/train_auc=0.701]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 68.67it/s, v_num=0, ptl/val_loss=0.496, ptl/val_auc=0.743, ptl/train_loss=0.865, ptl/train_auc=0.701]\n",
      "Epoch 2:   1%|          | 1/111 [00:00<00:05, 18.59it/s, v_num=0, ptl/val_loss=0.714, ptl/val_auc=0.763, ptl/train_loss=0.565, ptl/train_auc=0.739]  \n",
      "Epoch 1:  92%|█████████▏| 102/111 [00:01<00:00, 70.81it/s, v_num=0, ptl/val_loss=0.481, ptl/val_auc=0.761, ptl/train_loss=0.848, ptl/train_auc=0.712]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 71.99it/s, v_num=0, ptl/val_loss=0.481, ptl/val_auc=0.761, ptl/train_loss=0.848, ptl/train_auc=0.712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:27:19,348\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  23%|██▎       | 26/111 [00:00<00:01, 56.59it/s, v_num=0, ptl/val_loss=0.714, ptl/val_auc=0.763, ptl/train_loss=0.565, ptl/train_auc=0.739]\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m \u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 96.42it/s] \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.415, ptl/val_auc=0.776, ptl/train_loss=0.548, ptl/train_auc=0.712]          \n",
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 2:  14%|█▍        | 16/111 [00:00<00:01, 61.24it/s, v_num=0, ptl/val_loss=0.714, ptl/val_auc=0.763, ptl/train_loss=0.565, ptl/train_auc=0.739]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Epoch 2:  49%|████▊     | 54/111 [00:00<00:00, 60.75it/s, v_num=0, ptl/val_loss=0.714, ptl/val_auc=0.763, ptl/train_loss=0.565, ptl/train_auc=0.739]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Epoch 2:   5%|▌         | 6/111 [00:00<00:02, 51.80it/s, v_num=0, ptl/val_loss=0.415, ptl/val_auc=0.776, ptl/train_loss=0.548, ptl/train_auc=0.752]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 2:   9%|▉         | 10/111 [00:00<00:01, 65.26it/s, v_num=0, ptl/val_loss=0.714, ptl/val_auc=0.763, ptl/train_loss=0.565, ptl/train_auc=0.739]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 2:  56%|█████▌    | 62/111 [00:01<00:00, 61.93it/s, v_num=0, ptl/val_loss=0.714, ptl/val_auc=0.763, ptl/train_loss=0.565, ptl/train_auc=0.739]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 2:  92%|█████████▏| 102/111 [00:01<00:00, 67.90it/s, v_num=0, ptl/val_loss=0.714, ptl/val_auc=0.763, ptl/train_loss=0.565, ptl/train_auc=0.739]\n",
      "Epoch 2:  98%|█████████▊| 109/111 [00:01<00:00, 67.58it/s, v_num=0, ptl/val_loss=0.714, ptl/val_auc=0.763, ptl/train_loss=0.565, ptl/train_auc=0.739]\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 67.68it/s, v_num=0, ptl/val_loss=0.714, ptl/val_auc=0.763, ptl/train_loss=0.565, ptl/train_auc=0.739]\n",
      "Epoch 2:  77%|███████▋  | 86/111 [00:01<00:00, 74.35it/s, v_num=0, ptl/val_loss=0.415, ptl/val_auc=0.776, ptl/train_loss=0.548, ptl/train_auc=0.752]\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "Epoch 2:  95%|█████████▌| 106/111 [00:01<00:00, 77.86it/s, v_num=0, ptl/val_loss=0.415, ptl/val_auc=0.776, ptl/train_loss=0.548, ptl/train_auc=0.752]\n",
      "Epoch 3:   3%|▎         | 3/111 [00:00<00:02, 45.06it/s, v_num=0, ptl/val_loss=0.449, ptl/val_auc=0.775, ptl/train_loss=0.550, ptl/train_auc=0.757]  \n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 78.68it/s, v_num=0, ptl/val_loss=0.415, ptl/val_auc=0.776, ptl/train_loss=0.548, ptl/train_auc=0.752]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 126.65it/s]\u001b[A\u001b[32m [repeated 124x across cluster]\u001b[0m\n",
      "Epoch 3:   5%|▍         | 5/111 [00:00<00:01, 57.55it/s, v_num=0, ptl/val_loss=0.629, ptl/val_auc=0.776, ptl/train_loss=0.544, ptl/train_auc=0.767]  \n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 130.56it/s]\u001b[A\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 67.03it/s, v_num=0, ptl/val_loss=0.629, ptl/val_auc=0.776, ptl/train_loss=0.548, ptl/train_auc=0.752]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 3:  43%|████▎     | 48/111 [00:00<00:00, 69.34it/s, v_num=0, ptl/val_loss=0.449, ptl/val_auc=0.775, ptl/train_loss=0.550, ptl/train_auc=0.757]\n",
      "Epoch 3:  43%|████▎     | 48/111 [00:00<00:00, 69.32it/s, v_num=0, ptl/val_loss=0.449, ptl/val_auc=0.775, ptl/train_loss=0.550, ptl/train_auc=0.757]\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 64.55it/s, v_num=0, ptl/val_loss=0.629, ptl/val_auc=0.776, ptl/train_loss=0.544, ptl/train_auc=0.752]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 3:  95%|█████████▌| 106/111 [00:01<00:00, 75.21it/s, v_num=0, ptl/val_loss=0.449, ptl/val_auc=0.775, ptl/train_loss=0.550, ptl/train_auc=0.757]\n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 76.17it/s, v_num=0, ptl/val_loss=0.449, ptl/val_auc=0.775, ptl/train_loss=0.550, ptl/train_auc=0.757]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:27:22,570\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  98%|█████████▊| 109/111 [00:01<00:00, 82.03it/s, v_num=0, ptl/val_loss=0.629, ptl/val_auc=0.776, ptl/train_loss=0.544, ptl/train_auc=0.767]\n",
      "Epoch 3:  72%|███████▏  | 80/111 [00:01<00:00, 78.20it/s, v_num=0, ptl/val_loss=0.629, ptl/val_auc=0.776, ptl/train_loss=0.544, ptl/train_auc=0.767]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00000_0_hidden_size=512,num_resblocks=5,weight_decay=0.0051_2024-04-17_10-26-56/checkpoint_000003)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   5%|▍         | 5/111 [00:00<00:02, 52.54it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.782, ptl/train_loss=0.549, ptl/train_auc=0.766]  \n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 67.84it/s, v_num=0, ptl/val_loss=0.466, ptl/val_auc=0.779, ptl/train_loss=0.524, ptl/train_auc=0.767]\n",
      "Epoch 4:  27%|██▋       | 30/111 [00:00<00:01, 58.58it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.782, ptl/train_loss=0.549, ptl/train_auc=0.766]\n",
      "Epoch 4:  97%|█████████▋| 108/111 [00:01<00:00, 70.53it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.782, ptl/train_loss=0.549, ptl/train_auc=0.766]\n",
      "Epoch 4: 100%|██████████| 111/111 [00:01<00:00, 70.88it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.782, ptl/train_loss=0.549, ptl/train_auc=0.766]\n",
      "\u001b[36m(RayTrainWorker pid=16593)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 5:   1%|          | 1/111 [00:00<00:04, 23.04it/s, v_num=0, ptl/val_loss=0.562, ptl/val_auc=0.786, ptl/train_loss=0.533, ptl/train_auc=0.774]  \n",
      "Epoch 4:  67%|██████▋   | 74/111 [00:01<00:00, 66.54it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.782, ptl/train_loss=0.549, ptl/train_auc=0.766]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 5:  11%|█         | 12/111 [00:00<00:01, 78.50it/s, v_num=0, ptl/val_loss=0.562, ptl/val_auc=0.786, ptl/train_loss=0.533, ptl/train_auc=0.774]\n",
      "Epoch 4:  74%|███████▍  | 82/111 [00:01<00:00, 66.47it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.782, ptl/train_loss=0.549, ptl/train_auc=0.766]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 5:  21%|██        | 23/111 [00:00<00:00, 90.65it/s, v_num=0, ptl/val_loss=0.562, ptl/val_auc=0.786, ptl/train_loss=0.533, ptl/train_auc=0.774]\n",
      "Epoch 4:  62%|██████▏   | 69/111 [00:01<00:00, 67.58it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.782, ptl/train_loss=0.549, ptl/train_auc=0.766]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 4:  89%|████████▉ | 99/111 [00:01<00:00, 69.10it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.782, ptl/train_loss=0.549, ptl/train_auc=0.766]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "Epoch 4:  68%|██████▊   | 75/111 [00:01<00:00, 66.34it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.782, ptl/train_loss=0.549, ptl/train_auc=0.766]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 82.46it/s, v_num=0, ptl/val_loss=0.629, ptl/val_auc=0.776, ptl/train_loss=0.544, ptl/train_auc=0.767]\n",
      "Epoch 5:  94%|█████████▎| 104/111 [00:01<00:00, 88.55it/s, v_num=0, ptl/val_loss=0.562, ptl/val_auc=0.786, ptl/train_loss=0.533, ptl/train_auc=0.774]\n",
      "Epoch 5: 100%|██████████| 111/111 [00:01<00:00, 89.01it/s, v_num=0, ptl/val_loss=0.562, ptl/val_auc=0.786, ptl/train_loss=0.533, ptl/train_auc=0.774]\n",
      "Epoch 5:  85%|████████▍ | 94/111 [00:01<00:00, 88.10it/s, v_num=0, ptl/val_loss=0.562, ptl/val_auc=0.786, ptl/train_loss=0.533, ptl/train_auc=0.774]\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:27:26,129\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.680, ptl/val_auc=0.788, ptl/train_loss=0.536, ptl/train_auc=0.774]          \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 135.24it/s]\u001b[A\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 136.95it/s]\u001b[A\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 5: 100%|██████████| 111/111 [00:01<00:00, 75.10it/s, v_num=0, ptl/val_loss=0.680, ptl/val_auc=0.788, ptl/train_loss=0.533, ptl/train_auc=0.774]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 5: 100%|██████████| 111/111 [00:01<00:00, 70.50it/s, v_num=0, ptl/val_loss=0.680, ptl/val_auc=0.788, ptl/train_loss=0.536, ptl/train_auc=0.774]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 6: 100%|██████████| 111/111 [00:01<00:00, 82.88it/s, v_num=0, ptl/val_loss=0.680, ptl/val_auc=0.788, ptl/train_loss=0.536, ptl/train_auc=0.780]\n",
      "Epoch 6:  91%|█████████ | 101/111 [00:01<00:00, 81.07it/s, v_num=0, ptl/val_loss=0.680, ptl/val_auc=0.788, ptl/train_loss=0.536, ptl/train_auc=0.780]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00000_0_hidden_size=512,num_resblocks=5,weight_decay=0.0051_2024-04-17_10-26-56/checkpoint_000006)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 111/111 [00:01<00:00, 60.85it/s, v_num=0, ptl/val_loss=0.562, ptl/val_auc=0.786, ptl/train_loss=0.533, ptl/train_auc=0.766]\n",
      "Epoch 7:   3%|▎         | 3/111 [00:00<00:03, 32.23it/s, v_num=0, ptl/val_loss=0.484, ptl/val_auc=0.790, ptl/train_loss=0.523, ptl/train_auc=0.785]  \n",
      "Epoch 7:   6%|▋         | 7/111 [00:00<00:02, 39.09it/s, v_num=0, ptl/val_loss=0.484, ptl/val_auc=0.790, ptl/train_loss=0.523, ptl/train_auc=0.785]\n",
      "Epoch 7:  10%|▉         | 11/111 [00:00<00:02, 37.63it/s, v_num=0, ptl/val_loss=0.484, ptl/val_auc=0.790, ptl/train_loss=0.523, ptl/train_auc=0.785]\n",
      "Epoch 7:  95%|█████████▍| 105/111 [00:01<00:00, 73.39it/s, v_num=0, ptl/val_loss=0.484, ptl/val_auc=0.790, ptl/train_loss=0.523, ptl/train_auc=0.785]\n",
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Epoch 7: 100%|██████████| 111/111 [00:01<00:00, 74.80it/s, v_num=0, ptl/val_loss=0.484, ptl/val_auc=0.790, ptl/train_loss=0.523, ptl/train_auc=0.785]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:27:29,609\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 111/111 [00:01<00:00, 64.05it/s, v_num=0, ptl/val_loss=0.645, ptl/val_auc=0.790, ptl/train_loss=0.532, ptl/train_auc=0.785]\n",
      "Epoch 8:   4%|▎         | 4/111 [00:00<00:02, 46.84it/s, v_num=0, ptl/val_loss=0.645, ptl/val_auc=0.790, ptl/train_loss=0.532, ptl/train_auc=0.788]  \n",
      "Epoch 6:  44%|████▍     | 49/111 [00:00<00:00, 78.16it/s, v_num=0, ptl/val_loss=0.680, ptl/val_auc=0.788, ptl/train_loss=0.536, ptl/train_auc=0.780]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 7:  15%|█▌        | 17/111 [00:00<00:02, 41.77it/s, v_num=0, ptl/val_loss=0.484, ptl/val_auc=0.790, ptl/train_loss=0.523, ptl/train_auc=0.785]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 7:  72%|███████▏  | 80/111 [00:01<00:00, 71.78it/s, v_num=0, ptl/val_loss=0.484, ptl/val_auc=0.790, ptl/train_loss=0.523, ptl/train_auc=0.785]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Epoch 8:  12%|█▏        | 13/111 [00:00<00:01, 66.64it/s, v_num=0, ptl/val_loss=0.645, ptl/val_auc=0.790, ptl/train_loss=0.532, ptl/train_auc=0.788]\n",
      "Epoch 8:  12%|█▏        | 13/111 [00:00<00:01, 66.57it/s, v_num=0, ptl/val_loss=0.645, ptl/val_auc=0.790, ptl/train_loss=0.532, ptl/train_auc=0.788]\n",
      "Epoch 7:  73%|███████▎  | 81/111 [00:01<00:00, 72.05it/s, v_num=0, ptl/val_loss=0.484, ptl/val_auc=0.790, ptl/train_loss=0.523, ptl/train_auc=0.785]\n",
      "Epoch 8:  36%|███▌      | 40/111 [00:00<00:00, 80.05it/s, v_num=0, ptl/val_loss=0.645, ptl/val_auc=0.790, ptl/train_loss=0.532, ptl/train_auc=0.788]\n",
      "Epoch 8:  49%|████▊     | 54/111 [00:00<00:00, 77.56it/s, v_num=0, ptl/val_loss=0.645, ptl/val_auc=0.790, ptl/train_loss=0.532, ptl/train_auc=0.788]\n",
      "Epoch 8:  52%|█████▏    | 58/111 [00:00<00:00, 72.27it/s, v_num=0, ptl/val_loss=0.645, ptl/val_auc=0.790, ptl/train_loss=0.532, ptl/train_auc=0.788]\n",
      "Epoch 8:  72%|███████▏  | 80/111 [00:01<00:00, 71.81it/s, v_num=0, ptl/val_loss=0.645, ptl/val_auc=0.790, ptl/train_loss=0.532, ptl/train_auc=0.788]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 8: 100%|██████████| 111/111 [00:01<00:00, 78.13it/s, v_num=0, ptl/val_loss=0.645, ptl/val_auc=0.790, ptl/train_loss=0.532, ptl/train_auc=0.788]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 131.20it/s]\u001b[A\u001b[32m [repeated 75x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.788]          \n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 135.15it/s]\u001b[A\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 8: 100%|██████████| 111/111 [00:01<00:00, 67.07it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.532, ptl/train_auc=0.788]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 8: 100%|██████████| 111/111 [00:01<00:00, 64.09it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.788]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=17053)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=17053)\u001b[0m - (ip=172.26.79.196, pid=17107) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  59%|█████▊    | 65/111 [00:00<00:00, 76.51it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\n",
      "Epoch 9:  76%|███████▌  | 84/111 [00:01<00:00, 79.78it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 9:  83%|████████▎ | 92/111 [00:01<00:00, 79.53it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\n",
      "Epoch 9:  87%|████████▋ | 97/111 [00:01<00:00, 79.13it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\n",
      "Epoch 9:  93%|█████████▎| 103/111 [00:01<00:00, 76.25it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\n",
      "Epoch 9:  94%|█████████▎| 104/111 [00:01<00:00, 76.34it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\n",
      "Epoch 9: 100%|██████████| 111/111 [00:01<00:00, 75.95it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=16585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00000_0_hidden_size=512,num_resblocks=5,weight_decay=0.0051_2024-04-17_10-26-56/checkpoint_000009)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2024-04-17 10:27:33,293\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00002_2_hidden_size=512,num_resblocks=5,weight_decay=0.0057_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 1.0               833\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "Epoch 9:  82%|████████▏ | 91/111 [00:01<00:00, 79.61it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 9:  13%|█▎        | 14/111 [00:00<00:01, 61.81it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 9:  67%|██████▋   | 74/111 [00:00<00:00, 78.16it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 9:  50%|████▉     | 55/111 [00:00<00:00, 74.38it/s, v_num=0, ptl/val_loss=0.692, ptl/val_auc=0.791, ptl/train_loss=0.521, ptl/train_auc=0.791]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 166.20it/s]\u001b[A\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 167.89it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 9: 100%|██████████| 111/111 [00:01<00:00, 66.70it/s, v_num=0, ptl/val_loss=0.701, ptl/val_auc=0.792, ptl/train_loss=0.521, ptl/train_auc=0.791]\n",
      "Epoch 9: 100%|██████████| 111/111 [00:01<00:00, 63.89it/s, v_num=0, ptl/val_loss=0.701, ptl/val_auc=0.792, ptl/train_loss=0.528, ptl/train_auc=0.791]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 2 | resblocks | Sequential       | 1.3 M \n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 2.8 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 2.8 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m 11.287    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:   7%|▋         | 8/111 [00:00<00:01, 62.31it/s, v_num=0]\n",
      "Epoch 0:   7%|▋         | 8/111 [00:00<00:01, 62.16it/s, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 17/111 [00:00<00:01, 72.93it/s, v_num=0]\n",
      "Epoch 0:  20%|█▉        | 22/111 [00:00<00:01, 66.80it/s, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 32/111 [00:00<00:01, 74.26it/s, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 91/111 [00:01<00:00, 78.86it/s, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 100/111 [00:01<00:00, 79.48it/s, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 107/111 [00:01<00:00, 79.10it/s, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 108/111 [00:01<00:00, 79.28it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 79.91it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 70.23it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.750]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 66.45it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.750, ptl/train_loss=0.843]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=17107)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00002_2_hidden_size=512,num_resblocks=5,weight_decay=0.0057_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=17192)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=17192)\u001b[0m - (ip=172.26.79.196, pid=17317) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 1.0               833\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Epoch 0:  50%|█████     | 56/111 [00:00<00:00, 76.06it/s, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 41/111 [00:00<00:00, 76.67it/s, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 99/111 [00:01<00:00, 79.63it/s, v_num=0]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 173.04it/s]\u001b[A\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 175.30it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00003_3_hidden_size=1024,num_resblocks=1,weight_decay=0.0003_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 1 | fc1       | Linear           | 3.0 M \n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 2 | resblocks | Sequential       | 1.1 M \n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 3 | fc2       | Linear           | 2.0 K \n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 4.1 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 4.1 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m 16.211    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▎         | 4/111 [00:00<00:01, 57.08it/s, v_num=0]\n",
      "                                                                           \n",
      "Epoch 0:  13%|█▎        | 14/111 [00:00<00:01, 86.76it/s, v_num=0]\n",
      "Epoch 0:  14%|█▎        | 15/111 [00:00<00:01, 85.50it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=17433)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=17433)\u001b[0m - (ip=172.26.79.196, pid=17520) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  17%|█▋        | 19/111 [00:00<00:01, 70.69it/s, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 26/111 [00:00<00:01, 68.92it/s, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 31/111 [00:00<00:01, 69.38it/s, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 38/111 [00:00<00:01, 64.93it/s, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 47/111 [00:00<00:00, 68.66it/s, v_num=0]\n",
      "Epoch 0:  51%|█████▏    | 57/111 [00:00<00:00, 72.38it/s, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 58/111 [00:00<00:00, 72.88it/s, v_num=0]\n",
      "Epoch 0:  61%|██████▏   | 68/111 [00:00<00:00, 76.51it/s, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 69/111 [00:00<00:00, 76.69it/s, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 80/111 [00:00<00:00, 80.07it/s, v_num=0]\n",
      "Epoch 0:  81%|████████  | 90/111 [00:01<00:00, 81.66it/s, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 101/111 [00:01<00:00, 83.88it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 86.01it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m \n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 348.10it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 270.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 224.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 231.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 200.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 199.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 206.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 207.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 208.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 199.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 200.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 199.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 193.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 195.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 191.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 189.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 191.33it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m \n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 190.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 189.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 187.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 187.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 187.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 189.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 191.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 191.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 193.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 194.98it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 196.56it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 75.45it/s, v_num=0, ptl/val_loss=0.374, ptl/val_auc=0.749]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17317)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00003_3_hidden_size=1024,num_resblocks=1,weight_decay=0.0003_2024-04-17_10-26-56/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 67.82it/s, v_num=0, ptl/val_loss=0.374, ptl/val_auc=0.749, ptl/train_loss=0.659]\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00004_4_hidden_size=512,num_resblocks=1,weight_decay=0.0044_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 2 | resblocks | Sequential       | 263 K \n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 1.8 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 1.8 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 7.069     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m 1.0               833\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 3/111 [00:00<00:02, 41.29it/s, v_num=0]\n",
      "Epoch 0:   4%|▎         | 4/111 [00:00<00:04, 21.74it/s, v_num=0]\n",
      "Epoch 0:   5%|▍         | 5/111 [00:00<00:04, 26.03it/s, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 17/111 [00:00<00:01, 57.75it/s, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 27/111 [00:00<00:01, 69.41it/s, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 38/111 [00:00<00:00, 76.53it/s, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 48/111 [00:00<00:00, 80.03it/s, v_num=0]\n",
      "Epoch 0:  50%|█████     | 56/111 [00:00<00:00, 80.47it/s, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 66/111 [00:00<00:00, 82.52it/s, v_num=0]\n",
      "Epoch 0:  60%|██████    | 67/111 [00:00<00:00, 83.00it/s, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 74/111 [00:00<00:00, 82.02it/s, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 82/111 [00:01<00:00, 80.92it/s, v_num=0]\n",
      "Epoch 0:  81%|████████  | 90/111 [00:01<00:00, 81.00it/s, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 98/111 [00:01<00:00, 80.73it/s, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 107/111 [00:01<00:00, 80.96it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 81.92it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 274.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 260.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 278.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 220.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 220.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 222.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 214.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 216.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 217.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 214.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 203.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 204.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 194.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 195.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 195.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 192.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 194.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 196.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 198.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 197.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 199.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 200.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 202.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 201.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 201.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 201.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 199.05it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 199.17it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 69.42it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623]\n",
      "Epoch 1:   3%|▎         | 3/111 [00:00<00:02, 40.04it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00004_4_hidden_size=512,num_resblocks=1,weight_decay=0.0044_2024-04-17_10-26-56/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|▉         | 11/111 [00:00<00:01, 60.06it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  13%|█▎        | 14/111 [00:00<00:01, 50.75it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  21%|██        | 23/111 [00:00<00:01, 59.85it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  29%|██▉       | 32/111 [00:00<00:01, 66.38it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  36%|███▌      | 40/111 [00:00<00:01, 67.80it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  37%|███▋      | 41/111 [00:00<00:01, 68.33it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  43%|████▎     | 48/111 [00:00<00:00, 68.82it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  49%|████▊     | 54/111 [00:00<00:00, 67.92it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  55%|█████▍    | 61/111 [00:00<00:00, 67.80it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  62%|██████▏   | 69/111 [00:01<00:00, 68.36it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  69%|██████▉   | 77/111 [00:01<00:00, 69.68it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  77%|███████▋  | 86/111 [00:01<00:00, 70.42it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  86%|████████▌ | 95/111 [00:01<00:00, 72.09it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1:  92%|█████████▏| 102/111 [00:01<00:00, 71.71it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 73.05it/s, v_num=0, ptl/val_loss=0.421, ptl/val_auc=0.767, ptl/train_loss=0.623, ptl/train_auc=0.760]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 292.18it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 180.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 199.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 214.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 182.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 193.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 199.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 195.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 198.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 197.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 189.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 190.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 193.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 192.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 193.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 193.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 191.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 192.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 194.51it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 193.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 192.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 192.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 192.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 193.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 195.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 198.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 200.04it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 201.10it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 64.19it/s, v_num=0, ptl/val_loss=0.778, ptl/val_auc=0.754, ptl/train_loss=0.562, ptl/train_auc=0.760]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17520)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00004_4_hidden_size=512,num_resblocks=1,weight_decay=0.0044_2024-04-17_10-26-56/checkpoint_000001)\n",
      "\u001b[36m(TorchTrainer pid=17736)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=17736)\u001b[0m - (ip=172.26.79.196, pid=17862) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 1.0               833\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00005_5_hidden_size=512,num_resblocks=3,weight_decay=0.0050_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 2 | resblocks | Sequential       | 791 K \n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 2.3 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 2.3 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m 9.178     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:   8%|▊         | 9/111 [00:00<00:01, 76.96it/s, v_num=0]\n",
      "Epoch 0:   9%|▉         | 10/111 [00:00<00:01, 76.35it/s, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 18/111 [00:00<00:01, 78.59it/s, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 27/111 [00:00<00:01, 80.16it/s, v_num=0]\n",
      "Epoch 0:  31%|███       | 34/111 [00:00<00:00, 77.47it/s, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 41/111 [00:00<00:00, 75.86it/s, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 48/111 [00:00<00:00, 75.60it/s, v_num=0]\n",
      "Epoch 0:  50%|████▉     | 55/111 [00:00<00:00, 73.63it/s, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 62/111 [00:00<00:00, 73.04it/s, v_num=0]\n",
      "Epoch 0:  60%|██████    | 67/111 [00:00<00:00, 70.06it/s, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 76/111 [00:01<00:00, 71.64it/s, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 86/111 [00:01<00:00, 74.05it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  86%|████████▌ | 95/111 [00:01<00:00, 75.28it/s, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 105/111 [00:01<00:00, 76.84it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 77.95it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 224.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 189.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 202.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 193.52it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m \n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 168.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 174.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 172.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 165.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 165.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 162.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 163.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 164.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 164.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 164.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 166.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 165.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 167.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 169.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 170.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 172.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 172.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 173.59it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m \n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 174.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 174.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 175.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 175.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 176.64it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 178.06it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 68.67it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753]\n",
      "Epoch 1:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00005_5_hidden_size=512,num_resblocks=3,weight_decay=0.0050_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   5%|▍         | 5/111 [00:00<00:02, 43.58it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  12%|█▏        | 13/111 [00:00<00:01, 61.99it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  19%|█▉        | 21/111 [00:00<00:01, 68.11it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  20%|█▉        | 22/111 [00:00<00:01, 68.23it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  29%|██▉       | 32/111 [00:00<00:01, 76.07it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=18005)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=18005)\u001b[0m - (ip=172.26.79.196, pid=18068) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 37/111 [00:00<00:01, 72.18it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  43%|████▎     | 48/111 [00:00<00:00, 75.87it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  53%|█████▎    | 59/111 [00:00<00:00, 80.85it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  60%|██████    | 67/111 [00:00<00:00, 80.76it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  61%|██████▏   | 68/111 [00:00<00:00, 80.75it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  68%|██████▊   | 75/111 [00:00<00:00, 80.49it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  68%|██████▊   | 76/111 [00:00<00:00, 80.41it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  76%|███████▌  | 84/111 [00:01<00:00, 80.66it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  82%|████████▏ | 91/111 [00:01<00:00, 80.13it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1:  92%|█████████▏| 102/111 [00:01<00:00, 81.82it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 84.01it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.753, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 291.53it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 254.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 269.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 260.28it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m \n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 212.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 226.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 235.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 232.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 223.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 223.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 208.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 206.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 207.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 196.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 197.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 197.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 193.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 193.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 194.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 190.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 190.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 190.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 190.15it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m \n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 191.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 192.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 190.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 191.15it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 192.17it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 74.23it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.707, ptl/train_auc=0.751]\n",
      "Epoch 2:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.751]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00005_5_hidden_size=512,num_resblocks=3,weight_decay=0.0050_2024-04-17_10-26-56/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   7%|▋         | 8/111 [00:00<00:01, 77.81it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  17%|█▋        | 19/111 [00:00<00:00, 93.35it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  26%|██▌       | 29/111 [00:00<00:00, 95.56it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  32%|███▏      | 35/111 [00:00<00:00, 95.83it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  39%|███▊      | 43/111 [00:00<00:00, 84.86it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  40%|███▉      | 44/111 [00:00<00:00, 84.92it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  40%|███▉      | 44/111 [00:00<00:00, 84.88it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  49%|████▊     | 54/111 [00:00<00:00, 87.31it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  58%|█████▊    | 64/111 [00:00<00:00, 89.21it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  68%|██████▊   | 75/111 [00:00<00:00, 91.04it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  76%|███████▌  | 84/111 [00:00<00:00, 91.42it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  77%|███████▋  | 85/111 [00:00<00:00, 91.32it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  84%|████████▍ | 93/111 [00:01<00:00, 89.92it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m dtype: int64\n",
      "Epoch 2:  84%|████████▍ | 93/111 [00:01<00:00, 89.89it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  90%|█████████ | 100/111 [00:01<00:00, 88.39it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2:  99%|█████████▉| 110/111 [00:01<00:00, 88.79it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 88.82it/s, v_num=0, ptl/val_loss=0.548, ptl/val_auc=0.763, ptl/train_loss=0.557, ptl/train_auc=0.768]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 210.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 147.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 158.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 68.37it/s] \u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 60.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 68.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 69.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 75.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 76.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 76.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 77.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 79.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 78.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 82.52it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:28:10,737\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 83.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 84.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 84.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 85.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 84.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 85.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 86.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 86.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 88.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 88.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 89.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 89.16it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 89.28it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 69.70it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.557, ptl/train_auc=0.768]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00005_5_hidden_size=512,num_resblocks=3,weight_decay=0.0050_2024-04-17_10-26-56/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 66.38it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.768]\n",
      "Epoch 3:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.768]          \n",
      "Epoch 3:   3%|▎         | 3/111 [00:00<00:03, 29.15it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:   7%|▋         | 8/111 [00:00<00:02, 42.07it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  11%|█         | 12/111 [00:00<00:02, 40.85it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  12%|█▏        | 13/111 [00:00<00:02, 41.46it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  16%|█▌        | 18/111 [00:00<00:02, 43.30it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  20%|█▉        | 22/111 [00:00<00:02, 42.92it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  24%|██▍       | 27/111 [00:00<00:01, 43.89it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  30%|██▉       | 33/111 [00:00<00:01, 45.77it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  37%|███▋      | 41/111 [00:00<00:01, 49.18it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  44%|████▍     | 49/111 [00:00<00:01, 52.28it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  50%|████▉     | 55/111 [00:01<00:01, 53.10it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  57%|█████▋    | 63/111 [00:01<00:00, 55.17it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  63%|██████▎   | 70/111 [00:01<00:00, 56.82it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  71%|███████   | 79/111 [00:01<00:00, 58.71it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  80%|████████  | 89/111 [00:01<00:00, 61.33it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m \u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00006_6_hidden_size=256,num_resblocks=3,weight_decay=0.0063_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  88%|████████▊ | 98/111 [00:01<00:00, 63.22it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  98%|█████████▊| 109/111 [00:01<00:00, 66.03it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3:  99%|█████████▉| 110/111 [00:01<00:00, 66.35it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 66.60it/s, v_num=0, ptl/val_loss=0.390, ptl/val_auc=0.756, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 299.53it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 248.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 254.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 237.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 204.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 203.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 201.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 207.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 209.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 200.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 200.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 203.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 195.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 188.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 180.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 178.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 177.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 170.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 163.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 160.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 156.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 151.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 146.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 142.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 143.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 141.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 141.05it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 137.69it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 58.17it/s, v_num=0, ptl/val_loss=0.547, ptl/val_auc=0.762, ptl/train_loss=0.539, ptl/train_auc=0.780]\n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 55.54it/s, v_num=0, ptl/val_loss=0.547, ptl/val_auc=0.762, ptl/train_loss=0.536, ptl/train_auc=0.780]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=17862)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00005_5_hidden_size=512,num_resblocks=3,weight_decay=0.0050_2024-04-17_10-26-56/checkpoint_000003)\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m 1 | fc1       | Linear           | 748 K \n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m 2 | resblocks | Sequential       | 198 K \n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m 3 | fc2       | Linear           | 514   \n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m 953 K     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m 953 K     Total params\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m 3.814     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 0:   2%|▏         | 2/111 [00:00<00:03, 31.43it/s, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 13/111 [00:00<00:01, 74.53it/s, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 51/111 [00:00<00:00, 87.50it/s, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 91/111 [00:00<00:00, 91.61it/s, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 102/111 [00:01<00:00, 93.13it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 93.81it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00006_6_hidden_size=256,num_resblocks=3,weight_decay=0.0063_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=18068)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  27%|██▋       | 30/111 [00:00<00:01, 79.85it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 77.91it/s, v_num=0, ptl/val_loss=0.964, ptl/val_auc=0.734, ptl/train_loss=0.630]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=18299)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=18299)\u001b[0m - (ip=172.26.79.196, pid=18484) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m dtype: int64\n",
      "Epoch 0:  54%|█████▍    | 60/111 [00:00<00:00, 88.66it/s, v_num=0]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 0:  72%|███████▏  | 80/111 [00:00<00:00, 90.35it/s, v_num=0]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 1.0               833\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 167.89it/s]\u001b[A\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 174.41it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 80.41it/s, v_num=0, ptl/val_loss=0.964, ptl/val_auc=0.734]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00007_7_hidden_size=512,num_resblocks=3,weight_decay=0.0002_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 2 | resblocks | Sequential       | 791 K \n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 2.3 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 2.3 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m 9.178     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=18474)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=18474)\u001b[0m - (ip=172.26.79.196, pid=18607) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  0.76it/s]\n",
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 2/111 [00:00<00:04, 24.56it/s, v_num=0]\n",
      "Epoch 0:   6%|▋         | 7/111 [00:00<00:02, 37.01it/s, v_num=0]\n",
      "Epoch 0:   9%|▉         | 10/111 [00:00<00:02, 34.37it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m dtype: int64\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 1.0               833\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 0:  11%|█         | 12/111 [00:00<00:03, 32.08it/s, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 16/111 [00:00<00:03, 31.42it/s, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 16/111 [00:00<00:03, 31.39it/s, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 21/111 [00:00<00:02, 34.25it/s, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 26/111 [00:00<00:02, 36.63it/s, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 27/111 [00:00<00:02, 37.22it/s, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 35/111 [00:00<00:01, 42.38it/s, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 40/111 [00:00<00:01, 42.90it/s, v_num=0]\n",
      "Epoch 0:  41%|████      | 45/111 [00:01<00:01, 43.58it/s, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 50/111 [00:01<00:01, 44.08it/s, v_num=0]\n",
      "Epoch 0:  51%|█████▏    | 57/111 [00:01<00:01, 46.29it/s, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 58/111 [00:01<00:01, 46.65it/s, v_num=0]\n",
      "Epoch 0:  60%|██████    | 67/111 [00:01<00:00, 50.05it/s, v_num=0]\n",
      "Epoch 0:  61%|██████▏   | 68/111 [00:01<00:00, 50.37it/s, v_num=0]\n",
      "Epoch 0:  61%|██████▏   | 68/111 [00:01<00:00, 50.36it/s, v_num=0]\n",
      "Epoch 0:  70%|███████   | 78/111 [00:01<00:00, 53.67it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  78%|███████▊  | 87/111 [00:01<00:00, 55.92it/s, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 96/111 [00:01<00:00, 58.09it/s, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 105/111 [00:01<00:00, 59.83it/s, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 109/111 [00:01<00:00, 58.58it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 58.59it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 134.60it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 141.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 150.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 136.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 111.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 104.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 109.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 112.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 113.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 106.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 107.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 107.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 105.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 105.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 105.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 105.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 106.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 107.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 107.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 107.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 108.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 110.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 112.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 113.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 115.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 116.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 118.18it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 119.55it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 50.97it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 48.88it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00008_8_hidden_size=512,num_resblocks=4,weight_decay=0.0014_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00007_7_hidden_size=512,num_resblocks=3,weight_decay=0.0002_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738]          \n",
      "Epoch 1:   7%|▋         | 8/111 [00:00<00:01, 62.34it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  14%|█▎        | 15/111 [00:00<00:01, 62.73it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  18%|█▊        | 20/111 [00:00<00:01, 59.95it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  19%|█▉        | 21/111 [00:00<00:01, 60.72it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  19%|█▉        | 21/111 [00:00<00:01, 60.63it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  25%|██▌       | 28/111 [00:00<00:01, 63.42it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  33%|███▎      | 37/111 [00:00<00:01, 67.97it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  34%|███▍      | 38/111 [00:00<00:01, 68.41it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  41%|████▏     | 46/111 [00:00<00:00, 70.90it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  46%|████▌     | 51/111 [00:00<00:00, 67.29it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 2 | resblocks | Sequential       | 1.1 M \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  48%|████▊     | 53/111 [00:00<00:00, 61.37it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  57%|█████▋    | 63/111 [00:00<00:00, 64.72it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Epoch 1:  61%|██████▏   | 68/111 [00:01<00:00, 63.81it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  68%|██████▊   | 76/111 [00:01<00:00, 64.85it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 1:  69%|██████▉   | 77/111 [00:01<00:00, 65.05it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  76%|███████▌  | 84/111 [00:01<00:00, 65.95it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  77%|███████▋  | 85/111 [00:01<00:00, 66.01it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  85%|████████▍ | 94/111 [00:01<00:00, 67.76it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  93%|█████████▎| 103/111 [00:01<00:00, 69.01it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1:  93%|█████████▎| 103/111 [00:01<00:00, 69.00it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 69.83it/s, v_num=0, ptl/val_loss=0.375, ptl/val_auc=0.752, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 270.34it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 198.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 176.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 182.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 148.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 157.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 160.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 148.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 151.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 146.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 150.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 154.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 154.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 156.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 154.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 155.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 155.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 154.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 156.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 156.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 155.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 156.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 157.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 158.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 161.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 162.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 163.54it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 165.15it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 62.12it/s, v_num=0, ptl/val_loss=0.387, ptl/val_auc=0.744, ptl/train_loss=0.738, ptl/train_auc=0.758]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 59.08it/s, v_num=0, ptl/val_loss=0.387, ptl/val_auc=0.744, ptl/train_loss=0.336, ptl/train_auc=0.758]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00007_7_hidden_size=512,num_resblocks=3,weight_decay=0.0002_2024-04-17_10-26-56/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m -----------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 2.6 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 2.6 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m 10.233    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:   5%|▌         | 6/111 [00:00<00:01, 53.51it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=18484)\u001b[0m \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 0:  41%|████      | 45/111 [00:00<00:00, 73.56it/s, v_num=0]\n",
      "Epoch 0:  35%|███▌      | 39/111 [00:00<00:00, 73.98it/s, v_num=0]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 0:  59%|█████▊    | 65/111 [00:00<00:00, 77.07it/s, v_num=0]\n",
      "Epoch 0:  59%|█████▊    | 65/111 [00:00<00:00, 77.06it/s, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 76/111 [00:00<00:00, 80.09it/s, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 76/111 [00:00<00:00, 80.07it/s, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 91/111 [00:01<00:00, 81.91it/s, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 92/111 [00:01<00:00, 73.39it/s, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 85/111 [00:01<00:00, 81.07it/s, v_num=0]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 0:  93%|█████████▎| 103/111 [00:01<00:00, 75.71it/s, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 75/111 [00:00<00:00, 79.89it/s, v_num=0]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 160.26it/s]\u001b[A\u001b[32m [repeated 25x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18607)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00008_8_hidden_size=512,num_resblocks=4,weight_decay=0.0014_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(TorchTrainer pid=18909)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=18909)\u001b[0m - (ip=172.26.79.196, pid=19066) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m dtype: int64\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 159.11it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 67.28it/s, v_num=0, ptl/val_loss=0.877, ptl/val_auc=0.747]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 62.45it/s, v_num=0, ptl/val_loss=0.877, ptl/val_auc=0.747, ptl/train_loss=0.643]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 77.42it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 1.0               833\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00009_9_hidden_size=256,num_resblocks=1,weight_decay=0.0001_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 1 | fc1       | Linear           | 748 K \n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 2 | resblocks | Sequential       | 66.3 K\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 3 | fc2       | Linear           | 514   \n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 820 K     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 820 K     Total params\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m 3.284     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=19016)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=19016)\u001b[0m - (ip=172.26.79.196, pid=19123) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  0.62it/s]\n",
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 2/111 [00:00<00:02, 50.91it/s, v_num=0]\n",
      "Epoch 0:  11%|█         | 12/111 [00:00<00:01, 83.51it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m dtype: int64\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 1.0               833\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 0:  13%|█▎        | 14/111 [00:00<00:01, 84.72it/s, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 18/111 [00:00<00:01, 51.24it/s, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 27/111 [00:00<00:01, 59.91it/s, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 37/111 [00:00<00:01, 65.64it/s, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 44/111 [00:00<00:01, 66.85it/s, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 52/111 [00:00<00:00, 68.06it/s, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 59/111 [00:00<00:00, 68.44it/s, v_num=0]\n",
      "Epoch 0:  59%|█████▊    | 65/111 [00:00<00:00, 66.82it/s, v_num=0]\n",
      "Epoch 0:  61%|██████▏   | 68/111 [00:01<00:00, 64.21it/s, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 75/111 [00:01<00:00, 63.32it/s, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 83/111 [00:01<00:00, 64.84it/s, v_num=0]\n",
      "Epoch 0:  81%|████████  | 90/111 [00:01<00:00, 65.26it/s, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 99/111 [00:01<00:00, 66.19it/s, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 108/111 [00:01<00:00, 67.60it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 68.15it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 269.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 213.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 176.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 116.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 52.99it/s] \u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 48.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 49.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 52.78it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 2 | resblocks | Sequential       | 1.1 M \n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 2.6 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 2.6 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00010_10_hidden_size=512,num_resblocks=4,weight_decay=0.0002_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 51.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 52.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 56.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 58.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 61.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 63.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 66.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 68.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 71.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 73.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 75.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 78.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 80.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 82.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 84.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 86.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 88.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 90.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 91.75it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 92.87it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 55.00it/s, v_num=0, ptl/val_loss=0.526, ptl/val_auc=0.747, ptl/train_loss=0.520]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00009_9_hidden_size=256,num_resblocks=1,weight_decay=0.0001_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=19066)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m -----------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m 10.233    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:  47%|████▋     | 52/111 [00:00<00:00, 82.31it/s, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 70/111 [00:00<00:00, 78.01it/s, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 103/111 [00:01<00:00, 76.17it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 77.44it/s, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 94/111 [00:01<00:00, 75.09it/s, v_num=0]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "                                                                         \u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 66.60it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 61.84it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819]\n",
      "Epoch 0:  55%|█████▍    | 61/111 [00:00<00:00, 83.32it/s, v_num=0]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 1:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819]          \n",
      "Epoch 0:  76%|███████▌  | 84/111 [00:01<00:00, 73.66it/s, v_num=0]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 0:  62%|██████▏   | 69/111 [00:00<00:00, 84.43it/s, v_num=0]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 1:   9%|▉         | 10/111 [00:00<00:03, 27.61it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  13%|█▎        | 14/111 [00:00<00:03, 29.25it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 0:  29%|██▉       | 32/111 [00:00<00:01, 75.03it/s, v_num=0]\n",
      "Epoch 1:  16%|█▌        | 18/111 [00:00<00:02, 31.66it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:   5%|▌         | 6/111 [00:00<00:04, 23.35it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 1:  17%|█▋        | 19/111 [00:00<00:02, 32.51it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  17%|█▋        | 19/111 [00:00<00:02, 32.43it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  22%|██▏       | 24/111 [00:00<00:02, 35.59it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  23%|██▎       | 25/111 [00:00<00:02, 36.14it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  26%|██▌       | 29/111 [00:00<00:02, 36.51it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  31%|███       | 34/111 [00:00<00:02, 38.25it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  36%|███▌      | 40/111 [00:00<00:01, 40.25it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  40%|███▉      | 44/111 [00:01<00:01, 41.30it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 150.88it/s]\u001b[A\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Epoch 1:  47%|████▋     | 52/111 [00:01<00:01, 40.25it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  48%|████▊     | 53/111 [00:01<00:01, 40.43it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  48%|████▊     | 53/111 [00:01<00:01, 40.41it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  52%|█████▏    | 58/111 [00:01<00:01, 41.30it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  58%|█████▊    | 64/111 [00:01<00:01, 42.33it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 147.01it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 1:  66%|██████▌   | 73/111 [00:01<00:00, 42.53it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  73%|███████▎  | 81/111 [00:01<00:00, 44.23it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  98%|█████████▊| 109/111 [00:02<00:00, 51.00it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:02<00:00, 51.49it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:28:55,507\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 111/111 [00:02<00:00, 46.63it/s, v_num=0, ptl/val_loss=0.389, ptl/val_auc=0.730, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:02<00:00, 45.06it/s, v_num=0, ptl/val_loss=0.389, ptl/val_auc=0.730, ptl/train_loss=0.418, ptl/train_auc=0.744]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00010_10_hidden_size=512,num_resblocks=4,weight_decay=0.0002_2024-04-17_10-26-56/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=19123)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=19453)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=19453)\u001b[0m - (ip=172.26.79.196, pid=19572) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 1.0               833\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 1:  89%|████████▉ | 99/111 [00:02<00:00, 48.70it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 1:  72%|███████▏  | 80/111 [00:01<00:00, 44.02it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\n",
      "Epoch 1:  60%|██████    | 67/111 [00:01<00:01, 41.71it/s, v_num=0, ptl/val_loss=0.435, ptl/val_auc=0.755, ptl/train_loss=0.819, ptl/train_auc=0.744]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 144.25it/s]\u001b[A\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 140.95it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00011_11_hidden_size=512,num_resblocks=3,weight_decay=0.0100_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 2 | resblocks | Sequential       | 791 K \n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 2.3 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 2.3 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m 9.178     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=19621)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=19621)\u001b[0m - (ip=172.26.79.196, pid=19707) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: |          | 0/? [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:   6%|▋         | 7/111 [00:00<00:01, 65.98it/s, v_num=0]\n",
      "                                                                           \n",
      "Epoch 0:  14%|█▍        | 16/111 [00:00<00:01, 77.63it/s, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 25/111 [00:00<00:01, 81.80it/s, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 31/111 [00:00<00:01, 75.04it/s, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 38/111 [00:00<00:00, 74.72it/s, v_num=0]\n",
      "Epoch 0:  41%|████▏     | 46/111 [00:00<00:00, 75.07it/s, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:00<00:00, 72.81it/s, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 59/111 [00:00<00:00, 71.04it/s, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 66/111 [00:00<00:00, 71.39it/s, v_num=0]\n",
      "Epoch 0:  60%|██████    | 67/111 [00:00<00:00, 71.57it/s, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 76/111 [00:01<00:00, 73.50it/s, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 81/111 [00:01<00:00, 71.51it/s, v_num=0]\n",
      "Epoch 0:  80%|████████  | 89/111 [00:01<00:00, 71.72it/s, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 96/111 [00:01<00:00, 70.99it/s, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 96/111 [00:01<00:00, 70.97it/s, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 102/111 [00:01<00:00, 70.59it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 71.83it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m \n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 250.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 196.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 190.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 198.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 181.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 174.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 176.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 179.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 170.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 169.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 159.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 160.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 160.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 159.72it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m \n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 160.67it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m \n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 156.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 157.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 155.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 155.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 155.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 154.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 155.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 154.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 155.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 157.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 156.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 157.98it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 159.33it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 63.01it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00011_11_hidden_size=512,num_resblocks=3,weight_decay=0.0100_2024-04-17_10-26-56/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 59.25it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757]\n",
      "Epoch 1:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▎         | 4/111 [00:00<00:02, 36.66it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  10%|▉         | 11/111 [00:00<00:01, 51.55it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  11%|█         | 12/111 [00:00<00:01, 53.04it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  11%|█         | 12/111 [00:00<00:01, 52.91it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  18%|█▊        | 20/111 [00:00<00:01, 61.70it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m dtype: int64\n",
      "Epoch 1:  25%|██▌       | 28/111 [00:00<00:01, 64.56it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  33%|███▎      | 37/111 [00:00<00:01, 69.13it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  41%|████▏     | 46/111 [00:00<00:00, 71.98it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  48%|████▊     | 53/111 [00:00<00:00, 72.38it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  49%|████▊     | 54/111 [00:00<00:00, 72.50it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  52%|█████▏    | 58/111 [00:00<00:00, 69.39it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  53%|█████▎    | 59/111 [00:00<00:00, 69.40it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  59%|█████▉    | 66/111 [00:00<00:00, 69.60it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  63%|██████▎   | 70/111 [00:01<00:00, 67.26it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  64%|██████▍   | 71/111 [00:01<00:00, 67.16it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  68%|██████▊   | 76/111 [00:01<00:00, 65.95it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  74%|███████▍  | 82/111 [00:01<00:00, 65.44it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  77%|███████▋  | 86/111 [00:01<00:00, 62.64it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  81%|████████  | 90/111 [00:01<00:00, 60.86it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  83%|████████▎ | 92/111 [00:01<00:00, 59.77it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  86%|████████▋ | 96/111 [00:01<00:00, 56.89it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  91%|█████████ | 101/111 [00:01<00:00, 56.42it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Epoch 1:  96%|█████████▋| 107/111 [00:01<00:00, 56.58it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 57.30it/s, v_num=0, ptl/val_loss=0.450, ptl/val_auc=0.753, ptl/train_loss=0.757, ptl/train_auc=0.722]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 125.09it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 119.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 119.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 110.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 111.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 109.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 105.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 107.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 107.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 108.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 106.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 105.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 105.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 105.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 107.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 107.49it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "2024-04-17 10:29:09,332\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 109.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 111.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 111.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 112.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 111.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 110.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 111.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 110.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 97.32it/s] \u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 95.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 92.78it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 90.69it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 111/111 [00:02<00:00, 48.58it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.757, ptl/train_auc=0.722]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00011_11_hidden_size=512,num_resblocks=3,weight_decay=0.0100_2024-04-17_10-26-56/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00012_12_hidden_size=256,num_resblocks=1,weight_decay=0.0021_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 111/111 [00:02<00:00, 47.24it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.722]\n",
      "Epoch 2:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.722]          \n",
      "Epoch 2:   5%|▌         | 6/111 [00:00<00:01, 62.67it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  14%|█▎        | 15/111 [00:00<00:01, 74.39it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  19%|█▉        | 21/111 [00:00<00:01, 69.18it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  25%|██▌       | 28/111 [00:00<00:01, 67.87it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  32%|███▏      | 36/111 [00:00<00:01, 69.59it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  38%|███▊      | 42/111 [00:00<00:01, 67.58it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  44%|████▍     | 49/111 [00:00<00:00, 68.46it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  51%|█████▏    | 57/111 [00:00<00:00, 68.98it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  59%|█████▉    | 66/111 [00:00<00:00, 70.98it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  68%|██████▊   | 75/111 [00:01<00:00, 72.79it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  74%|███████▍  | 82/111 [00:01<00:00, 72.93it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  75%|███████▍  | 83/111 [00:01<00:00, 72.83it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  75%|███████▍  | 83/111 [00:01<00:00, 72.81it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  79%|███████▉  | 88/111 [00:01<00:00, 72.65it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  87%|████████▋ | 97/111 [00:01<00:00, 72.12it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2:  97%|█████████▋| 108/111 [00:01<00:00, 74.75it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 74.99it/s, v_num=0, ptl/val_loss=0.477, ptl/val_auc=0.766, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 150.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 127.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 125.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 130.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 117.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 122.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 118.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 122.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 114.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 108.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 108.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 104.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 103.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 105.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 102.66it/s]\u001b[A\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 82.11it/s] \u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 62.63it/s]\u001b[A\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 64.42it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m 1 | fc1       | Linear           | 748 K \n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m 2 | resblocks | Sequential       | 66.3 K\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m 3 | fc2       | Linear           | 514   \n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m 820 K     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m 820 K     Total params\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m 3.284     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 64.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 66.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 68.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 70.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 72.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 73.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 75.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 79.11it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 81.01it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 59.82it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.581, ptl/train_auc=0.741]\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 55.92it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.741]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00011_11_hidden_size=512,num_resblocks=3,weight_decay=0.0100_2024-04-17_10-26-56/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   1%|          | 1/111 [00:00<00:05, 21.80it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.741]  \n",
      "Epoch 3:   1%|          | 1/111 [00:00<00:05, 21.69it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:   7%|▋         | 8/111 [00:00<00:01, 57.02it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:   8%|▊         | 9/111 [00:00<00:01, 59.06it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  14%|█▍        | 16/111 [00:00<00:01, 64.91it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m \u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "Epoch 3:  23%|██▎       | 26/111 [00:00<00:01, 72.58it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  32%|███▏      | 35/111 [00:00<00:00, 76.37it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  35%|███▌      | 39/111 [00:00<00:01, 69.51it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  43%|████▎     | 48/111 [00:00<00:00, 72.40it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  50%|█████     | 56/111 [00:00<00:00, 73.82it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  51%|█████▏    | 57/111 [00:00<00:00, 73.59it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  57%|█████▋    | 63/111 [00:00<00:00, 72.97it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  58%|█████▊    | 64/111 [00:00<00:00, 72.73it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  63%|██████▎   | 70/111 [00:00<00:00, 71.76it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  70%|███████   | 78/111 [00:01<00:00, 72.04it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  76%|███████▌  | 84/111 [00:01<00:00, 70.53it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  83%|████████▎ | 92/111 [00:01<00:00, 71.23it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3:  91%|█████████ | 101/111 [00:01<00:00, 72.53it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 74.50it/s, v_num=0, ptl/val_loss=0.441, ptl/val_auc=0.772, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 217.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 136.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 149.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 160.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 153.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 150.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 143.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 150.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 154.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 151.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 154.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 157.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 158.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 160.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 161.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 161.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 161.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 161.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 160.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 161.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 162.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 162.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 164.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 163.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 163.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 164.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 144.00it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 130.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 61.69it/s, v_num=0, ptl/val_loss=0.765, ptl/val_auc=0.760, ptl/train_loss=0.562, ptl/train_auc=0.755]\n",
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19572)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00011_11_hidden_size=512,num_resblocks=3,weight_decay=0.0100_2024-04-17_10-26-56/checkpoint_000003)\n",
      "2024-04-17 10:29:13,548\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 111/111 [00:02<00:00, 55.30it/s, v_num=0, ptl/val_loss=0.765, ptl/val_auc=0.760, ptl/train_loss=0.548, ptl/train_auc=0.755]\n",
      "Epoch 0:   6%|▋         | 7/111 [00:00<00:03, 29.36it/s, v_num=0]\n",
      "Epoch 0:   8%|▊         | 9/111 [00:00<00:03, 26.13it/s, v_num=0]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 0:  50%|█████     | 56/111 [00:00<00:00, 65.00it/s, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 77/111 [00:01<00:00, 71.88it/s, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 102/111 [00:01<00:00, 73.69it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 75.65it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 1/111 [00:00<00:03, 30.92it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 1:  13%|█▎        | 14/111 [00:00<00:01, 96.39it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 1:  23%|██▎       | 25/111 [00:00<00:00, 101.67it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 1:  36%|███▌      | 40/111 [00:00<00:00, 87.54it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 0:  41%|████      | 45/111 [00:00<00:01, 59.35it/s, v_num=0]\n",
      "Epoch 1:  46%|████▌     | 51/111 [00:00<00:00, 91.37it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 0:  91%|█████████ | 101/111 [00:01<00:00, 73.48it/s, v_num=0]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 1:  54%|█████▍    | 60/111 [00:00<00:00, 91.06it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 1:  74%|███████▍  | 82/111 [00:00<00:00, 94.10it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 1:  87%|████████▋ | 97/111 [00:01<00:00, 93.93it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 1:  93%|█████████▎| 103/111 [00:01<00:00, 87.62it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 89.05it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:29:16,486\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 75.93it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.771]\n",
      "Epoch 1:  36%|███▌      | 40/111 [00:00<00:00, 87.42it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\n",
      "Epoch 2:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.771]          \n",
      "Epoch 1:  29%|██▉       | 32/111 [00:00<00:00, 90.60it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 2:  19%|█▉        | 21/111 [00:00<00:01, 71.22it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Epoch 2:  14%|█▍        | 16/111 [00:00<00:01, 79.33it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Epoch 2:  27%|██▋       | 30/111 [00:00<00:01, 73.46it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Epoch 2:  35%|███▌      | 39/111 [00:00<00:00, 76.17it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Epoch 2:  45%|████▌     | 50/111 [00:00<00:00, 81.55it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Epoch 2:  59%|█████▉    | 66/111 [00:00<00:00, 79.83it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Epoch 2:   5%|▌         | 6/111 [00:00<00:01, 59.77it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 2:  72%|███████▏  | 80/111 [00:01<00:00, 77.61it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Epoch 2:  79%|███████▉  | 88/111 [00:01<00:00, 77.15it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Epoch 2:  89%|████████▉ | 99/111 [00:01<00:00, 79.74it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Epoch 2:  98%|█████████▊| 109/111 [00:01<00:00, 80.78it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 80.91it/s, v_num=0, ptl/val_loss=0.639, ptl/val_auc=0.778, ptl/train_loss=0.536, ptl/train_auc=0.787]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 151.24it/s]\u001b[A\u001b[32m [repeated 72x across cluster]\u001b[0m\n",
      "Epoch 3:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.522, ptl/train_auc=0.787]          \n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 117.63it/s]\u001b[A\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 67.67it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.536, ptl/train_auc=0.787]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 3:  15%|█▌        | 17/111 [00:00<00:01, 71.64it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.522, ptl/train_auc=0.797]\n",
      "Epoch 1:  94%|█████████▎| 104/111 [00:01<00:00, 87.80it/s, v_num=0, ptl/val_loss=0.494, ptl/val_auc=0.769, ptl/train_loss=0.595, ptl/train_auc=0.771]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 3:  34%|███▍      | 38/111 [00:00<00:01, 69.15it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.522, ptl/train_auc=0.797]\n",
      "Epoch 3:  53%|█████▎    | 59/111 [00:00<00:00, 78.34it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.522, ptl/train_auc=0.797]\n",
      "Epoch 3:  58%|█████▊    | 64/111 [00:00<00:00, 75.64it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.522, ptl/train_auc=0.797]\n",
      "Epoch 3:  66%|██████▌   | 73/111 [00:00<00:00, 76.53it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.522, ptl/train_auc=0.797]\n",
      "Epoch 3:  76%|███████▌  | 84/111 [00:01<00:00, 79.06it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.522, ptl/train_auc=0.797]\n",
      "Epoch 3:  95%|█████████▍| 105/111 [00:01<00:00, 82.69it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.522, ptl/train_auc=0.797]\n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 82.82it/s, v_num=0, ptl/val_loss=0.479, ptl/val_auc=0.783, ptl/train_loss=0.522, ptl/train_auc=0.797]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:29:19,891\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00012_12_hidden_size=256,num_resblocks=1,weight_decay=0.0021_2024-04-17_10-26-56/checkpoint_000003)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.797]          \n",
      "Epoch 4:   4%|▎         | 4/111 [00:00<00:02, 35.91it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\n",
      "Epoch 4:  23%|██▎       | 25/111 [00:00<00:01, 61.02it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\n",
      "Epoch 4:  30%|██▉       | 33/111 [00:00<00:01, 62.36it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\n",
      "Epoch 4:  10%|▉         | 11/111 [00:00<00:01, 52.26it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 4:  65%|██████▍   | 72/111 [00:00<00:00, 76.53it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\n",
      "Epoch 4:  73%|███████▎  | 81/111 [00:01<00:00, 77.95it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\n",
      "Epoch 4:  82%|████████▏ | 91/111 [00:01<00:00, 78.98it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\n",
      "Epoch 4:  95%|█████████▌| 106/111 [00:01<00:00, 78.66it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\n",
      "Epoch 4: 100%|██████████| 111/111 [00:01<00:00, 76.12it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\n",
      "Epoch 5:   1%|          | 1/111 [00:00<00:04, 24.41it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]  \n",
      "Epoch 4:  74%|███████▍  | 82/111 [00:01<00:00, 78.19it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 5:   8%|▊         | 9/111 [00:00<00:01, 60.81it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Epoch 5:  16%|█▌        | 18/111 [00:00<00:01, 71.80it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]\n",
      "Epoch 4:  57%|█████▋    | 63/111 [00:00<00:00, 75.05it/s, v_num=0, ptl/val_loss=0.511, ptl/val_auc=0.789, ptl/train_loss=0.517, ptl/train_auc=0.802]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 5:  47%|████▋     | 52/111 [00:00<00:00, 78.41it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]\n",
      "Epoch 5:  52%|█████▏    | 58/111 [00:00<00:00, 75.11it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 5:  66%|██████▌   | 73/111 [00:00<00:00, 74.60it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]\n",
      "Epoch 5:  84%|████████▍ | 93/111 [00:01<00:00, 78.55it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]\n",
      "Epoch 5:  91%|█████████ | 101/111 [00:01<00:00, 78.64it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]\n",
      "Epoch 5:  99%|█████████▉| 110/111 [00:01<00:00, 79.47it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 5: 100%|██████████| 111/111 [00:01<00:00, 77.37it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.500, ptl/train_auc=0.808]\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 154.77it/s]\u001b[A\u001b[32m [repeated 61x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "2024-04-17 10:29:23,354\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 111/111 [00:01<00:00, 65.78it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.808]\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 143.48it/s]\u001b[A\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 6:   2%|▏         | 2/111 [00:00<00:03, 28.36it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]  \n",
      "Epoch 4: 100%|██████████| 111/111 [00:01<00:00, 66.78it/s, v_num=0, ptl/val_loss=0.378, ptl/val_auc=0.786, ptl/train_loss=0.517, ptl/train_auc=0.802]\n",
      "Epoch 6:  14%|█▎        | 15/111 [00:00<00:01, 52.78it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\n",
      "Epoch 6:  17%|█▋        | 19/111 [00:00<00:01, 52.77it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\n",
      "Epoch 6:  32%|███▏      | 35/111 [00:00<00:01, 58.05it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\n",
      "Epoch 6:  39%|███▊      | 43/111 [00:00<00:01, 62.22it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=20071)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20071)\u001b[0m - (ip=172.26.79.196, pid=20129) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  74%|███████▍  | 82/111 [00:01<00:00, 72.53it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\n",
      "Epoch 6:  83%|████████▎ | 92/111 [00:01<00:00, 74.56it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\n",
      "Epoch 6:  83%|████████▎ | 92/111 [00:01<00:00, 74.55it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\n",
      "Epoch 6:  92%|█████████▏| 102/111 [00:01<00:00, 76.40it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\n",
      "Epoch 6: 100%|██████████| 111/111 [00:01<00:00, 78.77it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00012_12_hidden_size=256,num_resblocks=1,weight_decay=0.0021_2024-04-17_10-26-56/checkpoint_000006)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.813]          \n",
      "Epoch 7:  19%|█▉        | 21/111 [00:00<00:00, 100.10it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\n",
      "Epoch 7:  29%|██▉       | 32/111 [00:00<00:00, 100.03it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\n",
      "Epoch 7:  39%|███▊      | 43/111 [00:00<00:00, 101.59it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\n",
      "Epoch 7:  49%|████▊     | 54/111 [00:00<00:00, 102.70it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\n",
      "Epoch 6:   9%|▉         | 10/111 [00:00<00:01, 52.85it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 7:  62%|██████▏   | 69/111 [00:00<00:00, 95.35it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816] \n",
      "Epoch 7:  76%|███████▌  | 84/111 [00:00<00:00, 89.23it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\n",
      "Epoch 7:  83%|████████▎ | 92/111 [00:01<00:00, 88.14it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\n",
      "Epoch 7:  98%|█████████▊| 109/111 [00:01<00:00, 86.86it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\n",
      "Epoch 7: 100%|██████████| 111/111 [00:01<00:00, 87.42it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 93.04it/s] \u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:29:26,716\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 111/111 [00:01<00:00, 71.39it/s, v_num=0, ptl/val_loss=0.744, ptl/val_auc=0.785, ptl/train_loss=0.489, ptl/train_auc=0.816]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00013_13_hidden_size=1024,num_resblocks=1,weight_decay=0.0004_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Epoch 6:  40%|███▉      | 44/111 [00:00<00:01, 61.49it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m \u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Epoch 7:  58%|█████▊    | 64/111 [00:00<00:00, 102.50it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Epoch 6:  45%|████▌     | 50/111 [00:00<00:01, 60.97it/s, v_num=0, ptl/val_loss=0.445, ptl/val_auc=0.787, ptl/train_loss=0.494, ptl/train_auc=0.813]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 123.16it/s]\u001b[A\u001b[32m [repeated 66x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 125.99it/s]\u001b[A\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m 1 | fc1       | Linear           | 3.0 M \n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m 2 | resblocks | Sequential       | 1.1 M \n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m 3 | fc2       | Linear           | 2.0 K \n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m 4.1 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m 4.1 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m 16.211    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]\n",
      "Epoch 7:  89%|████████▉ | 99/111 [00:01<00:00, 86.54it/s, v_num=0, ptl/val_loss=0.583, ptl/val_auc=0.788, ptl/train_loss=0.489, ptl/train_auc=0.816]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=19707)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00012_12_hidden_size=256,num_resblocks=1,weight_decay=0.0021_2024-04-17_10-26-56/checkpoint_000007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:   7%|▋         | 8/111 [00:00<00:02, 41.47it/s, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 24/111 [00:00<00:01, 59.29it/s, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 24/111 [00:00<00:01, 59.20it/s, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 33/111 [00:00<00:01, 64.70it/s, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 42/111 [00:00<00:00, 69.43it/s, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:00<00:00, 74.54it/s, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 69/111 [00:00<00:00, 74.75it/s, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 80/111 [00:01<00:00, 77.82it/s, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 92/111 [00:01<00:00, 80.99it/s, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 102/111 [00:01<00:00, 82.47it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 84.27it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 71.78it/s, v_num=0, ptl/val_loss=0.422, ptl/val_auc=0.718]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 65.08it/s, v_num=0, ptl/val_loss=0.422, ptl/val_auc=0.718, ptl/train_loss=0.678]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20129)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00013_13_hidden_size=1024,num_resblocks=1,weight_decay=0.0004_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=20222)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20222)\u001b[0m - (ip=172.26.79.196, pid=20404) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m dtype: int64\n",
      "Epoch 0:  49%|████▊     | 54/111 [00:00<00:00, 75.12it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 1.0               833\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 0:  54%|█████▍    | 60/111 [00:00<00:00, 75.11it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 142.31it/s]\u001b[A\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 144.60it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 0:  14%|█▎        | 15/111 [00:00<00:01, 50.89it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00014_14_hidden_size=512,num_resblocks=4,weight_decay=0.0004_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 2 | resblocks | Sequential       | 1.1 M \n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 2.6 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 2.6 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m 10.233    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=20456)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20456)\u001b[0m - (ip=172.26.79.196, pid=20540) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 3/111 [00:00<00:02, 46.72it/s, v_num=0]\n",
      "Epoch 0:  10%|▉         | 11/111 [00:00<00:01, 63.92it/s, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 19/111 [00:00<00:01, 71.74it/s, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 28/111 [00:00<00:01, 73.71it/s, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 31/111 [00:00<00:01, 69.68it/s, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 36/111 [00:00<00:01, 61.88it/s, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 40/111 [00:00<00:01, 58.28it/s, v_num=0]\n",
      "Epoch 0:  41%|████      | 45/111 [00:00<00:01, 56.70it/s, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:00<00:00, 58.83it/s, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 61/111 [00:00<00:00, 61.20it/s, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 62/111 [00:01<00:00, 61.50it/s, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 71/111 [00:01<00:00, 64.15it/s, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 77/111 [00:01<00:00, 64.90it/s, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 82/111 [00:01<00:00, 62.42it/s, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 91/111 [00:01<00:00, 63.87it/s, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 91/111 [00:01<00:00, 63.86it/s, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 99/111 [00:01<00:00, 64.98it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 0.0             27195\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 1.0              1610\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 0.0             6813\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 1.0              389\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 0.0             14599\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 1.0               833\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m dtype: int64\n",
      "Epoch 0:  97%|█████████▋| 108/111 [00:01<00:00, 66.51it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 66.69it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 268.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 138.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 147.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 129.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 123.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 122.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 124.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 123.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 128.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 122.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 122.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 120.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 120.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 119.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 120.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 117.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 115.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 116.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 115.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 115.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 115.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 116.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 116.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 117.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 116.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 117.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 117.35it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 118.48it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 57.14it/s, v_num=0, ptl/val_loss=0.482, ptl/val_auc=0.748]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 52.77it/s, v_num=0, ptl/val_loss=0.482, ptl/val_auc=0.748, ptl/train_loss=0.757]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00014_14_hidden_size=512,num_resblocks=4,weight_decay=0.0004_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=20404)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00015_15_hidden_size=256,num_resblocks=2,weight_decay=0.0006_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 2 | resblocks | Sequential       | 132 K \n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 3 | fc2       | Linear           | 514   \n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 887 K     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 887 K     Total params\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m -----------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 1 | fc1       | Linear           | 748 K \n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m 3.549     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 1/111 [00:00<00:03, 32.86it/s, v_num=0]\n",
      "                                                                           \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 0:  10%|▉         | 11/111 [00:00<00:01, 82.77it/s, v_num=0]\n",
      "Epoch 0:  21%|██        | 23/111 [00:00<00:00, 97.21it/s, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 30/111 [00:00<00:00, 90.19it/s, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 36/111 [00:00<00:00, 83.51it/s, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 37/111 [00:00<00:00, 82.53it/s, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 37/111 [00:00<00:00, 82.48it/s, v_num=0]\n",
      "Epoch 0:  41%|████      | 45/111 [00:00<00:00, 82.42it/s, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 51/111 [00:00<00:00, 78.89it/s, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 61/111 [00:00<00:00, 80.54it/s, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 70/111 [00:00<00:00, 82.10it/s, v_num=0]\n",
      "Epoch 0:  70%|███████   | 78/111 [00:00<00:00, 81.68it/s, v_num=0]\n",
      "Epoch 0:  71%|███████   | 79/111 [00:00<00:00, 81.70it/s, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 88/111 [00:01<00:00, 82.80it/s, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 98/111 [00:01<00:00, 84.34it/s, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 99/111 [00:01<00:00, 84.32it/s, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 109/111 [00:01<00:00, 85.66it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 85.93it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 363.30it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 210.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 191.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 204.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 195.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 192.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 198.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 207.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 207.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 200.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 203.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 202.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 198.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 197.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 196.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 193.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 192.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 191.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 188.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 187.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 186.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 187.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 187.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 168.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 165.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 164.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 157.30it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 156.77it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 71.63it/s, v_num=0, ptl/val_loss=0.401, ptl/val_auc=0.739, ptl/train_loss=0.622]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=20540)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00015_15_hidden_size=256,num_resblocks=2,weight_decay=0.0006_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=20766)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20766)\u001b[0m - (ip=172.26.79.196, pid=20952) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 1.0               833\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00016_16_hidden_size=1024,num_resblocks=5,weight_decay=0.0093_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 1 | fc1       | Linear           | 3.0 M \n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 2 | resblocks | Sequential       | 5.3 M \n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 3 | fc2       | Linear           | 2.0 K \n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 8.3 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 8.3 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m 33.037    Total estimated model params size (MB)\n",
      "\u001b[36m(TorchTrainer pid=20998)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20998)\u001b[0m - (ip=172.26.79.196, pid=21084) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:   1%|          | 1/111 [00:00<00:05, 21.07it/s, v_num=0]\n",
      "Epoch 0:   7%|▋         | 8/111 [00:00<00:01, 55.18it/s, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 16/111 [00:00<00:01, 63.66it/s, v_num=0]\n",
      "                                                                           \n",
      "Epoch 0:  20%|█▉        | 22/111 [00:00<00:01, 65.90it/s, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 28/111 [00:00<00:01, 60.32it/s, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 36/111 [00:00<00:01, 63.50it/s, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 44/111 [00:00<00:01, 65.48it/s, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 52/111 [00:00<00:00, 67.76it/s, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:00<00:00, 68.03it/s, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:00<00:00, 68.01it/s, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 60/111 [00:00<00:00, 68.71it/s, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 61/111 [00:00<00:00, 68.93it/s, v_num=0]\n",
      "Epoch 0:  61%|██████▏   | 68/111 [00:00<00:00, 68.98it/s, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 73/111 [00:01<00:00, 67.31it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 0.0             27236\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 1.0              1569\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 0.0             6772\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 1.0              430\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 0.0             14599\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 1.0               833\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m dtype: int64\n",
      "Epoch 0:  68%|██████▊   | 76/111 [00:01<00:00, 64.61it/s, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 77/111 [00:01<00:00, 64.04it/s, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 82/111 [00:01<00:00, 63.06it/s, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 88/111 [00:01<00:00, 62.71it/s, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 94/111 [00:01<00:00, 62.58it/s, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 95/111 [00:01<00:00, 62.57it/s, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 102/111 [00:01<00:00, 63.22it/s, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 110/111 [00:01<00:00, 64.22it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 64.29it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 86.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 76.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 78.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 89.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 85.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 89.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 90.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 95.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 96.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 99.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 97.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 97.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 100.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 102.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 104.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 106.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 107.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 107.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 107.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 107.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 107.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 109.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 110.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 111.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 111.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 112.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 113.23it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 113.08it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 54.58it/s, v_num=0, ptl/val_loss=0.751, ptl/val_auc=0.721]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 44.09it/s, v_num=0, ptl/val_loss=0.751, ptl/val_auc=0.721, ptl/train_loss=0.987]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00016_16_hidden_size=1024,num_resblocks=5,weight_decay=0.0093_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00017_17_hidden_size=1024,num_resblocks=5,weight_decay=0.0019_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 1 | fc1       | Linear           | 3.0 M \n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 2 | resblocks | Sequential       | 5.3 M \n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 3 | fc2       | Linear           | 2.0 K \n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 8.3 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 8.3 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m 33.037    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "\u001b[36m(RayTrainWorker pid=20952)\u001b[0m \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 0:   5%|▌         | 6/111 [00:00<00:02, 49.73it/s, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 16/111 [00:00<00:01, 69.21it/s, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 24/111 [00:00<00:01, 72.47it/s, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 33/111 [00:00<00:01, 75.28it/s, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 42/111 [00:00<00:00, 77.43it/s, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 50/111 [00:00<00:00, 78.04it/s, v_num=0]\n",
      "Epoch 0:  50%|████▉     | 55/111 [00:00<00:00, 73.98it/s, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 63/111 [00:00<00:00, 74.07it/s, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 70/111 [00:00<00:00, 73.49it/s, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 76/111 [00:01<00:00, 72.14it/s, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 83/111 [00:01<00:00, 71.51it/s, v_num=0]\n",
      "Epoch 0:  80%|████████  | 89/111 [00:01<00:00, 70.42it/s, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 97/111 [00:01<00:00, 70.96it/s, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 106/111 [00:01<00:00, 72.19it/s, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 107/111 [00:01<00:00, 72.25it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 72.78it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 247.29it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 213.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 186.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 189.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 169.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 175.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 163.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 169.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 168.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 166.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 168.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 167.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 165.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 164.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 158.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 157.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 152.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 152.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 149.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 146.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 145.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 144.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 141.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 142.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 139.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 139.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 137.04it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 131.14it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 62.38it/s, v_num=0, ptl/val_loss=1.200, ptl/val_auc=0.745]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=21084)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00017_17_hidden_size=1024,num_resblocks=5,weight_decay=0.0019_2024-04-17_10-26-56/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 53.57it/s, v_num=0, ptl/val_loss=1.200, ptl/val_auc=0.745, ptl/train_loss=1.150]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=21313)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=21313)\u001b[0m - (ip=172.26.79.196, pid=21494) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 1.0               833\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00018_18_hidden_size=512,num_resblocks=3,weight_decay=0.0048_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 2 | resblocks | Sequential       | 791 K \n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 2.3 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 2.3 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m 9.178     Total estimated model params size (MB)\n",
      "\u001b[36m(TorchTrainer pid=21500)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=21500)\u001b[0m - (ip=172.26.79.196, pid=21628) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 1/111 [00:00<00:08, 12.62it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m dtype: int64\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 1.0               833\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 0:   7%|▋         | 8/111 [00:00<00:02, 43.85it/s, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 18/111 [00:00<00:01, 61.74it/s, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 24/111 [00:00<00:01, 62.07it/s, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 25/111 [00:00<00:01, 62.38it/s, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 31/111 [00:00<00:01, 62.38it/s, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 40/111 [00:00<00:01, 66.33it/s, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 48/111 [00:00<00:00, 68.50it/s, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 49/111 [00:00<00:00, 68.32it/s, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 52/111 [00:00<00:00, 63.85it/s, v_num=0]\n",
      "Epoch 0:  51%|█████▏    | 57/111 [00:00<00:00, 61.59it/s, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 61/111 [00:01<00:00, 59.98it/s, v_num=0]\n",
      "Epoch 0:  60%|██████    | 67/111 [00:01<00:00, 58.94it/s, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 74/111 [00:01<00:00, 60.23it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  72%|███████▏  | 80/111 [00:01<00:00, 59.74it/s, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 83/111 [00:01<00:00, 57.44it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  81%|████████  | 90/111 [00:01<00:00, 58.15it/s, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 96/111 [00:01<00:00, 57.97it/s, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 101/111 [00:01<00:00, 57.50it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 59.24it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 34.11it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 42.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 54.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 60.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 58.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 65.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 72.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 78.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 84.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 87.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 92.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 96.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 98.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 103.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 107.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 109.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 112.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 115.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 117.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 120.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 123.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 126.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 127.47it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00019_19_hidden_size=512,num_resblocks=4,weight_decay=0.0026_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 129.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 130.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 131.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 132.79it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 134.13it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 52.14it/s, v_num=0, ptl/val_loss=0.940, ptl/val_auc=0.733]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 50.01it/s, v_num=0, ptl/val_loss=0.940, ptl/val_auc=0.733, ptl/train_loss=0.721]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00018_18_hidden_size=512,num_resblocks=3,weight_decay=0.0048_2024-04-17_10-26-56/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 2 | resblocks | Sequential       | 1.1 M \n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m -----------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 2.6 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 2.6 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m 10.233    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 2/111 [00:00<00:02, 44.78it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=21494)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 0:  12%|█▏        | 13/111 [00:00<00:01, 83.99it/s, v_num=0]\n",
      "Epoch 0:   3%|▎         | 3/111 [00:00<00:01, 56.45it/s, v_num=0]\n",
      "Epoch 0:  21%|██        | 23/111 [00:00<00:01, 87.49it/s, v_num=0]\n",
      "Epoch 0:  21%|██        | 23/111 [00:00<00:01, 87.42it/s, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 32/111 [00:00<00:00, 88.64it/s, v_num=0]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 0:  30%|██▉       | 33/111 [00:00<00:00, 89.04it/s, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 33/111 [00:00<00:00, 88.99it/s, v_num=0]\n",
      "Epoch 0:  41%|████▏     | 46/111 [00:00<00:00, 79.50it/s, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:00<00:00, 78.35it/s, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 38/111 [00:00<00:00, 81.47it/s, v_num=0]\n",
      "Epoch 0:  49%|████▊     | 54/111 [00:00<00:00, 78.36it/s, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 62/111 [00:00<00:00, 78.21it/s, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 69/111 [00:00<00:00, 77.31it/s, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 101/111 [00:01<00:00, 77.27it/s, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 108/111 [00:01<00:00, 76.55it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 76.88it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 155.55it/s]\u001b[A\u001b[32m [repeated 25x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=21628)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00019_19_hidden_size=512,num_resblocks=4,weight_decay=0.0026_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(TorchTrainer pid=21861)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=21861)\u001b[0m - (ip=172.26.79.196, pid=22046) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m dtype: int64\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 154.02it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 66.07it/s, v_num=0, ptl/val_loss=0.370, ptl/val_auc=0.747]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 60.94it/s, v_num=0, ptl/val_loss=0.370, ptl/val_auc=0.747, ptl/train_loss=0.783]\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 1.0               833\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Epoch 0:  86%|████████▋ | 96/111 [00:01<00:00, 79.71it/s, v_num=0]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00020_20_hidden_size=1024,num_resblocks=1,weight_decay=0.0015_2024-04-17_10-26-56/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 1 | fc1       | Linear           | 3.0 M \n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 2 | resblocks | Sequential       | 1.1 M \n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 3 | fc2       | Linear           | 2.0 K \n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 4.1 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 4.1 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m 16.211    Total estimated model params size (MB)\n",
      "\u001b[36m(TorchTrainer pid=22047)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22047)\u001b[0m - (ip=172.26.79.196, pid=22170) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:   6%|▋         | 7/111 [00:00<00:02, 42.95it/s, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 16/111 [00:00<00:01, 61.17it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 0.0             27242\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 1.0              1563\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 0.0             6766\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 1.0              436\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 0.0             14599\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 1.0               833\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m dtype: int64\n",
      "Epoch 0:  15%|█▌        | 17/111 [00:00<00:01, 62.66it/s, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 25/111 [00:00<00:01, 66.45it/s, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 29/111 [00:00<00:01, 61.57it/s, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 32/111 [00:00<00:01, 55.98it/s, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 36/111 [00:00<00:01, 51.89it/s, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 40/111 [00:00<00:01, 50.24it/s, v_num=0]\n",
      "Epoch 0:  41%|████      | 45/111 [00:00<00:01, 49.45it/s, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 52/111 [00:01<00:01, 51.35it/s, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:01<00:01, 51.76it/s, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:01<00:01, 51.75it/s, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 63/111 [00:01<00:00, 56.00it/s, v_num=0]\n",
      "Epoch 0:  65%|██████▍   | 72/111 [00:01<00:00, 59.25it/s, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 80/111 [00:01<00:00, 59.89it/s, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 87/111 [00:01<00:00, 60.53it/s, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 96/111 [00:01<00:00, 62.30it/s, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 104/111 [00:01<00:00, 63.15it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 63.71it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 151.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 119.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 114.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 103.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 104.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 99.46it/s] \u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 93.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 93.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 92.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 89.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 89.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 91.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 93.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 96.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 97.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 99.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 101.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 101.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 99.57it/s] \u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 100.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 99.18it/s] \u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 99.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 101.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 102.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 103.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 104.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 104.96it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 106.19it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 53.68it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00020_20_hidden_size=1024,num_resblocks=1,weight_decay=0.0015_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:02<00:00, 47.05it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706]\n",
      "Epoch 1:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   5%|▌         | 6/111 [00:00<00:04, 26.08it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  12%|█▏        | 13/111 [00:00<00:02, 39.12it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  13%|█▎        | 14/111 [00:00<00:02, 41.06it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  16%|█▌        | 18/111 [00:00<00:02, 40.44it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  23%|██▎       | 26/111 [00:00<00:01, 48.00it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  32%|███▏      | 35/111 [00:00<00:01, 53.93it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  41%|████      | 45/111 [00:00<00:01, 59.37it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  49%|████▊     | 54/111 [00:00<00:00, 62.79it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  52%|█████▏    | 58/111 [00:00<00:00, 62.04it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  57%|█████▋    | 63/111 [00:01<00:00, 59.50it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  60%|██████    | 67/111 [00:01<00:00, 57.68it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  61%|██████▏   | 68/111 [00:01<00:00, 57.61it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  69%|██████▉   | 77/111 [00:01<00:00, 60.21it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  75%|███████▍  | 83/111 [00:01<00:00, 61.08it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  83%|████████▎ | 92/111 [00:01<00:00, 61.88it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  88%|████████▊ | 98/111 [00:01<00:00, 61.64it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  89%|████████▉ | 99/111 [00:01<00:00, 61.81it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1:  99%|█████████▉| 110/111 [00:01<00:00, 64.51it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 64.82it/s, v_num=0, ptl/val_loss=0.794, ptl/val_auc=0.758, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 283.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 226.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 233.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 212.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 171.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 173.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 179.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 180.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 177.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 177.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 177.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 176.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 175.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 174.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 172.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 169.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 166.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 165.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 164.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 161.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 162.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 159.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 159.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 157.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 157.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 153.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 153.34it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00020_20_hidden_size=1024,num_resblocks=1,weight_decay=0.0015_2024-04-17_10-26-56/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "2024-04-17 10:30:42,184\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 151.12it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 57.48it/s, v_num=0, ptl/val_loss=0.334, ptl/val_auc=0.732, ptl/train_loss=0.706, ptl/train_auc=0.758]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:02<00:00, 52.20it/s, v_num=0, ptl/val_loss=0.334, ptl/val_auc=0.732, ptl/train_loss=0.516, ptl/train_auc=0.758]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00021_21_hidden_size=512,num_resblocks=1,weight_decay=0.0007_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "\u001b[36m(RayTrainWorker pid=22046)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 2 | resblocks | Sequential       | 263 K \n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 1.8 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 1.8 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m 7.069     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 3/111 [00:00<00:02, 46.00it/s, v_num=0]\n",
      "Epoch 0:  10%|▉         | 11/111 [00:00<00:01, 67.14it/s, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 20/111 [00:00<00:01, 74.49it/s, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 31/111 [00:00<00:00, 82.43it/s, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 32/111 [00:00<00:00, 83.48it/s, v_num=0]\n",
      "Epoch 0:  39%|███▊      | 43/111 [00:00<00:00, 89.33it/s, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 44/111 [00:00<00:00, 89.52it/s, v_num=0]\n",
      "Epoch 0:  50%|████▉     | 55/111 [00:00<00:00, 92.80it/s, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 64/111 [00:00<00:00, 92.32it/s, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 74/111 [00:00<00:00, 92.83it/s, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 86/111 [00:00<00:00, 94.99it/s, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 98/111 [00:01<00:00, 97.13it/s, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 110/111 [00:01<00:00, 99.27it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 256.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 216.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 205.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 207.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 186.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 199.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 205.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 206.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 206.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 208.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 203.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 201.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 202.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 201.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 200.37it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 99.38it/s, v_num=0]\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 201.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 200.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 198.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 197.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 197.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 193.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 191.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 187.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 187.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 187.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 185.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 183.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 182.29it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 84.81it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 79.75it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00021_21_hidden_size=512,num_resblocks=1,weight_decay=0.0007_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565]          \n",
      "Epoch 1:   4%|▎         | 4/111 [00:00<00:04, 23.72it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  10%|▉         | 11/111 [00:00<00:02, 38.66it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  19%|█▉        | 21/111 [00:00<00:01, 53.75it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  29%|██▉       | 32/111 [00:00<00:01, 64.34it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  39%|███▊      | 43/111 [00:00<00:00, 72.35it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  47%|████▋     | 52/111 [00:00<00:00, 74.81it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  48%|████▊     | 53/111 [00:00<00:00, 74.88it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  51%|█████▏    | 57/111 [00:00<00:00, 70.87it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  58%|█████▊    | 64/111 [00:00<00:00, 70.49it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  65%|██████▍   | 72/111 [00:01<00:00, 70.65it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  70%|███████   | 78/111 [00:01<00:00, 69.49it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  76%|███████▌  | 84/111 [00:01<00:00, 68.20it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  81%|████████  | 90/111 [00:01<00:00, 67.31it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1:  89%|████████▉ | 99/111 [00:01<00:00, 68.61it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 71.49it/s, v_num=0, ptl/val_loss=0.433, ptl/val_auc=0.750, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 317.65it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 266.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 226.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 212.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 197.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 187.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 188.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 182.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 178.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 181.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 183.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 178.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 178.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 180.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 179.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 177.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 174.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 174.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 173.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 170.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 168.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 167.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 168.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 168.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 168.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 169.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 170.20it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 169.78it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 63.68it/s, v_num=0, ptl/val_loss=0.657, ptl/val_auc=0.754, ptl/train_loss=0.565, ptl/train_auc=0.808]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 61.54it/s, v_num=0, ptl/val_loss=0.657, ptl/val_auc=0.754, ptl/train_loss=0.447, ptl/train_auc=0.808]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22170)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00021_21_hidden_size=512,num_resblocks=1,weight_decay=0.0007_2024-04-17_10-26-56/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=22410)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22410)\u001b[0m - (ip=172.26.79.196, pid=22594) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 1.0               833\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00022_22_hidden_size=512,num_resblocks=1,weight_decay=0.0033_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 1 | fc1       | Linear           | 1.5 M \n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 2 | resblocks | Sequential       | 263 K \n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 3 | fc2       | Linear           | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 1.8 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 1.8 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m 7.069     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=22643)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22643)\u001b[0m - (ip=172.26.79.196, pid=22727) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] \n",
      "Epoch 0:   2%|▏         | 2/111 [00:00<00:06, 16.60it/s, v_num=0]\n",
      "Epoch 0:   8%|▊         | 9/111 [00:00<00:02, 39.80it/s, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 20/111 [00:00<00:01, 59.38it/s, v_num=0]\n",
      "                                                                           \n",
      "Epoch 0:  29%|██▉       | 32/111 [00:00<00:01, 72.94it/s, v_num=0]\n",
      "Epoch 0:  39%|███▊      | 43/111 [00:00<00:00, 79.92it/s, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 44/111 [00:00<00:00, 80.31it/s, v_num=0]\n",
      "Epoch 0:  49%|████▊     | 54/111 [00:00<00:00, 83.35it/s, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 59/111 [00:00<00:00, 79.27it/s, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 60/111 [00:00<00:00, 78.87it/s, v_num=0]\n",
      "Epoch 0:  61%|██████▏   | 68/111 [00:00<00:00, 78.74it/s, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 69/111 [00:00<00:00, 79.10it/s, v_num=0]\n",
      "Epoch 0:  70%|███████   | 78/111 [00:00<00:00, 79.77it/s, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 87/111 [00:01<00:00, 81.18it/s, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 88/111 [00:01<00:00, 81.19it/s, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 96/111 [00:01<00:00, 81.26it/s, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 106/111 [00:01<00:00, 82.16it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 83.28it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 255.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 192.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 190.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 179.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 165.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 168.54it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m \n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 165.59it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m \n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 166.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 151.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 158.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 155.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 155.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 153.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 152.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 152.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 152.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 130.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 120.72it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m \n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 119.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 115.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 117.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 118.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 116.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 116.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 117.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 119.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 118.92it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 120.26it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 69.12it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 66.14it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660]\n",
      "Epoch 1:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00022_22_hidden_size=512,num_resblocks=1,weight_decay=0.0033_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   6%|▋         | 7/111 [00:00<00:01, 66.65it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:   6%|▋         | 7/111 [00:00<00:01, 66.34it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  14%|█▍        | 16/111 [00:00<00:01, 77.50it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  20%|█▉        | 22/111 [00:00<00:01, 69.42it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  20%|█▉        | 22/111 [00:00<00:01, 69.36it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  26%|██▌       | 29/111 [00:00<00:01, 70.12it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m Train : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m val : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m dtype: int64\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m Test : incident_cad\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m dtype: int64\n",
      "Epoch 1:  27%|██▋       | 30/111 [00:00<00:01, 69.83it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  33%|███▎      | 37/111 [00:00<00:01, 71.04it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  40%|███▉      | 44/111 [00:00<00:00, 69.08it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  44%|████▍     | 49/111 [00:00<00:00, 66.85it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  51%|█████▏    | 57/111 [00:00<00:00, 67.27it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  59%|█████▉    | 66/111 [00:00<00:00, 69.16it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  63%|██████▎   | 70/111 [00:01<00:00, 67.03it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  70%|███████   | 78/111 [00:01<00:00, 67.33it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  76%|███████▌  | 84/111 [00:01<00:00, 67.14it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  77%|███████▋  | 85/111 [00:01<00:00, 66.81it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  81%|████████  | 90/111 [00:01<00:00, 65.34it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  84%|████████▍ | 93/111 [00:01<00:00, 62.62it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  87%|████████▋ | 97/111 [00:01<00:00, 61.32it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1:  94%|█████████▎| 104/111 [00:01<00:00, 61.67it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  98%|█████████▊| 109/111 [00:01<00:00, 60.73it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 59.54it/s, v_num=0, ptl/val_loss=0.439, ptl/val_auc=0.757, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 47.50it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 36.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 48.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 51.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 53.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 56.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 61.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 64.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 64.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 67.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 68.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 69.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 71.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 72.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 72.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 75.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 75.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 77.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 77.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 77.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 78.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 79.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 80.06it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:31:02,467\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 79.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 80.92it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 81.31it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 111/111 [00:02<00:00, 48.23it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.660, ptl/train_auc=0.758]\n",
      "Epoch 2:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.758]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00022_22_hidden_size=512,num_resblocks=1,weight_decay=0.0033_2024-04-17_10-26-56/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m /tmp/ipykernel_23003/1866389822.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m [rank: 0] Seed set to 42\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m Missing logger folder: /tmp/ray/session_2024-04-17_10-26-51_374774_23003/artifacts/2024-04-17_10-26-55/TorchTrainer_2024-04-17_10-26-51/working_dirs/TorchTrainer_f5695_00023_23_hidden_size=256,num_resblocks=2,weight_decay=0.0005_2024-04-17_10-26-56/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   5%|▌         | 6/111 [00:00<00:01, 60.17it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  11%|█         | 12/111 [00:00<00:01, 59.71it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  12%|█▏        | 13/111 [00:00<00:01, 60.21it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  12%|█▏        | 13/111 [00:00<00:01, 60.15it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  19%|█▉        | 21/111 [00:00<00:01, 65.17it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  26%|██▌       | 29/111 [00:00<00:01, 69.99it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  32%|███▏      | 36/111 [00:00<00:01, 67.49it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  37%|███▋      | 41/111 [00:00<00:01, 65.10it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  38%|███▊      | 42/111 [00:00<00:01, 65.39it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  45%|████▌     | 50/111 [00:00<00:00, 66.88it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  51%|█████▏    | 57/111 [00:00<00:00, 66.86it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  57%|█████▋    | 63/111 [00:00<00:00, 66.05it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  65%|██████▍   | 72/111 [00:01<00:00, 67.62it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  65%|██████▍   | 72/111 [00:01<00:00, 67.60it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  72%|███████▏  | 80/111 [00:01<00:00, 68.42it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  80%|████████  | 89/111 [00:01<00:00, 69.71it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  88%|████████▊ | 98/111 [00:01<00:00, 71.22it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  89%|████████▉ | 99/111 [00:01<00:00, 71.47it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2:  97%|█████████▋| 108/111 [00:01<00:00, 72.50it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 73.08it/s, v_num=0, ptl/val_loss=0.634, ptl/val_auc=0.760, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 210.30it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 171.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 177.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 179.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 163.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 159.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 163.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 165.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 157.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 152.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 152.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 150.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 149.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 150.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 152.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 152.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 153.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 154.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 152.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 153.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 110.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 111.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 113.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 115.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 117.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 115.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 116.65it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 117.96it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 111/111 [00:01<00:00, 62.27it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.558, ptl/train_auc=0.771]\n",
      "Epoch 3:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.771]          \n",
      "Epoch 3:   5%|▍         | 5/111 [00:00<00:02, 50.97it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00022_22_hidden_size=512,num_resblocks=1,weight_decay=0.0033_2024-04-17_10-26-56/checkpoint_000002)\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m   | Name      | Type             | Params\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m 0 | norm      | LayerNorm        | 5.8 K \n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m 1 | fc1       | Linear           | 748 K \n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m 2 | resblocks | Sequential       | 132 K \n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m 3 | fc2       | Linear           | 514   \n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m 4 | loss_fn   | CrossEntropyLoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m -----------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m 887 K     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m 887 K     Total params\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m 3.549     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   6%|▋         | 7/111 [00:00<00:03, 32.39it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  11%|█         | 12/111 [00:00<00:02, 35.92it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 3:  17%|█▋        | 19/111 [00:00<00:02, 42.64it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  24%|██▍       | 27/111 [00:00<00:01, 49.23it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  33%|███▎      | 37/111 [00:00<00:01, 56.77it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  41%|████      | 45/111 [00:00<00:01, 59.88it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  49%|████▊     | 54/111 [00:00<00:00, 62.66it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  56%|█████▌    | 62/111 [00:00<00:00, 64.30it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  60%|██████    | 67/111 [00:01<00:00, 63.24it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  68%|██████▊   | 75/111 [00:01<00:00, 63.89it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  75%|███████▍  | 83/111 [00:01<00:00, 64.23it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  84%|████████▍ | 93/111 [00:01<00:00, 66.83it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  85%|████████▍ | 94/111 [00:01<00:00, 67.06it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  91%|█████████ | 101/111 [00:01<00:00, 67.30it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  98%|█████████▊| 109/111 [00:01<00:00, 67.63it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3:  98%|█████████▊| 109/111 [00:01<00:00, 67.62it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 67.88it/s, v_num=0, ptl/val_loss=0.711, ptl/val_auc=0.765, ptl/train_loss=0.531, ptl/train_auc=0.784]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▎         | 1/28 [00:00<00:00, 239.81it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 2/28 [00:00<00:00, 214.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 3/28 [00:00<00:00, 167.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 4/28 [00:00<00:00, 179.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 5/28 [00:00<00:00, 167.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██▏       | 6/28 [00:00<00:00, 154.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 7/28 [00:00<00:00, 160.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 8/28 [00:00<00:00, 156.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 9/28 [00:00<00:00, 158.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 10/28 [00:00<00:00, 152.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 11/28 [00:00<00:00, 153.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 12/28 [00:00<00:00, 157.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████▋     | 13/28 [00:00<00:00, 154.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 14/28 [00:00<00:00, 157.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████▎    | 15/28 [00:00<00:00, 159.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 16/28 [00:00<00:00, 155.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 17/28 [00:00<00:00, 155.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 18/28 [00:00<00:00, 152.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 19/28 [00:00<00:00, 149.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 20/28 [00:00<00:00, 146.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 21/28 [00:00<00:00, 146.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▊  | 22/28 [00:00<00:00, 145.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 23/28 [00:00<00:00, 146.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 24/28 [00:00<00:00, 145.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 25/28 [00:00<00:00, 146.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 26/28 [00:00<00:00, 147.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▋| 27/28 [00:00<00:00, 147.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 28/28 [00:00<00:00, 149.03it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 59.77it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.771, ptl/train_loss=0.531, ptl/train_auc=0.784]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:31:06,413\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=22594)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00022_22_hidden_size=512,num_resblocks=1,weight_decay=0.0033_2024-04-17_10-26-56/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 111/111 [00:01<00:00, 57.36it/s, v_num=0, ptl/val_loss=0.530, ptl/val_auc=0.771, ptl/train_loss=0.539, ptl/train_auc=0.784]\n",
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/val_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▌         | 6/111 [00:00<00:01, 65.10it/s, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 63/111 [00:00<00:00, 75.52it/s, v_num=0]\n",
      "Epoch 0:  71%|███████   | 79/111 [00:01<00:00, 76.44it/s, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 101/111 [00:01<00:00, 74.60it/s, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 106/111 [00:01<00:00, 72.22it/s, v_num=0]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:01<00:00, 73.06it/s, v_num=0]\n",
      "Epoch 1:   0%|          | 0/111 [00:00<?, ?it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588]          \n",
      "Epoch 1:   7%|▋         | 8/111 [00:00<00:01, 71.32it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\n",
      "Epoch 1:   7%|▋         | 8/111 [00:00<00:01, 71.09it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00023_23_hidden_size=256,num_resblocks=2,weight_decay=0.0005_2024-04-17_10-26-56/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m /home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('ptl/train_auc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  41%|████▏     | 46/111 [00:00<00:00, 87.79it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\n",
      "Epoch 0:  29%|██▉       | 32/111 [00:00<00:01, 63.54it/s, v_num=0]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 1:  66%|██████▌   | 73/111 [00:00<00:00, 87.45it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\n",
      "Epoch 1:  69%|██████▉   | 77/111 [00:00<00:00, 82.72it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\n",
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 0:  86%|████████▌ | 95/111 [00:01<00:00, 75.71it/s, v_num=0]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 1:  14%|█▍        | 16/111 [00:00<00:01, 76.70it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 1:  92%|█████████▏| 102/111 [00:01<00:00, 80.98it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\n",
      "Epoch 1:  60%|██████    | 67/111 [00:00<00:00, 91.73it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 1:  98%|█████████▊| 109/111 [00:01<00:00, 80.26it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\n",
      "Epoch 1:  99%|█████████▉| 110/111 [00:01<00:00, 80.23it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\n",
      "Epoch 1: 100%|██████████| 111/111 [00:01<00:00, 80.33it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\n",
      "Epoch 1:  86%|████████▋ | 96/111 [00:01<00:00, 83.37it/s, v_num=0, ptl/val_loss=0.602, ptl/val_auc=0.752, ptl/train_loss=0.588, ptl/train_auc=0.792]\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22727)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/xutingfeng/ray_results/TorchTrainer_2024-04-17_10-26-51/TorchTrainer_f5695_00023_23_hidden_size=256,num_resblocks=2,weight_decay=0.0005_2024-04-17_10-26-56/checkpoint_000001)\n",
      "2024-04-17 10:31:10,511\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    }
   ],
   "source": [
    "from ray.train.lightning import (\n",
    "    RayDDPStrategy,\n",
    "    RayLightningEnvironment,\n",
    "    RayTrainReportCallback,\n",
    "    prepare_trainer,\n",
    ")\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "\n",
    "def train_func(config):\n",
    "    dataset = DatasetModule(\n",
    "        train=train_imputed,\n",
    "        test=test_imputed,\n",
    "        features=used_fatures,\n",
    "        label=[\"incident_cad\"],\n",
    "        num_classes=2,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "    # model = FullyConnectedNet(\n",
    "    #     input_size=len(proteomics),\n",
    "    #     hidden_size=config[\"hidden_size\"],\n",
    "    #     output_size=2,\n",
    "    #     lr=config[\"lr\"],\n",
    "    #     weight_decay=config[\"weight_decay\"],\n",
    "    #     weight=config[\"weight\"],\n",
    "    #     num_resblocks=config[\"num_resblocks\"],\n",
    "    # )\n",
    "    model = FullyConnectedNet(**config)\n",
    "    trainer = Trainer(\n",
    "        devices=\"auto\",\n",
    "        strategy=RayDDPStrategy(),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.fit(model, dataset)\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"features\": used_fatures,\n",
    "    \"output_size\": 2,\n",
    "    #    \"hidden_size\": tune.choice([256, 128, 64]),\n",
    "    \"hidden_size\": tune.choice([256, 512, 1024]),\n",
    "    #    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"lr\": 0.0074,\n",
    "    \"weight_decay\": tune.loguniform(1e-4, 1e-2),\n",
    "    #    \"weight\": tune.choice([[1, 1], [0.1, 1], [0.1, 10], [0.1, 100]]),\n",
    "    \"weight\": [1, 1],\n",
    "    \"batch_size\": 256,\n",
    "    #    \"batch_size\": tune.choice([256]),\n",
    "    \"num_resblocks\": tune.choice([1, 2, 3, 4, 5]),\n",
    "}\n",
    "\n",
    "\n",
    "# The maximum training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Number of sampls from parameter space\n",
    "num_samples = 50\n",
    "scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 3, \"GPU\": 0.5}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"ptl/val_auc\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define a TorchTrainer without hyper-parameters for Tuner\n",
    "ray_trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "\n",
    "def tune_asha(num_samples=10):\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        ray_trainer,\n",
    "        param_space={\"train_loop_config\": search_space},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"ptl/val_auc\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "    )\n",
    "    return tuner.fit()\n",
    "\n",
    "\n",
    "results = tune_asha(num_samples=num_samples)\n",
    "\n",
    "\n",
    "# results.get_best_result(\"ptl/val_auc\")\n",
    "\n",
    "best_result = results.get_best_result(\"ptl/val_auc\")\n",
    "best_params = best_result.config\n",
    "best_result_epoch_dir = (\n",
    "    best_result.get_best_checkpoint(\"ptl/val_auc\", \"max\").path + \"/checkpoint.ckpt\"\n",
    ")\n",
    "best_model_state = torch.load(best_result_epoch_dir)\n",
    "best_model = FullyConnectedNet(**best_params[\"train_loop_config\"])\n",
    "best_model.load_state_dict(best_model_state[\"state_dict\"])\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptl/val_loss</th>\n",
       "      <th>ptl/val_auc</th>\n",
       "      <th>ptl/train_loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>checkpoint_dir_name</th>\n",
       "      <th>should_checkpoint</th>\n",
       "      <th>done</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>config/train_loop_config/features</th>\n",
       "      <th>config/train_loop_config/output_size</th>\n",
       "      <th>config/train_loop_config/hidden_size</th>\n",
       "      <th>config/train_loop_config/lr</th>\n",
       "      <th>config/train_loop_config/weight_decay</th>\n",
       "      <th>config/train_loop_config/weight</th>\n",
       "      <th>config/train_loop_config/batch_size</th>\n",
       "      <th>config/train_loop_config/num_resblocks</th>\n",
       "      <th>ptl/train_auc</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.760173</td>\n",
       "      <td>0.469385</td>\n",
       "      <td>1.876581</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250225</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.039256</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.133671</td>\n",
       "      <td>0.483149</td>\n",
       "      <td>4.743559</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250064</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>130.268478</td>\n",
       "      <td>0.493366</td>\n",
       "      <td>50.511715</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250306</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.056077</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.742477</td>\n",
       "      <td>0.494961</td>\n",
       "      <td>1.494878</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250502</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.060112</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>19.235968</td>\n",
       "      <td>0.497104</td>\n",
       "      <td>19.780983</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250422</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.073539</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.810828</td>\n",
       "      <td>0.502995</td>\n",
       "      <td>22.124250</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250441</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.046260</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.408884</td>\n",
       "      <td>0.506463</td>\n",
       "      <td>1.211488</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250415</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.029619</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.747489</td>\n",
       "      <td>0.515900</td>\n",
       "      <td>1.141289</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250301</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.079681</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.985663</td>\n",
       "      <td>0.528829</td>\n",
       "      <td>5.112461</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250269</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.082075</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.694574</td>\n",
       "      <td>0.533914</td>\n",
       "      <td>9.452261</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250263</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.061577</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.216439</td>\n",
       "      <td>0.547156</td>\n",
       "      <td>0.198814</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250318</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.020983</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.430613</td>\n",
       "      <td>0.630838</td>\n",
       "      <td>1.477843</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250183</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.043229</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.187396</td>\n",
       "      <td>0.654339</td>\n",
       "      <td>0.199294</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250357</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.016467</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.337921</td>\n",
       "      <td>0.690561</td>\n",
       "      <td>0.301504</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250453</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.201257</td>\n",
       "      <td>0.693016</td>\n",
       "      <td>0.330223</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250288</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.052408</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.397255</td>\n",
       "      <td>0.698218</td>\n",
       "      <td>0.274294</td>\n",
       "      <td>3</td>\n",
       "      <td>444</td>\n",
       "      <td>1713250380</td>\n",
       "      <td>checkpoint_000003</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.763963</td>\n",
       "      <td>b1adb_00033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.610312</td>\n",
       "      <td>0.701725</td>\n",
       "      <td>0.375353</td>\n",
       "      <td>3</td>\n",
       "      <td>444</td>\n",
       "      <td>1713250094</td>\n",
       "      <td>checkpoint_000003</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.747862</td>\n",
       "      <td>b1adb_00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.639389</td>\n",
       "      <td>0.703730</td>\n",
       "      <td>2.303217</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250252</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.391116</td>\n",
       "      <td>0.704912</td>\n",
       "      <td>0.520778</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250234</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.609961</td>\n",
       "      <td>0.711256</td>\n",
       "      <td>1.394719</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250171</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.009160</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.405460</td>\n",
       "      <td>0.714494</td>\n",
       "      <td>0.484575</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250208</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.716115</td>\n",
       "      <td>0.323404</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250189</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.396205</td>\n",
       "      <td>0.716602</td>\n",
       "      <td>0.267928</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250435</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>0.805707</td>\n",
       "      <td>b1adb_00039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.532975</td>\n",
       "      <td>0.717194</td>\n",
       "      <td>0.467302</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250535</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.375265</td>\n",
       "      <td>0.717894</td>\n",
       "      <td>0.321704</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250337</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.820951</td>\n",
       "      <td>b1adb_00030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.285297</td>\n",
       "      <td>0.718358</td>\n",
       "      <td>0.300852</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250525</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010011</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.317792</td>\n",
       "      <td>0.721067</td>\n",
       "      <td>0.086505</td>\n",
       "      <td>3</td>\n",
       "      <td>444</td>\n",
       "      <td>1713250147</td>\n",
       "      <td>checkpoint_000003</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.908599</td>\n",
       "      <td>b1adb_00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.422811</td>\n",
       "      <td>0.721171</td>\n",
       "      <td>0.488164</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250520</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.596747</td>\n",
       "      <td>0.721458</td>\n",
       "      <td>0.629529</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250377</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.505787</td>\n",
       "      <td>0.722374</td>\n",
       "      <td>0.507059</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250100</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.368252</td>\n",
       "      <td>0.725346</td>\n",
       "      <td>0.275299</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250112</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.004786</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.136108</td>\n",
       "      <td>0.731303</td>\n",
       "      <td>0.079632</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250245</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759838</td>\n",
       "      <td>b1adb_00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.661766</td>\n",
       "      <td>0.733042</td>\n",
       "      <td>2.689992</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250152</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.029926</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>[0.1, 100]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.480903</td>\n",
       "      <td>0.736904</td>\n",
       "      <td>0.548813</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1713250119</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b1adb_00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.311506</td>\n",
       "      <td>0.738047</td>\n",
       "      <td>0.320365</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250326</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.801510</td>\n",
       "      <td>b1adb_00029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.159112</td>\n",
       "      <td>0.740037</td>\n",
       "      <td>0.151974</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250084</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.025533</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>0.603852</td>\n",
       "      <td>b1adb_00004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.124090</td>\n",
       "      <td>0.743035</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>3</td>\n",
       "      <td>444</td>\n",
       "      <td>1713250463</td>\n",
       "      <td>checkpoint_000003</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.808543</td>\n",
       "      <td>b1adb_00042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.146915</td>\n",
       "      <td>0.745466</td>\n",
       "      <td>0.113502</td>\n",
       "      <td>3</td>\n",
       "      <td>444</td>\n",
       "      <td>1713250404</td>\n",
       "      <td>checkpoint_000003</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.792660</td>\n",
       "      <td>b1adb_00036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.369092</td>\n",
       "      <td>0.745673</td>\n",
       "      <td>0.264192</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250397</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.042761</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.594735</td>\n",
       "      <td>b1adb_00035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.174659</td>\n",
       "      <td>0.746515</td>\n",
       "      <td>0.118818</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250482</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.771894</td>\n",
       "      <td>b1adb_00044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.460408</td>\n",
       "      <td>0.746610</td>\n",
       "      <td>0.330123</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250132</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.778733</td>\n",
       "      <td>b1adb_00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.363332</td>\n",
       "      <td>0.748374</td>\n",
       "      <td>0.425849</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250472</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.011132</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.737964</td>\n",
       "      <td>b1adb_00043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.574566</td>\n",
       "      <td>0.749706</td>\n",
       "      <td>0.577770</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250030</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>0.689252</td>\n",
       "      <td>b1adb_00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.496301</td>\n",
       "      <td>0.750191</td>\n",
       "      <td>0.258489</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250283</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.036407</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>0.541519</td>\n",
       "      <td>b1adb_00024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.170637</td>\n",
       "      <td>0.750969</td>\n",
       "      <td>0.111174</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250165</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.752905</td>\n",
       "      <td>b1adb_00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.325929</td>\n",
       "      <td>0.751750</td>\n",
       "      <td>0.200038</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1713250072</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>0.755068</td>\n",
       "      <td>b1adb_00003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.314046</td>\n",
       "      <td>0.772317</td>\n",
       "      <td>0.227965</td>\n",
       "      <td>7</td>\n",
       "      <td>888</td>\n",
       "      <td>1713250356</td>\n",
       "      <td>checkpoint_000007</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.781160</td>\n",
       "      <td>b1adb_00031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.494547</td>\n",
       "      <td>0.778508</td>\n",
       "      <td>0.397803</td>\n",
       "      <td>9</td>\n",
       "      <td>1110</td>\n",
       "      <td>1713250048</td>\n",
       "      <td>checkpoint_000009</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.842529</td>\n",
       "      <td>b1adb_00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.461955</td>\n",
       "      <td>0.783903</td>\n",
       "      <td>0.503716</td>\n",
       "      <td>9</td>\n",
       "      <td>1110</td>\n",
       "      <td>1713250216</td>\n",
       "      <td>checkpoint_000009</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.801768</td>\n",
       "      <td>b1adb_00016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.655194</td>\n",
       "      <td>0.801924</td>\n",
       "      <td>0.508758</td>\n",
       "      <td>9</td>\n",
       "      <td>1110</td>\n",
       "      <td>1713250506</td>\n",
       "      <td>checkpoint_000009</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>[C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.800488</td>\n",
       "      <td>b1adb_00045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ptl/val_loss  ptl/val_auc  ptl/train_loss  epoch  step   timestamp  \\\n",
       "18      1.760173     0.469385        1.876581      0   111  1713250225   \n",
       "2      14.133671     0.483149        4.743559      0   111  1713250064   \n",
       "27    130.268478     0.493366       50.511715      0   111  1713250306   \n",
       "46      0.742477     0.494961        1.494878      0   111  1713250502   \n",
       "38     19.235968     0.497104       19.780983      0   111  1713250422   \n",
       "40      2.810828     0.502995       22.124250      0   111  1713250441   \n",
       "37      0.408884     0.506463        1.211488      0   111  1713250415   \n",
       "26      0.747489     0.515900        1.141289      0   111  1713250301   \n",
       "23      0.985663     0.528829        5.112461      0   111  1713250269   \n",
       "22      9.694574     0.533914        9.452261      0   111  1713250263   \n",
       "28      0.216439     0.547156        0.198814      0   111  1713250318   \n",
       "14      0.430613     0.630838        1.477843      0   111  1713250183   \n",
       "32      0.187396     0.654339        0.199294      0   111  1713250357   \n",
       "41      0.337921     0.690561        0.301504      0   111  1713250453   \n",
       "25      0.201257     0.693016        0.330223      0   111  1713250288   \n",
       "33      0.397255     0.698218        0.274294      3   444  1713250380   \n",
       "5       0.610312     0.701725        0.375353      3   444  1713250094   \n",
       "21      0.639389     0.703730        2.303217      0   111  1713250252   \n",
       "19      0.391116     0.704912        0.520778      0   111  1713250234   \n",
       "13      0.609961     0.711256        1.394719      0   111  1713250171   \n",
       "17      0.405460     0.714494        0.484575      0   111  1713250208   \n",
       "15      0.396000     0.716115        0.323404      0   111  1713250189   \n",
       "39      0.396205     0.716602        0.267928      1   222  1713250435   \n",
       "49      0.532975     0.717194        0.467302      0   111  1713250535   \n",
       "30      0.375265     0.717894        0.321704      1   222  1713250337   \n",
       "48      0.285297     0.718358        0.300852      0   111  1713250525   \n",
       "10      0.317792     0.721067        0.086505      3   444  1713250147   \n",
       "47      0.422811     0.721171        0.488164      0   111  1713250520   \n",
       "34      0.596747     0.721458        0.629529      0   111  1713250377   \n",
       "6       0.505787     0.722374        0.507059      0   111  1713250100   \n",
       "7       0.368252     0.725346        0.275299      0   111  1713250112   \n",
       "20      0.136108     0.731303        0.079632      1   222  1713250245   \n",
       "11      0.661766     0.733042        2.689992      0   111  1713250152   \n",
       "8       0.480903     0.736904        0.548813      0   111  1713250119   \n",
       "29      0.311506     0.738047        0.320365      1   222  1713250326   \n",
       "4       0.159112     0.740037        0.151974      1   222  1713250084   \n",
       "42      0.124090     0.743035        0.064941      3   444  1713250463   \n",
       "36      0.146915     0.745466        0.113502      3   444  1713250404   \n",
       "35      0.369092     0.745673        0.264192      1   222  1713250397   \n",
       "44      0.174659     0.746515        0.118818      1   222  1713250482   \n",
       "9       0.460408     0.746610        0.330123      1   222  1713250132   \n",
       "43      0.363332     0.748374        0.425849      1   222  1713250472   \n",
       "0       0.574566     0.749706        0.577770      1   222  1713250030   \n",
       "24      0.496301     0.750191        0.258489      1   222  1713250283   \n",
       "12      0.170637     0.750969        0.111174      1   222  1713250165   \n",
       "3       0.325929     0.751750        0.200038      1   222  1713250072   \n",
       "31      0.314046     0.772317        0.227965      7   888  1713250356   \n",
       "1       0.494547     0.778508        0.397803      9  1110  1713250048   \n",
       "16      0.461955     0.783903        0.503716      9  1110  1713250216   \n",
       "45      0.655194     0.801924        0.508758      9  1110  1713250506   \n",
       "\n",
       "   checkpoint_dir_name  should_checkpoint  done  training_iteration  ...  \\\n",
       "18   checkpoint_000000               True  True                   1  ...   \n",
       "2    checkpoint_000000               True  True                   1  ...   \n",
       "27   checkpoint_000000               True  True                   1  ...   \n",
       "46   checkpoint_000000               True  True                   1  ...   \n",
       "38   checkpoint_000000               True  True                   1  ...   \n",
       "40   checkpoint_000000               True  True                   1  ...   \n",
       "37   checkpoint_000000               True  True                   1  ...   \n",
       "26   checkpoint_000000               True  True                   1  ...   \n",
       "23   checkpoint_000000               True  True                   1  ...   \n",
       "22   checkpoint_000000               True  True                   1  ...   \n",
       "28   checkpoint_000000               True  True                   1  ...   \n",
       "14   checkpoint_000000               True  True                   1  ...   \n",
       "32   checkpoint_000000               True  True                   1  ...   \n",
       "41   checkpoint_000000               True  True                   1  ...   \n",
       "25   checkpoint_000000               True  True                   1  ...   \n",
       "33   checkpoint_000003               True  True                   4  ...   \n",
       "5    checkpoint_000003               True  True                   4  ...   \n",
       "21   checkpoint_000000               True  True                   1  ...   \n",
       "19   checkpoint_000000               True  True                   1  ...   \n",
       "13   checkpoint_000000               True  True                   1  ...   \n",
       "17   checkpoint_000000               True  True                   1  ...   \n",
       "15   checkpoint_000000               True  True                   1  ...   \n",
       "39   checkpoint_000001               True  True                   2  ...   \n",
       "49   checkpoint_000000               True  True                   1  ...   \n",
       "30   checkpoint_000001               True  True                   2  ...   \n",
       "48   checkpoint_000000               True  True                   1  ...   \n",
       "10   checkpoint_000003               True  True                   4  ...   \n",
       "47   checkpoint_000000               True  True                   1  ...   \n",
       "34   checkpoint_000000               True  True                   1  ...   \n",
       "6    checkpoint_000000               True  True                   1  ...   \n",
       "7    checkpoint_000000               True  True                   1  ...   \n",
       "20   checkpoint_000001               True  True                   2  ...   \n",
       "11   checkpoint_000000               True  True                   1  ...   \n",
       "8    checkpoint_000000               True  True                   1  ...   \n",
       "29   checkpoint_000001               True  True                   2  ...   \n",
       "4    checkpoint_000001               True  True                   2  ...   \n",
       "42   checkpoint_000003               True  True                   4  ...   \n",
       "36   checkpoint_000003               True  True                   4  ...   \n",
       "35   checkpoint_000001               True  True                   2  ...   \n",
       "44   checkpoint_000001               True  True                   2  ...   \n",
       "9    checkpoint_000001               True  True                   2  ...   \n",
       "43   checkpoint_000001               True  True                   2  ...   \n",
       "0    checkpoint_000001               True  True                   2  ...   \n",
       "24   checkpoint_000001               True  True                   2  ...   \n",
       "12   checkpoint_000001               True  True                   2  ...   \n",
       "3    checkpoint_000001               True  True                   2  ...   \n",
       "31   checkpoint_000007               True  True                   8  ...   \n",
       "1    checkpoint_000009               True  True                  10  ...   \n",
       "16   checkpoint_000009               True  True                  10  ...   \n",
       "45   checkpoint_000009               True  True                  10  ...   \n",
       "\n",
       "                    config/train_loop_config/features  \\\n",
       "18  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "2   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "27  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "46  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "38  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "40  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "37  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "26  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "23  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "22  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "28  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "14  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "32  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "41  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "25  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "33  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "5   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "21  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "19  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "13  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "17  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "15  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "39  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "49  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "30  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "48  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "10  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "47  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "34  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "6   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "7   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "20  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "11  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "8   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "29  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "4   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "42  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "36  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "35  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "44  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "9   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "43  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "0   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "24  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "12  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "3   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "31  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "1   [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "16  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "45  [C3, KLK7, GCHFR, NHLRC3, APOD, GAPDH, TP53I3,...   \n",
       "\n",
       "   config/train_loop_config/output_size  config/train_loop_config/hidden_size  \\\n",
       "18                                    2                                   128   \n",
       "2                                     2                                   256   \n",
       "27                                    2                                   128   \n",
       "46                                    2                                   256   \n",
       "38                                    2                                   256   \n",
       "40                                    2                                   128   \n",
       "37                                    2                                   128   \n",
       "26                                    2                                    64   \n",
       "23                                    2                                    64   \n",
       "22                                    2                                    64   \n",
       "28                                    2                                    64   \n",
       "14                                    2                                   128   \n",
       "32                                    2                                   128   \n",
       "41                                    2                                    64   \n",
       "25                                    2                                    64   \n",
       "33                                    2                                    64   \n",
       "5                                     2                                    64   \n",
       "21                                    2                                   256   \n",
       "19                                    2                                    64   \n",
       "13                                    2                                   128   \n",
       "17                                    2                                    64   \n",
       "15                                    2                                   128   \n",
       "39                                    2                                    64   \n",
       "49                                    2                                   128   \n",
       "30                                    2                                   256   \n",
       "48                                    2                                    64   \n",
       "10                                    2                                   128   \n",
       "47                                    2                                   128   \n",
       "34                                    2                                   128   \n",
       "6                                     2                                   256   \n",
       "7                                     2                                   128   \n",
       "20                                    2                                   256   \n",
       "11                                    2                                   128   \n",
       "8                                     2                                    64   \n",
       "29                                    2                                   256   \n",
       "4                                     2                                   128   \n",
       "42                                    2                                    64   \n",
       "36                                    2                                   256   \n",
       "35                                    2                                    64   \n",
       "44                                    2                                   128   \n",
       "9                                     2                                    64   \n",
       "43                                    2                                   256   \n",
       "0                                     2                                    64   \n",
       "24                                    2                                    64   \n",
       "12                                    2                                    64   \n",
       "3                                     2                                    64   \n",
       "31                                    2                                   128   \n",
       "1                                     2                                   256   \n",
       "16                                    2                                   128   \n",
       "45                                    2                                   256   \n",
       "\n",
       "    config/train_loop_config/lr  config/train_loop_config/weight_decay  \\\n",
       "18                     0.039256                               0.000113   \n",
       "2                      0.019132                               0.004371   \n",
       "27                     0.056077                               0.008174   \n",
       "46                     0.060112                               0.002438   \n",
       "38                     0.073539                               0.001687   \n",
       "40                     0.046260                               0.000291   \n",
       "37                     0.029619                               0.003661   \n",
       "26                     0.079681                               0.000378   \n",
       "23                     0.082075                               0.004236   \n",
       "22                     0.061577                               0.007046   \n",
       "28                     0.020983                               0.006452   \n",
       "14                     0.043229                               0.000111   \n",
       "32                     0.016467                               0.003274   \n",
       "41                     0.004604                               0.007526   \n",
       "25                     0.052408                               0.000272   \n",
       "33                     0.002972                               0.002187   \n",
       "5                      0.001020                               0.009085   \n",
       "21                     0.019512                               0.000632   \n",
       "19                     0.000249                               0.006375   \n",
       "13                     0.009160                               0.002587   \n",
       "17                     0.000968                               0.000911   \n",
       "15                     0.000201                               0.004914   \n",
       "39                     0.000452                               0.000131   \n",
       "49                     0.004666                               0.001794   \n",
       "30                     0.002443                               0.000650   \n",
       "48                     0.010011                               0.000204   \n",
       "10                     0.000124                               0.000105   \n",
       "47                     0.000722                               0.001531   \n",
       "34                     0.000249                               0.000322   \n",
       "6                      0.000500                               0.003215   \n",
       "7                      0.000215                               0.004786   \n",
       "20                     0.000358                               0.000414   \n",
       "11                     0.029926                               0.000226   \n",
       "8                      0.000385                               0.000449   \n",
       "29                     0.000167                               0.003141   \n",
       "4                      0.025533                               0.000624   \n",
       "42                     0.000253                               0.002836   \n",
       "36                     0.004880                               0.000190   \n",
       "35                     0.042761                               0.000364   \n",
       "44                     0.002722                               0.001351   \n",
       "9                      0.000426                               0.000292   \n",
       "43                     0.011132                               0.000244   \n",
       "0                      0.030418                               0.005366   \n",
       "24                     0.036407                               0.000136   \n",
       "12                     0.001139                               0.003347   \n",
       "3                      0.003344                               0.000185   \n",
       "31                     0.004398                               0.000404   \n",
       "1                      0.003570                               0.003723   \n",
       "16                     0.005552                               0.006661   \n",
       "45                     0.007395                               0.004496   \n",
       "\n",
       "   config/train_loop_config/weight config/train_loop_config/batch_size  \\\n",
       "18                       [0.1, 10]                                 256   \n",
       "2                       [0.1, 100]                                 256   \n",
       "27                      [0.1, 100]                                 256   \n",
       "46                          [1, 1]                                 256   \n",
       "38                       [0.1, 10]                                 256   \n",
       "40                      [0.1, 100]                                 256   \n",
       "37                       [0.1, 10]                                 256   \n",
       "26                          [1, 1]                                 256   \n",
       "23                      [0.1, 100]                                 256   \n",
       "22                       [0.1, 10]                                 256   \n",
       "28                        [0.1, 1]                                 256   \n",
       "14                       [0.1, 10]                                 256   \n",
       "32                        [0.1, 1]                                 256   \n",
       "41                       [0.1, 10]                                 256   \n",
       "25                        [0.1, 1]                                 256   \n",
       "33                       [0.1, 10]                                 256   \n",
       "5                       [0.1, 100]                                 256   \n",
       "21                      [0.1, 100]                                 256   \n",
       "19                       [0.1, 10]                                 256   \n",
       "13                      [0.1, 100]                                 256   \n",
       "17                          [1, 1]                                 256   \n",
       "15                       [0.1, 10]                                 256   \n",
       "39                          [1, 1]                                 256   \n",
       "49                      [0.1, 100]                                 256   \n",
       "30                          [1, 1]                                 256   \n",
       "48                       [0.1, 10]                                 256   \n",
       "10                          [1, 1]                                 256   \n",
       "47                          [1, 1]                                 256   \n",
       "34                      [0.1, 100]                                 256   \n",
       "6                           [1, 1]                                 256   \n",
       "7                        [0.1, 10]                                 256   \n",
       "20                        [0.1, 1]                                 256   \n",
       "11                      [0.1, 100]                                 256   \n",
       "8                           [1, 1]                                 256   \n",
       "29                          [1, 1]                                 256   \n",
       "4                         [0.1, 1]                                 256   \n",
       "42                        [0.1, 1]                                 256   \n",
       "36                        [0.1, 1]                                 256   \n",
       "35                       [0.1, 10]                                 256   \n",
       "44                        [0.1, 1]                                 256   \n",
       "9                           [1, 1]                                 256   \n",
       "43                          [1, 1]                                 256   \n",
       "0                           [1, 1]                                 256   \n",
       "24                       [0.1, 10]                                 256   \n",
       "12                        [0.1, 1]                                 256   \n",
       "3                        [0.1, 10]                                 256   \n",
       "31                       [0.1, 10]                                 256   \n",
       "1                           [1, 1]                                 256   \n",
       "16                          [1, 1]                                 256   \n",
       "45                          [1, 1]                                 256   \n",
       "\n",
       "    config/train_loop_config/num_resblocks  ptl/train_auc       logdir  \n",
       "18                                       5            NaN  b1adb_00018  \n",
       "2                                        5            NaN  b1adb_00002  \n",
       "27                                       3            NaN  b1adb_00027  \n",
       "46                                       3            NaN  b1adb_00046  \n",
       "38                                       4            NaN  b1adb_00038  \n",
       "40                                       4            NaN  b1adb_00040  \n",
       "37                                       5            NaN  b1adb_00037  \n",
       "26                                       4            NaN  b1adb_00026  \n",
       "23                                       4            NaN  b1adb_00023  \n",
       "22                                       3            NaN  b1adb_00022  \n",
       "28                                       5            NaN  b1adb_00028  \n",
       "14                                       3            NaN  b1adb_00014  \n",
       "32                                       4            NaN  b1adb_00032  \n",
       "41                                       5            NaN  b1adb_00041  \n",
       "25                                       4            NaN  b1adb_00025  \n",
       "33                                       3       0.763963  b1adb_00033  \n",
       "5                                        4       0.747862  b1adb_00005  \n",
       "21                                       3            NaN  b1adb_00021  \n",
       "19                                       4            NaN  b1adb_00019  \n",
       "13                                       3            NaN  b1adb_00013  \n",
       "17                                       5            NaN  b1adb_00017  \n",
       "15                                       3            NaN  b1adb_00015  \n",
       "39                                       5       0.805707  b1adb_00039  \n",
       "49                                       3            NaN  b1adb_00049  \n",
       "30                                       3       0.820951  b1adb_00030  \n",
       "48                                       3            NaN  b1adb_00048  \n",
       "10                                       3       0.908599  b1adb_00010  \n",
       "47                                       3            NaN  b1adb_00047  \n",
       "34                                       4            NaN  b1adb_00034  \n",
       "6                                        4            NaN  b1adb_00006  \n",
       "7                                        3            NaN  b1adb_00007  \n",
       "20                                       3       0.759838  b1adb_00020  \n",
       "11                                       3            NaN  b1adb_00011  \n",
       "8                                        4            NaN  b1adb_00008  \n",
       "29                                       3       0.801510  b1adb_00029  \n",
       "4                                        5       0.603852  b1adb_00004  \n",
       "42                                       4       0.808543  b1adb_00042  \n",
       "36                                       3       0.792660  b1adb_00036  \n",
       "35                                       3       0.594735  b1adb_00035  \n",
       "44                                       4       0.771894  b1adb_00044  \n",
       "9                                        4       0.778733  b1adb_00009  \n",
       "43                                       4       0.737964  b1adb_00043  \n",
       "0                                        5       0.689252  b1adb_00000  \n",
       "24                                       5       0.541519  b1adb_00024  \n",
       "12                                       3       0.752905  b1adb_00012  \n",
       "3                                        5       0.755068  b1adb_00003  \n",
       "31                                       3       0.781160  b1adb_00031  \n",
       "1                                        4       0.842529  b1adb_00001  \n",
       "16                                       4       0.801768  b1adb_00016  \n",
       "45                                       3       0.800488  b1adb_00045  \n",
       "\n",
       "[50 rows x 29 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.get_dataframe().sort_values(\"ptl/val_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullyConnectedNet(\n",
       "  (norm): LayerNorm((2922,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=2922, out_features=256, bias=True)\n",
       "  (resblocks): Sequential(\n",
       "    (0): LinearResBlock(\n",
       "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): LinearResBlock(\n",
       "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): LinearResBlock(\n",
       "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results.get_best_result(\"ptl/val_auc\")\n",
    "\n",
    "best_result = results.get_best_result(\"ptl/val_auc\")\n",
    "best_params = best_result.config\n",
    "best_result_epoch_dir = (\n",
    "    best_result.get_best_checkpoint(\"ptl/val_auc\", \"max\").path + \"/checkpoint.ckpt\"\n",
    ")\n",
    "best_model_state = torch.load(best_result_epoch_dir)\n",
    "best_model = FullyConnectedNet(**best_params[\"train_loop_config\"])\n",
    "best_model.load_state_dict(best_model_state[\"state_dict\"])\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': ['C3',\n",
       "  'KLK7',\n",
       "  'GCHFR',\n",
       "  'NHLRC3',\n",
       "  'APOD',\n",
       "  'GAPDH',\n",
       "  'TP53I3',\n",
       "  'CPA4',\n",
       "  'ANXA2',\n",
       "  'GRSF1',\n",
       "  'IL25',\n",
       "  'HMMR',\n",
       "  'MRPL52',\n",
       "  'PAIP2B',\n",
       "  'THAP12',\n",
       "  'FOS',\n",
       "  'FGF9',\n",
       "  'PITHD1',\n",
       "  'THSD1',\n",
       "  'PTGES2',\n",
       "  'DEFB103A_DEFB103B',\n",
       "  'ATP1B4',\n",
       "  'CYB5A',\n",
       "  'UNC79',\n",
       "  'SLC34A3',\n",
       "  'TAGLN3',\n",
       "  'SLIRP',\n",
       "  'CLASP1',\n",
       "  'PSMC3',\n",
       "  'KIR3DL2',\n",
       "  'BEX3',\n",
       "  'PFDN4',\n",
       "  'BCL7A',\n",
       "  'SMC3',\n",
       "  'SLC28A1',\n",
       "  'CDC123',\n",
       "  'GJA8',\n",
       "  'NMRK2',\n",
       "  'GATA3',\n",
       "  'CPLX2',\n",
       "  'RASGRF1',\n",
       "  'FGF7',\n",
       "  'ANKRA2',\n",
       "  'RBM25',\n",
       "  'LYZL2',\n",
       "  'CDK1',\n",
       "  'CREB3',\n",
       "  'CREBZF',\n",
       "  'IGLON5',\n",
       "  'SHC1',\n",
       "  'ZP4',\n",
       "  'TMOD4',\n",
       "  'CEP152',\n",
       "  'MYH7B',\n",
       "  'CEP350',\n",
       "  'CDC25A',\n",
       "  'TRIM26',\n",
       "  'MANEAL',\n",
       "  'MUCL3',\n",
       "  'GIMAP8',\n",
       "  'CYTH3',\n",
       "  'PDXDC1',\n",
       "  'CLINT1',\n",
       "  'MAPRE3',\n",
       "  'EVI2B',\n",
       "  'STAU1',\n",
       "  'PCNA',\n",
       "  'DNAJA1',\n",
       "  'JMJD1C',\n",
       "  'GAGE2A',\n",
       "  'GAD1',\n",
       "  'IZUMO1',\n",
       "  'PDCL2',\n",
       "  'PDE1C',\n",
       "  'STOML2',\n",
       "  'BSND',\n",
       "  'MAPK13',\n",
       "  'PDIA2',\n",
       "  'BTLA',\n",
       "  'MLLT1',\n",
       "  'TPRKB',\n",
       "  'ARHGAP5',\n",
       "  'BTNL10',\n",
       "  'PHLDB2',\n",
       "  'PDIA5',\n",
       "  'ATF4',\n",
       "  'PRAME',\n",
       "  'TOP1MT',\n",
       "  'KHDC3L',\n",
       "  'DCUN1D2',\n",
       "  'IL3',\n",
       "  'DCLRE1C',\n",
       "  'ERCC1',\n",
       "  'DCDC2C',\n",
       "  'VCPKMT',\n",
       "  'SPRING1',\n",
       "  'MORN4',\n",
       "  'ESPL1',\n",
       "  'H2AP',\n",
       "  'MORF4L2',\n",
       "  'SSH3',\n",
       "  'VWA5A',\n",
       "  'PBK',\n",
       "  'REST',\n",
       "  'SHD',\n",
       "  'TXNL1',\n",
       "  'TPM3',\n",
       "  'NEB',\n",
       "  'ATP1B2',\n",
       "  'CEP112',\n",
       "  'SART1',\n",
       "  'ATP6V1G2',\n",
       "  'ATP2B4',\n",
       "  'SAT1',\n",
       "  'ATP1B1',\n",
       "  'NECAP2',\n",
       "  'ATP5F1D',\n",
       "  'ATP1B3',\n",
       "  'ARNTL',\n",
       "  'ARL2BP',\n",
       "  'SCGB2A2',\n",
       "  'GAMT',\n",
       "  'ASS1',\n",
       "  'NFYA',\n",
       "  'GASK1A',\n",
       "  'MANSC4',\n",
       "  'HMGCS1',\n",
       "  'MMUT',\n",
       "  'CBX2',\n",
       "  'BRD3',\n",
       "  'BRDT',\n",
       "  'MAP1LC3B2',\n",
       "  'CASQ2',\n",
       "  'HIP1',\n",
       "  'GSTM4',\n",
       "  'GUK1',\n",
       "  'CALY',\n",
       "  'C1GALT1C1',\n",
       "  'TEF',\n",
       "  'CACNA1H',\n",
       "  'HADH',\n",
       "  'MEGF11',\n",
       "  'MED21',\n",
       "  'THRAP3',\n",
       "  'SPINK8',\n",
       "  'NAA10',\n",
       "  'MRPL24',\n",
       "  'GBP6',\n",
       "  'MYOM2',\n",
       "  'B3GAT3',\n",
       "  'GCLM',\n",
       "  'MYL1',\n",
       "  'HSD17B3',\n",
       "  'MYH4',\n",
       "  'TMED4',\n",
       "  'TMED10',\n",
       "  'SKIV2L',\n",
       "  'SLC12A2',\n",
       "  'SLC51B',\n",
       "  'MTR',\n",
       "  'CD2',\n",
       "  'BHMT2',\n",
       "  'SNU13',\n",
       "  'GP1BB',\n",
       "  'ARL13B',\n",
       "  'HCG22',\n",
       "  'RYR1',\n",
       "  'FDX2',\n",
       "  'ADRA2A',\n",
       "  'ERVV-1',\n",
       "  'EXOSC10',\n",
       "  'EXTL1',\n",
       "  'CYP24A1',\n",
       "  'KIF1C',\n",
       "  'USP47',\n",
       "  'PRKD2',\n",
       "  'PROCR',\n",
       "  'PACS2',\n",
       "  'KIF22',\n",
       "  'NXPE4',\n",
       "  'RTKN2',\n",
       "  'CSRP3',\n",
       "  'NUDT15',\n",
       "  'UHRF2',\n",
       "  'UGDH',\n",
       "  'CSF2',\n",
       "  'KRT17',\n",
       "  'FDX1',\n",
       "  'PYY',\n",
       "  'UBQLN3',\n",
       "  'CSDE1',\n",
       "  'DDA1',\n",
       "  'PALM3',\n",
       "  'VSIG10L',\n",
       "  'PKD2',\n",
       "  'ABCA2',\n",
       "  'EDEM2',\n",
       "  'ABRAXAS2',\n",
       "  'ECI2',\n",
       "  'PGLYRP4',\n",
       "  'PDZD2',\n",
       "  'EIF2AK3',\n",
       "  'EIF5',\n",
       "  'ELOB',\n",
       "  'ITPA',\n",
       "  'ACSL1',\n",
       "  'DENND2B',\n",
       "  'ZCCHC8',\n",
       "  'ACTN2',\n",
       "  'PDE4D',\n",
       "  'ACY3',\n",
       "  'ENOX2',\n",
       "  'YOD1',\n",
       "  'ENPEP',\n",
       "  'PMCH',\n",
       "  'PMM2',\n",
       "  'DHODH',\n",
       "  'KRT6C',\n",
       "  'NUP50',\n",
       "  'LAMA1',\n",
       "  'COPB2',\n",
       "  'LRCH4',\n",
       "  'TSNAX',\n",
       "  'LPP',\n",
       "  'TRPV3',\n",
       "  'IGHMBP2',\n",
       "  'LILRA4',\n",
       "  'FHIP2A',\n",
       "  'NOP56',\n",
       "  'RIPK4',\n",
       "  'TRAF3IP2',\n",
       "  'IGF2BP3',\n",
       "  'NFKB1',\n",
       "  'NFX1',\n",
       "  'REXO2',\n",
       "  'TSPAN15',\n",
       "  'RBM19',\n",
       "  'FRMD4B',\n",
       "  'NOS2',\n",
       "  'TPR',\n",
       "  'NPR1',\n",
       "  'RAB33A',\n",
       "  'RAB39B',\n",
       "  'RPS10',\n",
       "  'ANK2',\n",
       "  'IFNW1',\n",
       "  'CPTP',\n",
       "  'TTN',\n",
       "  'IL36G',\n",
       "  'IL31RA',\n",
       "  'RNASE4',\n",
       "  'LRIG3',\n",
       "  'CACNA1C',\n",
       "  'SCIN',\n",
       "  'DNLZ',\n",
       "  'STEAP4',\n",
       "  'CBLN1',\n",
       "  'CHP1',\n",
       "  'SAG',\n",
       "  'DOCK9',\n",
       "  'RRP15',\n",
       "  'SYNGAP1',\n",
       "  'CNTF',\n",
       "  'ECSCR',\n",
       "  'ELAVL4',\n",
       "  'FZD8',\n",
       "  'SCN2A',\n",
       "  'CNGB3',\n",
       "  'GABRA4',\n",
       "  'CACNB1',\n",
       "  'DEFB118',\n",
       "  'PNMA2',\n",
       "  'SMS',\n",
       "  'CDH4',\n",
       "  'SH3BGRL2',\n",
       "  'RAB3GAP1',\n",
       "  'RANBP2',\n",
       "  'MYOM1',\n",
       "  'CDKL5',\n",
       "  'CSPG5',\n",
       "  'CTNNA1',\n",
       "  'OMP',\n",
       "  'OTOA',\n",
       "  'GLP1R',\n",
       "  'CEND1',\n",
       "  'SNAP25',\n",
       "  'PCARE',\n",
       "  'FH',\n",
       "  'CORO6',\n",
       "  'SCN3B',\n",
       "  'DCUN1D1',\n",
       "  'NLGN2',\n",
       "  'DEFB104A_DEFB104B',\n",
       "  'DEFB116',\n",
       "  'CRYM',\n",
       "  'SPTBN2',\n",
       "  'GPR101',\n",
       "  'DGCR6',\n",
       "  'GRIN2B',\n",
       "  'ZPR1',\n",
       "  'CD3D',\n",
       "  'HTR1A',\n",
       "  'TFAP2A',\n",
       "  'BLOC1S2',\n",
       "  'IMPG1',\n",
       "  'BRME1',\n",
       "  'KLRC1',\n",
       "  'HTR1B',\n",
       "  'IFNL2',\n",
       "  'VAV3',\n",
       "  'ITPRIP',\n",
       "  'KLF4',\n",
       "  'KIF20B',\n",
       "  'ATXN2',\n",
       "  'TSPAN7',\n",
       "  'BCAT2',\n",
       "  'IGDCC3',\n",
       "  'LELP1',\n",
       "  'TMPRSS11B',\n",
       "  'KCNC4',\n",
       "  'MAP1LC3A',\n",
       "  'BRD2',\n",
       "  'LYPLA2',\n",
       "  'BOLA1',\n",
       "  'ART5',\n",
       "  'AGBL2',\n",
       "  'UPK3A',\n",
       "  'IL13RA2',\n",
       "  'HDAC9',\n",
       "  'ARMCX2',\n",
       "  'KIRREL1',\n",
       "  'TJP3',\n",
       "  'TUBB3',\n",
       "  'ARID3A',\n",
       "  'KRT8',\n",
       "  'BHLHE40',\n",
       "  'ARHGEF5',\n",
       "  'ADGRV1',\n",
       "  'LMOD2',\n",
       "  'GFRAL',\n",
       "  'DNAJB6',\n",
       "  'CD7',\n",
       "  'NAGA',\n",
       "  'PTPN9',\n",
       "  'NDUFA5',\n",
       "  'SCPEP1',\n",
       "  'PRR4',\n",
       "  'CSF3R',\n",
       "  'UNC5D',\n",
       "  'TYRP1',\n",
       "  'SHH',\n",
       "  'GLI2',\n",
       "  'GIPR',\n",
       "  'UBE2Z',\n",
       "  'GAD2',\n",
       "  'SLITRK1',\n",
       "  'BCL2L15',\n",
       "  'TLR1',\n",
       "  'EDNRB',\n",
       "  'NUMB',\n",
       "  'ALPI',\n",
       "  'KLRF1',\n",
       "  'SIRT1',\n",
       "  'HS6ST2',\n",
       "  'GIT1',\n",
       "  'CD36',\n",
       "  'TLR4',\n",
       "  'CSNK1D',\n",
       "  'CSF2RB',\n",
       "  'CD3G',\n",
       "  'RNF168',\n",
       "  'RAP1A',\n",
       "  'FGF12',\n",
       "  'REPS1',\n",
       "  'FOLH1',\n",
       "  'RICTOR',\n",
       "  'TRAF3',\n",
       "  'NFAT5',\n",
       "  'FOXJ3',\n",
       "  'CEBPA',\n",
       "  'TPSG1',\n",
       "  'NEDD9',\n",
       "  'RNF31',\n",
       "  'CEMIP2',\n",
       "  'RPA2',\n",
       "  'CLEC12A',\n",
       "  'NEDD4L',\n",
       "  'S100A13',\n",
       "  'NECTIN1',\n",
       "  'TOP2B',\n",
       "  'TP53BP1',\n",
       "  'SEMA6C',\n",
       "  'RELB',\n",
       "  'FGF16',\n",
       "  'NME1',\n",
       "  'NPHS2',\n",
       "  'NPHS1',\n",
       "  'FGF20',\n",
       "  'RALB',\n",
       "  'FGF3',\n",
       "  'IL12RB2',\n",
       "  'ANKMY2',\n",
       "  'FGF6',\n",
       "  'PTP4A3',\n",
       "  'BAG4',\n",
       "  'CPOX',\n",
       "  'TSPYL1',\n",
       "  'BABAM1',\n",
       "  'LATS1',\n",
       "  'TSC1',\n",
       "  'IGFL4',\n",
       "  'RBPMS',\n",
       "  'CD226',\n",
       "  'NXPH3',\n",
       "  'MTDH',\n",
       "  'DGKA',\n",
       "  'STX7',\n",
       "  'STX5',\n",
       "  'HIF1A',\n",
       "  'EIF4E',\n",
       "  'IL36A',\n",
       "  'CASP9',\n",
       "  'PGR',\n",
       "  'DENR',\n",
       "  'ST8SIA1',\n",
       "  'TGFBR1',\n",
       "  'KDM3A',\n",
       "  'PPL',\n",
       "  'DDX4',\n",
       "  'DDX39A',\n",
       "  'ACP1',\n",
       "  'PDZK1',\n",
       "  'SMPD3',\n",
       "  'MKI67',\n",
       "  'POLR2A',\n",
       "  'POF1B',\n",
       "  'PIKFYVE',\n",
       "  'C1QL2',\n",
       "  'ACRV1',\n",
       "  'ZBP1',\n",
       "  'PLCB1',\n",
       "  'YY1',\n",
       "  'ZNF174',\n",
       "  'ADAM12',\n",
       "  'XIAP',\n",
       "  'EP300',\n",
       "  'TERF1',\n",
       "  'ADAMTS1',\n",
       "  'WASL',\n",
       "  'SUMF1',\n",
       "  'ADAMTS4',\n",
       "  'PPM1B',\n",
       "  'STAT2',\n",
       "  'ERMAP',\n",
       "  'HDAC8',\n",
       "  'DAPK2',\n",
       "  'DAND5',\n",
       "  'IL21R',\n",
       "  'IL31',\n",
       "  'VAMP8',\n",
       "  'IL20RB',\n",
       "  'CCNE1',\n",
       "  'EVI5',\n",
       "  'MRPS16',\n",
       "  'PRR5',\n",
       "  'PRSS22',\n",
       "  'PSMG4',\n",
       "  'AKR7L',\n",
       "  'PER3',\n",
       "  'BLNK',\n",
       "  'CA8',\n",
       "  'DBN1',\n",
       "  'SPRED2',\n",
       "  'PALLD',\n",
       "  'SSBP1',\n",
       "  'BNIP3L',\n",
       "  'VEGFB',\n",
       "  'MCEMP1',\n",
       "  'ITGAL',\n",
       "  'INSR',\n",
       "  'ESR1',\n",
       "  'IFI30',\n",
       "  'CNP',\n",
       "  'NAGK',\n",
       "  'LAMP1',\n",
       "  'TP73',\n",
       "  'PGM2',\n",
       "  'DYNLT1',\n",
       "  'CHM',\n",
       "  'PFDN6',\n",
       "  'TPBGL',\n",
       "  'FZD10',\n",
       "  'CLIC5',\n",
       "  'DTX2',\n",
       "  'CLNS1A',\n",
       "  'RRAS',\n",
       "  'CLGN',\n",
       "  'PDRG1',\n",
       "  'RPGR',\n",
       "  'DUSP29',\n",
       "  'CLEC2L',\n",
       "  'EFNB2',\n",
       "  'CHRM1',\n",
       "  'CIT',\n",
       "  'LRFN2',\n",
       "  'AP2B1',\n",
       "  'FRMD7',\n",
       "  'CRTAP',\n",
       "  'PTH',\n",
       "  'FARSA',\n",
       "  'AKR1B10',\n",
       "  'PSMD5',\n",
       "  'FBN2',\n",
       "  'CUZD1',\n",
       "  'OSTN',\n",
       "  'UROS',\n",
       "  'AIDA',\n",
       "  'PRKAG3',\n",
       "  'NRXN3',\n",
       "  'AMIGO1',\n",
       "  'DCC',\n",
       "  'PPT1',\n",
       "  'ERC2',\n",
       "  'DOC2B',\n",
       "  'RAC3',\n",
       "  'DDX25',\n",
       "  'DDX53',\n",
       "  'TTF2',\n",
       "  'KCNH2',\n",
       "  'DIPK1C',\n",
       "  'RBP1',\n",
       "  'TRIM40',\n",
       "  'NLGN1',\n",
       "  'PMS1',\n",
       "  'COL28A1',\n",
       "  'EPB41L5',\n",
       "  'IFT20',\n",
       "  'CNTNAP4',\n",
       "  'LRP2',\n",
       "  'C2orf69',\n",
       "  'LYSMD3',\n",
       "  'MAG',\n",
       "  'MRI1',\n",
       "  'SCT',\n",
       "  'CASC3',\n",
       "  'LRTM1',\n",
       "  'SLC44A4',\n",
       "  'GTPBP2',\n",
       "  'TDO2',\n",
       "  'SLC1A4',\n",
       "  'SV2A',\n",
       "  'MFAP3L',\n",
       "  'GBA',\n",
       "  'SOX9',\n",
       "  'CAMLG',\n",
       "  'MN1',\n",
       "  'CABP2',\n",
       "  'CCDC28A',\n",
       "  'TMCO5A',\n",
       "  'NAA80',\n",
       "  'TEX101',\n",
       "  'STX1B',\n",
       "  'BATF',\n",
       "  'CADPS',\n",
       "  'LRRC38',\n",
       "  'SEZ6',\n",
       "  'MSLNL',\n",
       "  'MYL6B',\n",
       "  'MDM1',\n",
       "  'SOWAHA',\n",
       "  'LRP2BP',\n",
       "  'SCN2B',\n",
       "  'CD164L2',\n",
       "  'TBR1',\n",
       "  'MYLPF',\n",
       "  'CGN',\n",
       "  'TARM1',\n",
       "  'MICALL2',\n",
       "  'GNGT1',\n",
       "  'SCN3A',\n",
       "  'HNF1A',\n",
       "  'ANXA1',\n",
       "  'SUSD5',\n",
       "  'RBPMS2',\n",
       "  'RANBP1',\n",
       "  'COQ7',\n",
       "  'MYBPC2',\n",
       "  'DMP1',\n",
       "  'ANP32C',\n",
       "  'PRRT3',\n",
       "  'PNMA1',\n",
       "  'HSDL2',\n",
       "  'TMEM132A',\n",
       "  'IGSF21',\n",
       "  'MYL4',\n",
       "  'DLL4',\n",
       "  'DMD',\n",
       "  'MYL3',\n",
       "  'EDN1',\n",
       "  'GIP',\n",
       "  'HSBP1',\n",
       "  'BOLA2_BOLA2B',\n",
       "  'AIF1L',\n",
       "  'OXCT1',\n",
       "  'PAGR1',\n",
       "  'SNED1',\n",
       "  'OPLAH',\n",
       "  'GNPDA1',\n",
       "  'SNX5',\n",
       "  'AHNAK2',\n",
       "  'AHNAK',\n",
       "  'BECN1',\n",
       "  'FAM172A',\n",
       "  'VIPR1',\n",
       "  'HRC',\n",
       "  'KHK',\n",
       "  'POMC',\n",
       "  'HS1BP3',\n",
       "  'NUDT10',\n",
       "  'PYDC1',\n",
       "  'SIL1',\n",
       "  'HMGCL',\n",
       "  'SIGLEC8',\n",
       "  'CRYZL1',\n",
       "  'CCER2',\n",
       "  'LAMB1',\n",
       "  'GRP',\n",
       "  'CBS',\n",
       "  'ADAMTSL4',\n",
       "  'EPPK1',\n",
       "  'LIPF',\n",
       "  'B3GNT7',\n",
       "  'RECK',\n",
       "  'SCRIB',\n",
       "  'SEC31A',\n",
       "  'RNF149',\n",
       "  'COMMD1',\n",
       "  'ATP6V1G1',\n",
       "  'RNF5',\n",
       "  'ROBO4',\n",
       "  'FSHB',\n",
       "  'RPL14',\n",
       "  'CEP170',\n",
       "  'AAMDC',\n",
       "  'EIF2S2',\n",
       "  'SCN4B',\n",
       "  'SEL1L',\n",
       "  'INPP5D',\n",
       "  'FSTL1',\n",
       "  'EHD3',\n",
       "  'PECR',\n",
       "  'ECHS1',\n",
       "  'MECR',\n",
       "  'TOR1AIP1',\n",
       "  'ASRGL1',\n",
       "  'IDO1',\n",
       "  'ZP3',\n",
       "  'GADD45GIP1',\n",
       "  'RNASE10',\n",
       "  'MAN1A2',\n",
       "  'COL2A1',\n",
       "  'NIT1',\n",
       "  'ITPR1',\n",
       "  'ENPP6',\n",
       "  'ENO3',\n",
       "  'LONP1',\n",
       "  'DNAJC6',\n",
       "  'NFE2',\n",
       "  'ENTR1',\n",
       "  'GATD3',\n",
       "  'M6PR',\n",
       "  'CALCOCO2',\n",
       "  'APOBR',\n",
       "  'ECM1',\n",
       "  'ACYP1',\n",
       "  'WFDC1',\n",
       "  'GM2A',\n",
       "  'PLG',\n",
       "  'SH3GL3',\n",
       "  'PCBD1',\n",
       "  'RLN2',\n",
       "  'C1QTNF9',\n",
       "  'SERPINI1',\n",
       "  'GLA',\n",
       "  'CACYBP',\n",
       "  'MARS1',\n",
       "  'HMCN2',\n",
       "  'C7',\n",
       "  'LPA',\n",
       "  'FGA',\n",
       "  'CLEC3B',\n",
       "  'PAXX',\n",
       "  'C1QTNF5',\n",
       "  'MENT',\n",
       "  'ADGRD1',\n",
       "  'VTI1A',\n",
       "  'DAAM1',\n",
       "  'GNPDA2',\n",
       "  'PENK',\n",
       "  'SYAP1',\n",
       "  'ADD1',\n",
       "  'PINLYP',\n",
       "  'JAM3',\n",
       "  'PRKG1',\n",
       "  'ITGA2',\n",
       "  'DNAJB2',\n",
       "  'SNX15',\n",
       "  'DIPK2B',\n",
       "  'TBCA',\n",
       "  'GP5',\n",
       "  'YWHAQ',\n",
       "  'PDE5A',\n",
       "  'DTD1',\n",
       "  'DDI2',\n",
       "  'ADH1B',\n",
       "  'ST13',\n",
       "  'INHBB',\n",
       "  'ERP29',\n",
       "  'PHYKPL',\n",
       "  'MOCS2',\n",
       "  'AFAP1',\n",
       "  'SPART',\n",
       "  'HEG1',\n",
       "  'BMPER',\n",
       "  'PDIA3',\n",
       "  'DCTD',\n",
       "  'MFAP4',\n",
       "  'BMP10',\n",
       "  'SPINK2',\n",
       "  'EPHA4',\n",
       "  'ACHE',\n",
       "  'CHAD',\n",
       "  'UBXN1',\n",
       "  'TNFRSF17',\n",
       "  'SLC9A3R1',\n",
       "  'LZTFL1',\n",
       "  'ARHGAP45',\n",
       "  'AMOT',\n",
       "  'CD72',\n",
       "  'CELSR2',\n",
       "  'GIMAP7',\n",
       "  'SDK2',\n",
       "  'GHR',\n",
       "  'RABEP1',\n",
       "  'CD300A',\n",
       "  'SEMA3G',\n",
       "  'CRELD1',\n",
       "  'RIDA',\n",
       "  'SFRP4',\n",
       "  'MXRA8',\n",
       "  'APPL2',\n",
       "  'MYOM3',\n",
       "  'FGFR4',\n",
       "  'TNFAIP8L2',\n",
       "  'PTRHD1',\n",
       "  'COL5A1',\n",
       "  'FUOM',\n",
       "  'AKAP12',\n",
       "  'CTSE',\n",
       "  'SCGB3A1',\n",
       "  'TPD52L2',\n",
       "  'NAGPA',\n",
       "  'UROD',\n",
       "  'GMPR2',\n",
       "  'SNCA',\n",
       "  'GLRX5',\n",
       "  'KCTD5',\n",
       "  'UPK3BL1',\n",
       "  'TRIM24',\n",
       "  'CTAG1A_CTAG1B',\n",
       "  'FUT1',\n",
       "  'HRAS',\n",
       "  'TET2',\n",
       "  'COL4A4',\n",
       "  'TCN1',\n",
       "  'KLKB1',\n",
       "  'QSOX1',\n",
       "  'CEACAM18',\n",
       "  'EFCAB2',\n",
       "  'NEK7',\n",
       "  'NFKB2',\n",
       "  'CEACAM20',\n",
       "  'RGL2',\n",
       "  'SEPTIN7',\n",
       "  'SAP18',\n",
       "  'ARAF',\n",
       "  'GABARAPL1',\n",
       "  'SAT2',\n",
       "  'ARHGAP30',\n",
       "  'TRDMT1',\n",
       "  'ID4',\n",
       "  'PKN3',\n",
       "  'MAPKAPK2',\n",
       "  'TNPO1',\n",
       "  'TAP1',\n",
       "  'TCP11',\n",
       "  'ITGAX',\n",
       "  'IFIT3',\n",
       "  'ACADM',\n",
       "  'CEP290',\n",
       "  'TAB2',\n",
       "  'GAS2',\n",
       "  'RPE',\n",
       "  'ZNF75D',\n",
       "  'LSM8',\n",
       "  'CENPJ',\n",
       "  'CINP',\n",
       "  'RNF43',\n",
       "  'IFIT1',\n",
       "  'CA7',\n",
       "  'RNF4',\n",
       "  'CENPF',\n",
       "  'TPPP2',\n",
       "  'IL9',\n",
       "  'PAFAH2',\n",
       "  'EPN1',\n",
       "  'COL9A2',\n",
       "  'PPIE',\n",
       "  'TLR2',\n",
       "  'MNAT1',\n",
       "  'ERI1',\n",
       "  'CD3E',\n",
       "  'MAGEA3',\n",
       "  'ALMS1',\n",
       "  'PPP1R12B',\n",
       "  'VPS28',\n",
       "  'PTTG1',\n",
       "  'MORF4L1',\n",
       "  'KIAA1549',\n",
       "  'SPRR1B',\n",
       "  'SLK',\n",
       "  'TK1',\n",
       "  'OFD1',\n",
       "  'KIAA1549L',\n",
       "  'MTHFSD',\n",
       "  'EVPL',\n",
       "  'GADD45B',\n",
       "  'TIGIT',\n",
       "  'CCND2',\n",
       "  'BRD1',\n",
       "  'SHPK',\n",
       "  'VSTM2B',\n",
       "  'TEX33',\n",
       "  'GUCY2C',\n",
       "  'CDH22',\n",
       "  'SERPINH1',\n",
       "  'RAPGEF2',\n",
       "  'PRUNE2',\n",
       "  'MTUS1',\n",
       "  'TMED1',\n",
       "  'GTF2IRD1',\n",
       "  'CASP4',\n",
       "  'OGT',\n",
       "  'RAD51',\n",
       "  'TXK',\n",
       "  'PARD3',\n",
       "  'CD82',\n",
       "  'BCHE',\n",
       "  'SERPINF2',\n",
       "  'SERPINA1',\n",
       "  'SERPINA4',\n",
       "  'SGSH',\n",
       "  'CFB',\n",
       "  'NPC2',\n",
       "  'PRDX2',\n",
       "  'TXN',\n",
       "  'CYB5R2',\n",
       "  'MST1',\n",
       "  'CAT',\n",
       "  'CTBS',\n",
       "  'SERPINF1',\n",
       "  'BRAP',\n",
       "  'HPSE',\n",
       "  'SERPINA5',\n",
       "  'F11',\n",
       "  'MBL2',\n",
       "  'CFHR5',\n",
       "  'IST1',\n",
       "  'PGLYRP2',\n",
       "  'CWC15',\n",
       "  'PALM',\n",
       "  'TTR',\n",
       "  'PSAP',\n",
       "  'ASAH1',\n",
       "  'HGFAC',\n",
       "  'AMOTL2',\n",
       "  'CRISP3',\n",
       "  'NMI',\n",
       "  'EIF2AK2',\n",
       "  'APCS',\n",
       "  'SLURP1',\n",
       "  'DTNB',\n",
       "  'LACRT',\n",
       "  'BTN1A1',\n",
       "  'THTPA',\n",
       "  'MPRIP',\n",
       "  'KLK15',\n",
       "  'RNASE6',\n",
       "  'NAP1L4',\n",
       "  'CDC26',\n",
       "  'LMNB1',\n",
       "  'NUDT16',\n",
       "  'PPBP',\n",
       "  'PF4',\n",
       "  'CFHR2',\n",
       "  'GSR',\n",
       "  'MDH1',\n",
       "  'IL2RG',\n",
       "  'REG3G',\n",
       "  'FNTA',\n",
       "  'RFC4',\n",
       "  'CMIP',\n",
       "  'NUBP1',\n",
       "  'FAM171B',\n",
       "  'NENF',\n",
       "  'AHSA1',\n",
       "  'IL22',\n",
       "  'COMMD9',\n",
       "  'VSIG10',\n",
       "  'KIAA2013',\n",
       "  'RCC1',\n",
       "  'ALDH2',\n",
       "  'UNG',\n",
       "  'VPS4B',\n",
       "  'RALY',\n",
       "  'RAB44',\n",
       "  'PXDNL',\n",
       "  'RAB2B',\n",
       "  'VSIG2',\n",
       "  'KIR2DL2',\n",
       "  'USP25',\n",
       "  'UBE2B',\n",
       "  'LARP1',\n",
       "  'S100A3',\n",
       "  'TDP1',\n",
       "  'CAPN3',\n",
       "  'MINDY1',\n",
       "  'SUSD4',\n",
       "  'TADA3',\n",
       "  'TARS1',\n",
       "  'LRRFIP1',\n",
       "  'TG',\n",
       "  'STAM',\n",
       "  'TGFB2',\n",
       "  'LTB',\n",
       "  'LUZP2',\n",
       "  'MAMDC4',\n",
       "  'HSPA2',\n",
       "  'BCL7B',\n",
       "  'CCDC134',\n",
       "  'GPRC5C',\n",
       "  'LAMTOR5',\n",
       "  'MTSS2',\n",
       "  'NDST1',\n",
       "  'GABARAP',\n",
       "  'CHCHD6',\n",
       "  'NACC1',\n",
       "  'AP1G2',\n",
       "  'TRIM58',\n",
       "  'SLC13A1',\n",
       "  'GPD1',\n",
       "  'SMAD2',\n",
       "  'SMAD3',\n",
       "  'GLYR1',\n",
       "  'SMPDL3B',\n",
       "  'SNX2',\n",
       "  'ARG2',\n",
       "  'SDCCAG8',\n",
       "  'TMEM106A',\n",
       "  'ACRBP',\n",
       "  'DUSP13',\n",
       "  'DYNC1H1',\n",
       "  'EDDM3B',\n",
       "  'DYNLT3',\n",
       "  'WDR46',\n",
       "  'PCDHB15',\n",
       "  'EPGN',\n",
       "  'DHPS',\n",
       "  'IMMT',\n",
       "  'PRC1',\n",
       "  'PPP1CC',\n",
       "  'ERN1',\n",
       "  'ZNRF4',\n",
       "  'ZNF830',\n",
       "  'YJU2',\n",
       "  'PCSK7',\n",
       "  'INSL4',\n",
       "  'ENOPH1',\n",
       "  'ITIH5',\n",
       "  'ENSA',\n",
       "  'TMPRSS11D',\n",
       "  'FTCD',\n",
       "  'PLSCR3',\n",
       "  'SEPTIN8',\n",
       "  'PRKAR2A',\n",
       "  'SMTN',\n",
       "  'NFU1',\n",
       "  'PBXIP1',\n",
       "  'HIP1R',\n",
       "  'ZFYVE19',\n",
       "  ...],\n",
       " 'output_size': 2,\n",
       " 'hidden_size': 256,\n",
       " 'lr': 0.007395305965296179,\n",
       " 'weight_decay': 0.004495906560621455,\n",
       " 'weight': [1, 1],\n",
       " 'batch_size': 256,\n",
       " 'num_resblocks': 3}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params[\"train_loop_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df have NA: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>PRS</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>BSA</th>\n",
       "      <th>genotype_array</th>\n",
       "      <th>age</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>...</th>\n",
       "      <th>TGFBR3</th>\n",
       "      <th>CRTAC1</th>\n",
       "      <th>IGFBP7</th>\n",
       "      <th>SELE</th>\n",
       "      <th>VWF</th>\n",
       "      <th>NOTCH3</th>\n",
       "      <th>CNTN1</th>\n",
       "      <th>ENG</th>\n",
       "      <th>ICAM2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19409</th>\n",
       "      <td>2883530.0</td>\n",
       "      <td>1.030583</td>\n",
       "      <td>1.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>64.2</td>\n",
       "      <td>1.746282</td>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>71.3002</td>\n",
       "      <td>-100.66700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0087</td>\n",
       "      <td>-0.029539</td>\n",
       "      <td>0.022568</td>\n",
       "      <td>-0.027118</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>-0.026825</td>\n",
       "      <td>0.307764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19272</th>\n",
       "      <td>2867444.0</td>\n",
       "      <td>2.192278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>55.8</td>\n",
       "      <td>1.599219</td>\n",
       "      <td>2</td>\n",
       "      <td>53.0</td>\n",
       "      <td>-12.4815</td>\n",
       "      <td>3.16181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1859</td>\n",
       "      <td>0.291950</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>-0.120500</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.553233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49865</th>\n",
       "      <td>5869793.0</td>\n",
       "      <td>0.653794</td>\n",
       "      <td>1.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>77.3</td>\n",
       "      <td>1.916181</td>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-11.4721</td>\n",
       "      <td>2.20519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>0.369750</td>\n",
       "      <td>-0.155300</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>-0.276700</td>\n",
       "      <td>-0.043900</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>-0.990800</td>\n",
       "      <td>0.469619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39664</th>\n",
       "      <td>4880838.0</td>\n",
       "      <td>0.664819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>84.4</td>\n",
       "      <td>1.954852</td>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-11.1640</td>\n",
       "      <td>3.66252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>0.393200</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.873200</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.114200</td>\n",
       "      <td>0.134400</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.095878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30555</th>\n",
       "      <td>3987428.0</td>\n",
       "      <td>0.826465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>73.1</td>\n",
       "      <td>1.824859</td>\n",
       "      <td>1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-11.4666</td>\n",
       "      <td>2.77498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5216</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>-0.160200</td>\n",
       "      <td>0.181900</td>\n",
       "      <td>1.026700</td>\n",
       "      <td>-0.062150</td>\n",
       "      <td>-0.094500</td>\n",
       "      <td>-0.032700</td>\n",
       "      <td>0.213200</td>\n",
       "      <td>0.454483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43327</th>\n",
       "      <td>5241912.0</td>\n",
       "      <td>1.085083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2.381409</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-10.8083</td>\n",
       "      <td>4.46241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2348</td>\n",
       "      <td>-0.919950</td>\n",
       "      <td>0.803300</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.481500</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>-0.226200</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>0.239400</td>\n",
       "      <td>0.859506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29129</th>\n",
       "      <td>3851862.0</td>\n",
       "      <td>1.294348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>72.9</td>\n",
       "      <td>1.849932</td>\n",
       "      <td>2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-12.6549</td>\n",
       "      <td>3.40064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3290</td>\n",
       "      <td>-0.251250</td>\n",
       "      <td>-0.787400</td>\n",
       "      <td>-0.919000</td>\n",
       "      <td>0.212700</td>\n",
       "      <td>-0.617800</td>\n",
       "      <td>0.123900</td>\n",
       "      <td>-0.124100</td>\n",
       "      <td>-0.940500</td>\n",
       "      <td>0.082682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>1144512.0</td>\n",
       "      <td>0.722791</td>\n",
       "      <td>1.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>96.6</td>\n",
       "      <td>2.263883</td>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-12.7237</td>\n",
       "      <td>1.46547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1043</td>\n",
       "      <td>-0.284750</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>1.608600</td>\n",
       "      <td>-0.341300</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>-0.012000</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.406253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>1177099.0</td>\n",
       "      <td>1.335307</td>\n",
       "      <td>1.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>75.1</td>\n",
       "      <td>1.910679</td>\n",
       "      <td>2</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-15.1573</td>\n",
       "      <td>7.36690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2172</td>\n",
       "      <td>0.172250</td>\n",
       "      <td>0.431300</td>\n",
       "      <td>0.121750</td>\n",
       "      <td>-0.754900</td>\n",
       "      <td>0.530700</td>\n",
       "      <td>0.244000</td>\n",
       "      <td>-0.018900</td>\n",
       "      <td>-0.053400</td>\n",
       "      <td>0.468948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29456</th>\n",
       "      <td>3881441.0</td>\n",
       "      <td>1.055519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>58.3</td>\n",
       "      <td>1.614715</td>\n",
       "      <td>2</td>\n",
       "      <td>53.0</td>\n",
       "      <td>-12.4170</td>\n",
       "      <td>4.44358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1298</td>\n",
       "      <td>0.291350</td>\n",
       "      <td>-0.231600</td>\n",
       "      <td>-0.446000</td>\n",
       "      <td>-0.775500</td>\n",
       "      <td>0.385750</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.227400</td>\n",
       "      <td>0.281229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15287 rows × 2974 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             eid       PRS  sex  height  weight       BSA  genotype_array  \\\n",
       "19409  2883530.0  1.030583  1.0   171.0    64.2  1.746282               2   \n",
       "19272  2867444.0  2.192278  0.0   165.0    55.8  1.599219               2   \n",
       "49865  5869793.0  0.653794  1.0   171.0    77.3  1.916181               2   \n",
       "39664  4880838.0  0.664819  0.0   163.0    84.4  1.954852               2   \n",
       "30555  3987428.0  0.826465  0.0   164.0    73.1  1.824859               1   \n",
       "...          ...       ...  ...     ...     ...       ...             ...   \n",
       "43327  5241912.0  1.085083  1.0   176.0   116.0  2.381409               2   \n",
       "29129  3851862.0  1.294348  0.0   169.0    72.9  1.849932               2   \n",
       "1550   1144512.0  0.722791  1.0   191.0    96.6  2.263883               1   \n",
       "1888   1177099.0  1.335307  1.0   175.0    75.1  1.910679               2   \n",
       "29456  3881441.0  1.055519  0.0   161.0    58.3  1.614715               2   \n",
       "\n",
       "        age      PC1        PC2  ...  TGFBR3    CRTAC1    IGFBP7      SELE  \\\n",
       "19409  44.0  71.3002 -100.66700  ... -0.0087 -0.029539  0.022568 -0.027118   \n",
       "19272  53.0 -12.4815    3.16181  ...  0.1859  0.291950  0.147400 -0.120500   \n",
       "49865  62.0 -11.4721    2.20519  ...  0.0516  0.369750 -0.155300  0.035500   \n",
       "39664  62.0 -11.1640    3.66252  ... -0.0127  0.393200  0.174500  0.035700   \n",
       "30555  66.0 -11.4666    2.77498  ... -0.5216  0.005050 -0.160200  0.181900   \n",
       "...     ...      ...        ...  ...     ...       ...       ...       ...   \n",
       "43327  45.0 -10.8083    4.46241  ...  0.2348 -0.919950  0.803300  0.131600   \n",
       "29129  40.0 -12.6549    3.40064  ... -0.3290 -0.251250 -0.787400 -0.919000   \n",
       "1550   59.0 -12.7237    1.46547  ...  0.1043 -0.284750  0.350300  1.608600   \n",
       "1888   63.0 -15.1573    7.36690  ...  0.2172  0.172250  0.431300  0.121750   \n",
       "29456  53.0 -12.4170    4.44358  ...  0.1298  0.291350 -0.231600 -0.446000   \n",
       "\n",
       "            VWF    NOTCH3     CNTN1       ENG     ICAM2      pred  \n",
       "19409  0.008048  0.004249  0.000619  0.001707 -0.026825  0.307764  \n",
       "19272  0.597300  0.115700  0.243300  0.127800  0.063400  0.553233  \n",
       "49865 -0.276700 -0.043900  0.195500 -0.111000 -0.990800  0.469619  \n",
       "39664  0.873200  0.236600  0.114200  0.134400  0.008700  0.095878  \n",
       "30555  1.026700 -0.062150 -0.094500 -0.032700  0.213200  0.454483  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "43327  0.481500  0.279800 -0.226200  0.262600  0.239400  0.859506  \n",
       "29129  0.212700 -0.617800  0.123900 -0.124100 -0.940500  0.082682  \n",
       "1550  -0.341300  0.134100 -0.012000  0.226700  0.135200  0.406253  \n",
       "1888  -0.754900  0.530700  0.244000 -0.018900 -0.053400  0.468948  \n",
       "29456 -0.775500  0.385750  0.243300  0.219200  0.227400  0.281229  \n",
       "\n",
       "[15287 rows x 2974 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imputed = best_model.predict_df(test_imputed)\n",
    "test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cal_binary_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcal_binary_metrics\u001b[49m(test_imputed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincident_cad\u001b[39m\u001b[38;5;124m\"\u001b[39m], test_imputed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cal_binary_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "cal_binary_metrics(test_imputed[\"incident_cad\"], test_imputed[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "\n",
    "from tqdm.rich import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n",
    "def generate_multipletests_result(df, pvalue_col=\"pvalue\", alpha=0.05, method=\"fdr_bh\"):\n",
    "    df = df.copy()\n",
    "    pvalue_series = df[pvalue_col]\n",
    "    reject, pvals_corrected, _, _ = multipletests(\n",
    "        pvalue_series, alpha=alpha, method=\"fdr_bh\"\n",
    "    )\n",
    "    df[\"pval_corrected\"] = pvals_corrected\n",
    "    df[\"reject\"] = reject\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_best_cutoff(fpr, tpr, thresholds):\n",
    "    diff = tpr - fpr\n",
    "    Youden_index = np.argmax(diff)\n",
    "    optimal_threshold = thresholds[Youden_index]\n",
    "    optimal_FPR, optimal_TPR = fpr[Youden_index], tpr[Youden_index]\n",
    "    return optimal_threshold, optimal_FPR, optimal_TPR\n",
    "\n",
    "\n",
    "def cal_binary_metrics(y, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred)\n",
    "    AUC = roc_auc_score(y, y_pred)\n",
    "    # by best youden\n",
    "\n",
    "    optim_threshold, optim_fpr, optim_tpr = find_best_cutoff(fpr, tpr, thresholds)\n",
    "    y_pred_binary = (y_pred > optim_threshold).astype(int)\n",
    "    ACC = accuracy_score(y, y_pred_binary)\n",
    "    macro_f1 = f1_score(y, y_pred_binary, average=\"macro\")\n",
    "    sensitivity = optim_tpr\n",
    "    specificity = 1 - optim_fpr\n",
    "    precision, recall, _ = precision_recall_curve(y, y_pred)\n",
    "    APR = auc(recall, precision)\n",
    "\n",
    "    return {\n",
    "        \"AUC\": AUC,\n",
    "        \"ACC\": ACC,\n",
    "        \"Macro_F1\": macro_f1,\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"APR\": APR,\n",
    "    }\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.rich import tqdm\n",
    "\n",
    "# 定义神经网络模型\n",
    "\n",
    "\n",
    "class LinearResBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearResBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "        self.batch_norm = nn.LayerNorm(output_size)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight, nonlinearity=\"relu\")  # <6>\n",
    "\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)  # <7>\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_resblocks=3):\n",
    "        super(FullyConnectedNet, self).__init__()\n",
    "        self.norm = nn.LayerNorm(input_size)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[LinearResBlock(hidden_size, hidden_size) for _ in range(num_resblocks)]\n",
    "        )\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        out = torch.relu(self.fc1(x))\n",
    "        out = self.resblocks(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义训练函数\n",
    "def train(model, dataset, criterion, optimizer, num_epochs):\n",
    "    train_loader = dataset.train_dataloader()\n",
    "    val_loader = dataset.test_dataloader()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        auroc = torchmetrics.AUROC(num_classes=2, task=\"multiclass\")\n",
    "        for inputs, labels in tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=len(train_loader)\n",
    "        ):\n",
    "            inputs, labels = inputs.to(\"cuda:0\"), labels.to(\"cuda:0\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.squeeze(-1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            auroc.update(torch.softmax(outputs, dim=-1), torch.argmax(labels, dim=1))\n",
    "        auc = auroc.compute()\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, AUC: {auc}\"\n",
    "        )\n",
    "        if epoch % 1 == 0:\n",
    "            test_auc = torchmetrics.AUROC(num_classes=2, task=\"multiclass\")\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(\"cuda:0\"), labels.to(\"cuda:0\")\n",
    "                outputs = model(inputs)\n",
    "                test_auc.update(\n",
    "                    torch.softmax(outputs, dim=-1), torch.argmax(labels, dim=1)\n",
    "                )\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Test AUC: {test_auc.compute()}\")\n",
    "    # test_auc = torchmetrics.AUROC(num_classes=2, task=\"multiclass\")\n",
    "    # for inputs, labels in dataset.test_dataloader():\n",
    "    #     inputs, labels = inputs.to(\"cuda:0\"), labels.to(\"cuda:0\")\n",
    "    #     outputs = model(inputs)\n",
    "    #     test_auc.update(torch.softmax(outputs, dim=-1), torch.argmax(labels, dim=1))\n",
    "    # print(f\"Test AUC: {test_auc.compute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练参数\n",
    "input_size = len(proteomics)  # 输入特征维度\n",
    "hidden_size = 512  # 隐藏层维度\n",
    "output_size = 2  # 输出类别数\n",
    "learning_rate = 5e-4\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "# 创建模型实例\n",
    "best_model = FullyConnectedNet(\n",
    "    input_size=len(proteomics), hidden_size=256, output_size=2, num_resblocks=6\n",
    ")\n",
    "best_model.to(\"cuda:0\")\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([0.1, 100]).to(\"cuda:0\"))\n",
    "optimizer = optim.NAdam(best_model.parameters(), lr=learning_rate, weight_decay=5e-3)\n",
    "\n",
    "\n",
    "# 开始训练\n",
    "train(best_model, dataset, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_list = []\n",
    "AUC = torchmetrics.AUROC(num_classes=2, task=\"multiclass\")\n",
    "\n",
    "best_model.eval()\n",
    "for x, y in dataset.test_dataloader():\n",
    "    y_pred = best_model(x.to(\"cuda:0\")).cpu().detach()\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_list.append(y)\n",
    "    AUC.update(torch.softmax(y_pred, dim=-1), torch.argmax(y, dim=1))\n",
    "\n",
    "AUC_values = AUC.compute()\n",
    "print(f\"AUC: {AUC_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.softmax(torch.cat(y_pred_list), dim=-1)[:, 1].numpy()\n",
    "y_true = torch.argmax(torch.cat(y_list), dim=1).numpy()\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    {\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_true\": y_true,\n",
    "    }\n",
    ")\n",
    "\n",
    "cal_binary_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_binary_metrics(test_df[\"y_true\"], test_df[\"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
