{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. preprocess:按照rank的方式进行排序（参考geneformer）\n",
    "2. tokenizer构建，直接基于前面的预料库构建即可\n",
    "3. BERT架构构建（或者其他的）,MLM\n",
    "4. 预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: eid信息缺失了，并且部分蛋白是用mean填充的，后续可以根本不需要填充，因为可以传入的时候没有他们\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_pickle(\"result/part1/train_data.pkl\").set_index(\"eid\")\n",
    "test_data = pd.read_pickle(\"result/part1/test_data.pkl\").set_index(\"eid\").head(300)\n",
    "\n",
    "\n",
    "protein_cols = test_data.columns[test_data.columns.tolist().index(\"C3\") :].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['eid', 'proteins', 'values'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "res = defaultdict(list)\n",
    "\n",
    "for idx, row in test_data.iterrows():\n",
    "    ranked_row = row.sort_values(ascending=False).dropna()\n",
    "\n",
    "    res[\"eid\"].append(ranked_row.name)\n",
    "    res[\"proteins\"].append(\" \".join(ranked_row.index.tolist()))\n",
    "    res[\"values\"].append(ranked_row.values.tolist())\n",
    "\n",
    "test_dataset = Dataset.from_dict(res)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertTokenizerFast,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"transtab/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c82edfe60c45c0b30be520af865dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import multiprocessing\n",
    "from transformers import BertTokenizer, BertTokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "def group_texts(examples, max_length=2048):\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"proteins\"],\n",
    "        return_special_tokens_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        truncation_strategy=\"only_last\",\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = test_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    remove_columns=[\"proteins\"],\n",
    "    num_proc=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2775,\n",
       " 2537,\n",
       " 1374,\n",
       " 391,\n",
       " 957,\n",
       " 2323,\n",
       " 996,\n",
       " 1177,\n",
       " 2028,\n",
       " 1499,\n",
       " 449,\n",
       " 1542,\n",
       " 2632,\n",
       " 1242,\n",
       " 512,\n",
       " 1651,\n",
       " 447,\n",
       " 1198,\n",
       " 191,\n",
       " 1435,\n",
       " 105,\n",
       " 1413,\n",
       " 1392,\n",
       " 2084,\n",
       " 1376,\n",
       " 1433,\n",
       " 26,\n",
       " 2178,\n",
       " 1032,\n",
       " 2796,\n",
       " 1516,\n",
       " 940,\n",
       " 78,\n",
       " 1340,\n",
       " 2173,\n",
       " 2713,\n",
       " 1753,\n",
       " 1372,\n",
       " 2829,\n",
       " 102,\n",
       " 2422,\n",
       " 2077,\n",
       " 749,\n",
       " 199,\n",
       " 390,\n",
       " 2472,\n",
       " 2233,\n",
       " 1232,\n",
       " 814,\n",
       " 1020,\n",
       " 1382,\n",
       " 2161,\n",
       " 1326,\n",
       " 1156,\n",
       " 1699,\n",
       " 1128,\n",
       " 1498,\n",
       " 1782,\n",
       " 737,\n",
       " 1656,\n",
       " 242,\n",
       " 815,\n",
       " 2093,\n",
       " 748,\n",
       " 332,\n",
       " 2544,\n",
       " 621,\n",
       " 2528,\n",
       " 1248,\n",
       " 2492,\n",
       " 381,\n",
       " 727,\n",
       " 421,\n",
       " 1803,\n",
       " 1200,\n",
       " 1500,\n",
       " 1621,\n",
       " 1998,\n",
       " 1727,\n",
       " 399,\n",
       " 2640,\n",
       " 2788,\n",
       " 2083,\n",
       " 1949,\n",
       " 2754,\n",
       " 425,\n",
       " 2892,\n",
       " 884,\n",
       " 106,\n",
       " 1601,\n",
       " 1997,\n",
       " 397,\n",
       " 653,\n",
       " 1924,\n",
       " 794,\n",
       " 728,\n",
       " 877,\n",
       " 63,\n",
       " 666,\n",
       " 333,\n",
       " 74,\n",
       " 1021,\n",
       " 406,\n",
       " 2826,\n",
       " 2648,\n",
       " 1497,\n",
       " 1954,\n",
       " 256,\n",
       " 1700,\n",
       " 1306,\n",
       " 1237,\n",
       " 1219,\n",
       " 1169,\n",
       " 64,\n",
       " 2035,\n",
       " 444,\n",
       " 70,\n",
       " 13,\n",
       " 1321,\n",
       " 2367,\n",
       " 1831,\n",
       " 2441,\n",
       " 658,\n",
       " 711,\n",
       " 2740,\n",
       " 1019,\n",
       " 526,\n",
       " 162,\n",
       " 850,\n",
       " 633,\n",
       " 2148,\n",
       " 1332,\n",
       " 591,\n",
       " 1152,\n",
       " 405,\n",
       " 383,\n",
       " 382,\n",
       " 2020,\n",
       " 1860,\n",
       " 120,\n",
       " 2141,\n",
       " 1085,\n",
       " 1314,\n",
       " 1579,\n",
       " 782,\n",
       " 2032,\n",
       " 971,\n",
       " 602,\n",
       " 123,\n",
       " 200,\n",
       " 473,\n",
       " 1397,\n",
       " 1002,\n",
       " 2266,\n",
       " 703,\n",
       " 2850,\n",
       " 440,\n",
       " 328,\n",
       " 585,\n",
       " 1694,\n",
       " 443,\n",
       " 1588,\n",
       " 2150,\n",
       " 2525,\n",
       " 2844,\n",
       " 1818,\n",
       " 156,\n",
       " 2714,\n",
       " 1578,\n",
       " 2062,\n",
       " 656,\n",
       " 2282,\n",
       " 1363,\n",
       " 2259,\n",
       " 1584,\n",
       " 2181,\n",
       " 738,\n",
       " 1524,\n",
       " 795,\n",
       " 2324,\n",
       " 2889,\n",
       " 2019,\n",
       " 732,\n",
       " 1246,\n",
       " 2201,\n",
       " 1419,\n",
       " 2046,\n",
       " 988,\n",
       " 2821,\n",
       " 812,\n",
       " 1109,\n",
       " 1009,\n",
       " 867,\n",
       " 2369,\n",
       " 1515,\n",
       " 639,\n",
       " 682,\n",
       " 2679,\n",
       " 155,\n",
       " 2404,\n",
       " 1641,\n",
       " 331,\n",
       " 1717,\n",
       " 71,\n",
       " 1404,\n",
       " 2513,\n",
       " 2202,\n",
       " 1920,\n",
       " 514,\n",
       " 24,\n",
       " 1874,\n",
       " 2606,\n",
       " 552,\n",
       " 428,\n",
       " 654,\n",
       " 984,\n",
       " 2354,\n",
       " 537,\n",
       " 1488,\n",
       " 581,\n",
       " 1034,\n",
       " 2518,\n",
       " 2519,\n",
       " 41,\n",
       " 729,\n",
       " 1934,\n",
       " 1560,\n",
       " 674,\n",
       " 2873,\n",
       " 1518,\n",
       " 398,\n",
       " 2495,\n",
       " 2666,\n",
       " 1238,\n",
       " 1543,\n",
       " 1252,\n",
       " 592,\n",
       " 76,\n",
       " 2464,\n",
       " 2704,\n",
       " 227,\n",
       " 2153,\n",
       " 21,\n",
       " 2862,\n",
       " 429,\n",
       " 2805,\n",
       " 1640,\n",
       " 2342,\n",
       " 1357,\n",
       " 878,\n",
       " 1460,\n",
       " 688,\n",
       " 2056,\n",
       " 1687,\n",
       " 2194,\n",
       " 2870,\n",
       " 2145,\n",
       " 2373,\n",
       " 2157,\n",
       " 2381,\n",
       " 161,\n",
       " 1347,\n",
       " 2768,\n",
       " 1892,\n",
       " 1541,\n",
       " 1631,\n",
       " 1583,\n",
       " 2530,\n",
       " 1008,\n",
       " 1519,\n",
       " 883,\n",
       " 2071,\n",
       " 2680,\n",
       " 560,\n",
       " 2055,\n",
       " 2435,\n",
       " 2255,\n",
       " 2660,\n",
       " 1411,\n",
       " 1935,\n",
       " 1336,\n",
       " 2308,\n",
       " 606,\n",
       " 2453,\n",
       " 95,\n",
       " 730,\n",
       " 1816,\n",
       " 2246,\n",
       " 635,\n",
       " 2162,\n",
       " 446,\n",
       " 1195,\n",
       " 2370,\n",
       " 2352,\n",
       " 1881,\n",
       " 1761,\n",
       " 441,\n",
       " 2005,\n",
       " 1677,\n",
       " 384,\n",
       " 2423,\n",
       " 2247,\n",
       " 2517,\n",
       " 1023,\n",
       " 1331,\n",
       " 1643,\n",
       " 627,\n",
       " 1045,\n",
       " 663,\n",
       " 1016,\n",
       " 1976,\n",
       " 2344,\n",
       " 1459,\n",
       " 1764,\n",
       " 841,\n",
       " 1891,\n",
       " 52,\n",
       " 1000,\n",
       " 1673,\n",
       " 943,\n",
       " 1207,\n",
       " 2012,\n",
       " 742,\n",
       " 520,\n",
       " 2193,\n",
       " 2154,\n",
       " 2165,\n",
       " 68,\n",
       " 847,\n",
       " 2684,\n",
       " 1544,\n",
       " 2098,\n",
       " 112,\n",
       " 1650,\n",
       " 2695,\n",
       " 683,\n",
       " 2448,\n",
       " 1741,\n",
       " 959,\n",
       " 931,\n",
       " 2658,\n",
       " 1046,\n",
       " 2025,\n",
       " 252,\n",
       " 2865,\n",
       " 432,\n",
       " 1313,\n",
       " 1136,\n",
       " 1405,\n",
       " 2271,\n",
       " 2378,\n",
       " 1217,\n",
       " 796,\n",
       " 751,\n",
       " 23,\n",
       " 939,\n",
       " 2096,\n",
       " 733,\n",
       " 1279,\n",
       " 1565,\n",
       " 2531,\n",
       " 2313,\n",
       " 251,\n",
       " 2825,\n",
       " 725,\n",
       " 839,\n",
       " 293,\n",
       " 2103,\n",
       " 523,\n",
       " 2458,\n",
       " 2054,\n",
       " 1416,\n",
       " 2782,\n",
       " 1931,\n",
       " 1134,\n",
       " 1792,\n",
       " 280,\n",
       " 2569,\n",
       " 2486,\n",
       " 806,\n",
       " 2086,\n",
       " 588,\n",
       " 593,\n",
       " 418,\n",
       " 1159,\n",
       " 477,\n",
       " 698,\n",
       " 2328,\n",
       " 1466,\n",
       " 346,\n",
       " 2489,\n",
       " 2659,\n",
       " 2728,\n",
       " 1566,\n",
       " 1068,\n",
       " 2886,\n",
       " 2317,\n",
       " 1728,\n",
       " 779,\n",
       " 169,\n",
       " 1485,\n",
       " 1312,\n",
       " 2687,\n",
       " 2494,\n",
       " 887,\n",
       " 1403,\n",
       " 1698,\n",
       " 1035,\n",
       " 2474,\n",
       " 2715,\n",
       " 1537,\n",
       " 1387,\n",
       " 1535,\n",
       " 2863,\n",
       " 306,\n",
       " 2339,\n",
       " 2276,\n",
       " 1292,\n",
       " 2060,\n",
       " 2439,\n",
       " 1193,\n",
       " 2697,\n",
       " 1080,\n",
       " 2256,\n",
       " 1066,\n",
       " 2885,\n",
       " 1549,\n",
       " 2776,\n",
       " 271,\n",
       " 2045,\n",
       " 2425,\n",
       " 1380,\n",
       " 1775,\n",
       " 1022,\n",
       " 225,\n",
       " 2164,\n",
       " 1271,\n",
       " 1353,\n",
       " 1295,\n",
       " 56,\n",
       " 941,\n",
       " 1350,\n",
       " 336,\n",
       " 1393,\n",
       " 2626,\n",
       " 72,\n",
       " 2376,\n",
       " 1974,\n",
       " 2380,\n",
       " 1749,\n",
       " 1614,\n",
       " 292,\n",
       " 1126,\n",
       " 1006,\n",
       " 2723,\n",
       " 2245,\n",
       " 1153,\n",
       " 1095,\n",
       " 1043,\n",
       " 1345,\n",
       " 876,\n",
       " 1739,\n",
       " 857,\n",
       " 712,\n",
       " 1017,\n",
       " 149,\n",
       " 1822,\n",
       " 1805,\n",
       " 1610,\n",
       " 1806,\n",
       " 557,\n",
       " 160,\n",
       " 1675,\n",
       " 465,\n",
       " 2375,\n",
       " 934,\n",
       " 584,\n",
       " 2717,\n",
       " 2869,\n",
       " 1088,\n",
       " 1655,\n",
       " 427,\n",
       " 2846,\n",
       " 519,\n",
       " 1468,\n",
       " 417,\n",
       " 2712,\n",
       " 463,\n",
       " 1001,\n",
       " 1556,\n",
       " 32,\n",
       " 1540,\n",
       " 2536,\n",
       " 1533,\n",
       " 2470,\n",
       " 2305,\n",
       " 1904,\n",
       " 1160,\n",
       " 2082,\n",
       " 290,\n",
       " 2016,\n",
       " 561,\n",
       " 637,\n",
       " 681,\n",
       " 992,\n",
       " 1444,\n",
       " 1434,\n",
       " 409,\n",
       " 2074,\n",
       " 408,\n",
       " 1125,\n",
       " 713,\n",
       " 80,\n",
       " 147,\n",
       " 1489,\n",
       " 1531,\n",
       " 1527,\n",
       " 460,\n",
       " 458,\n",
       " 55,\n",
       " 158,\n",
       " 902,\n",
       " 2634,\n",
       " 2477,\n",
       " 2711,\n",
       " 1063,\n",
       " 2195,\n",
       " 2331,\n",
       " 422,\n",
       " 1668,\n",
       " 404,\n",
       " 1547,\n",
       " 1196,\n",
       " 430,\n",
       " 577,\n",
       " 188,\n",
       " 1218,\n",
       " 2609,\n",
       " 1780,\n",
       " 731,\n",
       " 1859,\n",
       " 1635,\n",
       " 797,\n",
       " 38,\n",
       " 1277,\n",
       " 2732,\n",
       " 521,\n",
       " 2440,\n",
       " 166,\n",
       " 2022,\n",
       " 33,\n",
       " 544,\n",
       " 2703,\n",
       " 678,\n",
       " 2770,\n",
       " 1351,\n",
       " 1344,\n",
       " 322,\n",
       " 2363,\n",
       " 2337,\n",
       " 1431,\n",
       " 2696,\n",
       " 580,\n",
       " 1994,\n",
       " 354,\n",
       " 1478,\n",
       " 497,\n",
       " 1986,\n",
       " 977,\n",
       " 1270,\n",
       " 16,\n",
       " 2347,\n",
       " 2377,\n",
       " 1514,\n",
       " 179,\n",
       " 2521,\n",
       " 1192,\n",
       " 1959,\n",
       " 1338,\n",
       " 888,\n",
       " 845,\n",
       " 2681,\n",
       " 1208,\n",
       " 858,\n",
       " 716,\n",
       " 1589,\n",
       " 1082,\n",
       " 1507,\n",
       " 1726,\n",
       " 819,\n",
       " 2880,\n",
       " 2459,\n",
       " 1323,\n",
       " 2602,\n",
       " 2649,\n",
       " 2002,\n",
       " 2643,\n",
       " 1209,\n",
       " 274,\n",
       " 1457,\n",
       " 1027,\n",
       " 1185,\n",
       " 1386,\n",
       " 150,\n",
       " 2710,\n",
       " 1194,\n",
       " 2149,\n",
       " 785,\n",
       " 1657,\n",
       " 1712,\n",
       " 2129,\n",
       " 1014,\n",
       " 2024,\n",
       " 2832,\n",
       " 2410,\n",
       " 1732,\n",
       " 789,\n",
       " 2725,\n",
       " 1751,\n",
       " 1155,\n",
       " 2851,\n",
       " 2563,\n",
       " 669,\n",
       " 1626,\n",
       " 2467,\n",
       " 127,\n",
       " 614,\n",
       " 2457,\n",
       " 2761,\n",
       " 668,\n",
       " 1574,\n",
       " 974,\n",
       " 1676,\n",
       " 1953,\n",
       " 1236,\n",
       " 151,\n",
       " 2277,\n",
       " 1529,\n",
       " 2819,\n",
       " 414,\n",
       " 2063,\n",
       " 213,\n",
       " 2205,\n",
       " 1921,\n",
       " 2209,\n",
       " 1962,\n",
       " 1526,\n",
       " 1618,\n",
       " 400,\n",
       " 1504,\n",
       " 132,\n",
       " 1318,\n",
       " 1620,\n",
       " 2877,\n",
       " 936,\n",
       " 345,\n",
       " 1600,\n",
       " 945,\n",
       " 1950,\n",
       " 1660,\n",
       " 2179,\n",
       " 450,\n",
       " 2811,\n",
       " 433,\n",
       " 629,\n",
       " 2432,\n",
       " 1412,\n",
       " 917,\n",
       " 2261,\n",
       " 820,\n",
       " 965,\n",
       " 1482,\n",
       " 2593,\n",
       " 1619,\n",
       " 720,\n",
       " 2445,\n",
       " 1202,\n",
       " 51,\n",
       " 1704,\n",
       " 1889,\n",
       " 1111,\n",
       " 2463,\n",
       " 1417,\n",
       " 2281,\n",
       " 1743,\n",
       " 2662,\n",
       " 50,\n",
       " 513,\n",
       " 993,\n",
       " 189,\n",
       " 1473,\n",
       " 1754,\n",
       " 546,\n",
       " 625,\n",
       " 1206,\n",
       " 665,\n",
       " 966,\n",
       " 2159,\n",
       " 2490,\n",
       " 2485,\n",
       " 2538,\n",
       " 253,\n",
       " 289,\n",
       " 2166,\n",
       " 1262,\n",
       " 622,\n",
       " 901,\n",
       " 438,\n",
       " 1222,\n",
       " 2556,\n",
       " 186,\n",
       " 59,\n",
       " 515,\n",
       " 261,\n",
       " 31,\n",
       " 2059,\n",
       " 277,\n",
       " 2258,\n",
       " 1453,\n",
       " 1967,\n",
       " 1765,\n",
       " 2188,\n",
       " 2872,\n",
       " 2175,\n",
       " 1982,\n",
       " 595,\n",
       " 2468,\n",
       " 369,\n",
       " 1980,\n",
       " 15,\n",
       " 562,\n",
       " 2532,\n",
       " 1922,\n",
       " 676,\n",
       " 952,\n",
       " 454,\n",
       " 2058,\n",
       " 655,\n",
       " 1364,\n",
       " 2329,\n",
       " 1308,\n",
       " 798,\n",
       " 1167,\n",
       " 1231,\n",
       " 1659,\n",
       " 2817,\n",
       " 2094,\n",
       " 1586,\n",
       " 73,\n",
       " 999,\n",
       " 1843,\n",
       " 507,\n",
       " 312,\n",
       " 1645,\n",
       " 2399,\n",
       " 2667,\n",
       " 755,\n",
       " 2690,\n",
       " 2437,\n",
       " 1906,\n",
       " 1939,\n",
       " 2400,\n",
       " 527,\n",
       " 1989,\n",
       " 1493,\n",
       " 2592,\n",
       " 152,\n",
       " 930,\n",
       " 590,\n",
       " 786,\n",
       " 1011,\n",
       " 437,\n",
       " 1778,\n",
       " 1567,\n",
       " 1520,\n",
       " 1324,\n",
       " 45,\n",
       " 587,\n",
       " 2017,\n",
       " 2655,\n",
       " 2840,\n",
       " 2499,\n",
       " 1395,\n",
       " 1470,\n",
       " 1104,\n",
       " 1562,\n",
       " 2156,\n",
       " 235,\n",
       " 466,\n",
       " 1436,\n",
       " 459,\n",
       " 1637,\n",
       " 1525,\n",
       " 873,\n",
       " 139,\n",
       " 1696,\n",
       " 1257,\n",
       " 1447,\n",
       " 1132,\n",
       " 2067,\n",
       " 834,\n",
       " 925,\n",
       " 218,\n",
       " 58,\n",
       " 2514,\n",
       " 2123,\n",
       " 2596,\n",
       " 245,\n",
       " 2091,\n",
       " 2491,\n",
       " 583,\n",
       " 468,\n",
       " 2654,\n",
       " 739,\n",
       " 1269,\n",
       " 628,\n",
       " 1505,\n",
       " 700,\n",
       " 1613,\n",
       " 555,\n",
       " 456,\n",
       " 518,\n",
       " 1975,\n",
       " 2607,\n",
       " 309,\n",
       " 1883,\n",
       " 2852,\n",
       " 2545,\n",
       " 2688,\n",
       " 1909,\n",
       " 947,\n",
       " 1119,\n",
       " 2769,\n",
       " 2815,\n",
       " 1320,\n",
       " 1343,\n",
       " 395,\n",
       " 1024,\n",
       " 946,\n",
       " 2292,\n",
       " 1752,\n",
       " 2909,\n",
       " 2090,\n",
       " 650,\n",
       " 1762,\n",
       " 2391,\n",
       " 2677,\n",
       " 634,\n",
       " 1091,\n",
       " 1896,\n",
       " 1365,\n",
       " 1013,\n",
       " 1451,\n",
       " 455,\n",
       " 1139,\n",
       " 2462,\n",
       " 492,\n",
       " 472,\n",
       " 2661,\n",
       " 2382,\n",
       " 2672,\n",
       " 373,\n",
       " 1162,\n",
       " 2859,\n",
       " 1420,\n",
       " 1254,\n",
       " 2674,\n",
       " 1074,\n",
       " 863,\n",
       " 2564,\n",
       " 2076,\n",
       " 1291,\n",
       " 296,\n",
       " 84,\n",
       " 1888,\n",
       " 109,\n",
       " 298,\n",
       " 231,\n",
       " 81,\n",
       " 607,\n",
       " 1612,\n",
       " 487,\n",
       " 1123,\n",
       " 554,\n",
       " 1421,\n",
       " 294,\n",
       " 411,\n",
       " 488,\n",
       " 896,\n",
       " 1546,\n",
       " 2190,\n",
       " 955,\n",
       " 2879,\n",
       " 502,\n",
       " 922,\n",
       " 897,\n",
       " 718,\n",
       " 2721,\n",
       " 2630,\n",
       " 2284,\n",
       " 976,\n",
       " 40,\n",
       " 559,\n",
       " 1259,\n",
       " 184,\n",
       " 2901,\n",
       " 892,\n",
       " 2358,\n",
       " 486,\n",
       " 159,\n",
       " 2073,\n",
       " 501,\n",
       " 722,\n",
       " 855,\n",
       " 2737,\n",
       " 1278,\n",
       " 907,\n",
       " 1605,\n",
       " 848,\n",
       " 2021,\n",
       " 268,\n",
       " 2300,\n",
       " 2272,\n",
       " 516,\n",
       " 1740,\n",
       " 2072,\n",
       " 2158,\n",
       " 2823,\n",
       " 2364,\n",
       " 2402,\n",
       " 2048,\n",
       " 875,\n",
       " 1774,\n",
       " 1187,\n",
       " 2288,\n",
       " 1093,\n",
       " 2673,\n",
       " 1522,\n",
       " 1005,\n",
       " 1304,\n",
       " 42,\n",
       " 2581,\n",
       " 1098,\n",
       " 1253,\n",
       " 8,\n",
       " 1755,\n",
       " 192,\n",
       " 548,\n",
       " 60,\n",
       " 94,\n",
       " 37,\n",
       " 2333,\n",
       " 2013,\n",
       " 2267,\n",
       " 881,\n",
       " 983,\n",
       " 1092,\n",
       " 1832,\n",
       " 2755,\n",
       " 1963,\n",
       " 1276,\n",
       " 267,\n",
       " 2843,\n",
       " 597,\n",
       " 1552,\n",
       " 1923,\n",
       " 2146,\n",
       " 1124,\n",
       " 2759,\n",
       " 532,\n",
       " 1358,\n",
       " 975,\n",
       " 237,\n",
       " 457,\n",
       " 954,\n",
       " 685,\n",
       " 1150,\n",
       " 2199,\n",
       " 1625,\n",
       " 1841,\n",
       " 1147,\n",
       " 2408,\n",
       " 131,\n",
       " 1226,\n",
       " 987,\n",
       " 2394,\n",
       " 2176,\n",
       " 2248,\n",
       " 140,\n",
       " 1744,\n",
       " 475,\n",
       " 1870,\n",
       " 1878,\n",
       " 1649,\n",
       " 2289,\n",
       " 2187,\n",
       " 344,\n",
       " 2482,\n",
       " 474,\n",
       " 1871,\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classifier_dropout_prob\": 0.1,\n",
       "  \"embedding_size\": 128,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_dropout_prob\": 0,\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"inner_group_num\": 1,\n",
       "  \"intermediate_size\": 16384,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"albert\",\n",
       "  \"num_attention_heads\": 64,\n",
       "  \"num_hidden_groups\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AlbertConfig, AlbertForMaskedLM\n",
    "\n",
    "albertconfig = AlbertConfig()\n",
    "\n",
    "albertconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForMaskedLM(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(2916, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2911, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=256, out_features=512, bias=True)\n",
       "              (ffn_output): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predictions): AlbertMLMHead(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (decoder): Linear(in_features=128, out_features=2916, bias=True)\n",
       "    (activation): NewGELUActivation()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albertconfig = AlbertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=tokenizer.model_max_length,\n",
    "    num_hidden_layers=6,\n",
    ")\n",
    "\n",
    "\n",
    "AlbertForMaskedLM(albertconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig()\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2916"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(2916, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2911, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=256, out_features=2916, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertconfig = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=tokenizer.model_max_length,\n",
    "    num_hidden_layers=6,\n",
    ")\n",
    "from transformers import AutoModelForMaskedLM, BertForMaskedLM\n",
    "\n",
    "BertForMaskedLM(bertconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForMaskedLM, BertForMaskedLM\n",
    "\n",
    "# model = BertForMaskedLM(bertconfig)\n",
    "from transformers import AutoModelForMaskedLM, BertForMaskedLM, AutoModelForPreTraining\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"result/dl/ProteomicsBERT/checkpoint-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2916, 256, padding_idx=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch.\n",
    "model.bert.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['eid', 'values', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "o = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.0208, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForPreTraining(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(2916, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2916, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=256, out_features=2916, bias=True)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1643, 2178,  574,  ...,  178, 2199, 1496],\n",
       "        [ 105, 1243, 2477,  ..., 2012, 1496, 2007]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(o.prediction_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2775, 2537, 1374,  ..., 2359, 2214,  948],\n",
       "        [1153,  835, 2225,  ..., 1497,    3, 1304]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, 1606, -100]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.0299, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.CrossEntropyLoss()(\n",
    "    o.prediction_logits.view(-1, tokenizer.vocab_size), batch[\"labels\"].view(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 2916])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " o.prediction_logits.view(-1, tokenizer.vocab_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2775, 2537, 1374,  ..., 2359, 2214,  948],\n",
       "        [1153,  835, 2225,  ..., 1497, 1453, 1304],\n",
       "        [1837, 1500, 2624,  ..., 1265,    3, 1918],\n",
       "        ...,\n",
       "        [ 371, 2825,  943,  ...,  524, 2886, 1549],\n",
       "        [2537, 2829,    3,  ..., 1380, 1278,  739],\n",
       "        [ 389, 2537, 2488,  ..., 1663, 1429, 1442]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trim26 spint3 il17f ccl20 erc2 s100a14 fam171a2 [MASK] pglyrp1 kir2ds4'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch[\"input_ids\"][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2775, 2537, 1374,  391,  957, 2323,  996,    3, 2028, 1499],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100, -100, -100, -100, -100, -100, -100, 1177, -100, -100],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.from_pretrained(\"result/dl/ProteomicsBERT/checkpoint-99500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[2778, 2540, 1377,  ...,  374,  856, 1287],\n",
       "        [1159,  839, 2228,  ..., 1552,  565, 1183],\n",
       "        [1840, 1503, 2627,  ..., 1319,    3,  731],\n",
       "        ...,\n",
       "        [ 375, 2828,  949,  ..., 2871, 1697,  394],\n",
       "        [2540, 2832,    3,  ..., 1667,   56, 1649],\n",
       "        [ 393, 2540, 2491,  ...,  121,  984,  182]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, 2641, -100],\n",
       "        [-100, -100, -100,  ..., -100, 1857, -100],\n",
       "        ...,\n",
       "        [-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [2540, -100,  717,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m o \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m      3\u001b[0m o\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/transformers/modeling_utils.py:2576\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2573\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2575\u001b[0m         )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "o = model(batch)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/IPython/core/formatters.py:711\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    704\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    705\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    707\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    708\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    709\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    710\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 711\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/IPython/lib/pretty.py:411\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    410\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 411\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/IPython/lib/pretty.py:779\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/collections/__init__.py:1123\u001b[0m, in \u001b[0;36mUserDict.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/_tensor.py:461\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    458\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    459\u001b[0m     )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/_tensor_str.py:677\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    676\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/_tensor_str.py:597\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    596\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 597\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    600\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/_tensor_str.py:349\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    347\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m _Formatter(\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/_tensor_str.py:385\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     start \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)]\n\u001b[1;32m    384\u001b[0m     end \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))]\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/_tensor_str.py:385\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    383\u001b[0m     start \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)]\n\u001b[1;32m    384\u001b[0m     end \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))]\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/torch/_tensor_str.py:375\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[0;32m--> 375\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medgeitems\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medgeitems\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# model(batch)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"distilbert/distilroberta-base\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertTokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "protein_tokenizer = BertTokenizerFast.from_pretrained(\"transtab/tokenizer\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return protein_tokenizer(examples[\"proteins\"])\n",
    "\n",
    "\n",
    "tokenized_datasets = test_dataset.map(\n",
    "    tokenize_function, batched=True, num_proc=4, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_tokenizer(test_dataset[0][\"proteins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_list = {}\n",
    "\n",
    "\n",
    "# for idx, row in test_data.set_index(\"eid\").iterrows():\n",
    "#     ranked_row = row.sort_values(ascending=False).dropna()\n",
    "#     res = {}\n",
    "#     res[\"eid\"] = ranked_row.name\n",
    "#     res[\"proteins\"] = ranked_row.index.tolist()\n",
    "#     res[\"values\"] = ranked_row.values.tolist()\n",
    "#     res_list.append(res)\n",
    "\n",
    "\n",
    "# def dict_generator():\n",
    "#     for i in range(len(res_list)):\n",
    "#         yield res_list[i]\n",
    "\n",
    "# Dataset.from_generator(dict_generator, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.from_generator(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def rank_genes(gene_vector, gene_tokens):\n",
    "    \"\"\"\n",
    "    Rank gene expression vector.\n",
    "    \"\"\"\n",
    "    # sort by median-scaled gene values\n",
    "    sorted_indices = np.argsort(-gene_vector)\n",
    "    return gene_tokens[sorted_indices], gene_vector[sorted_indices]\n",
    "\n",
    "\n",
    "def rank_sorted(examples, protein_cols=None):\n",
    "    \"\"\"\n",
    "    Rank protein expression vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    protein_expression_vectors = np.array([examples[col] for col in protein_cols])\n",
    "    protein_tokens = protein_cols\n",
    "    ranked_protein_tokens, ranked_protein_expression_vectors = rank_genes(\n",
    "        protein_expression_vectors, protein_tokens\n",
    "    )\n",
    "\n",
    "    other_cols = [col for col in examples.columns if col not in protein_cols]\n",
    "    return_res = {}\n",
    "    for i, col in enumerate(other_cols):\n",
    "        return_res[col] = examples[col].values\n",
    "    return_res[\"protein_tokens\"] = ranked_protein_tokens\n",
    "    return_res[\"protein_expression_vectors\"] = ranked_protein_expression_vectors\n",
    "    return return_res\n",
    "\n",
    "\n",
    "test_dataset.map(lambda x: rank_sorted(x, protein_cols=protein_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = True \n",
    "# if normalize:\n",
    "#     # pd.concat([train_data, test_data])[protein_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>3-Hydroxybutyrate</th>\n",
       "      <th>Acetate</th>\n",
       "      <th>Acetoacetate</th>\n",
       "      <th>Acetone</th>\n",
       "      <th>Alanine</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Apolipoprotein A1</th>\n",
       "      <th>Apolipoprotein B</th>\n",
       "      <th>Apolipoprotein B to Apolipoprotein A1 ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>Triglycerides to Total Lipids in Medium VLDL percentage</th>\n",
       "      <th>Triglycerides to Total Lipids in Small HDL percentage</th>\n",
       "      <th>Triglycerides to Total Lipids in Small LDL percentage</th>\n",
       "      <th>Triglycerides to Total Lipids in Small VLDL percentage</th>\n",
       "      <th>Triglycerides to Total Lipids in Very Large HDL percentage</th>\n",
       "      <th>Triglycerides to Total Lipids in Very Large VLDL percentage</th>\n",
       "      <th>Triglycerides to Total Lipids in Very Small VLDL percentage</th>\n",
       "      <th>Tyrosine</th>\n",
       "      <th>VLDL Cholesterol</th>\n",
       "      <th>Valine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.742980e+05</td>\n",
       "      <td>269219.000000</td>\n",
       "      <td>274092.000000</td>\n",
       "      <td>274286.000000</td>\n",
       "      <td>274294.000000</td>\n",
       "      <td>274188.000000</td>\n",
       "      <td>274252.000000</td>\n",
       "      <td>274296.000000</td>\n",
       "      <td>274296.000000</td>\n",
       "      <td>274296.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>274295.000000</td>\n",
       "      <td>274295.000000</td>\n",
       "      <td>274295.000000</td>\n",
       "      <td>274295.000000</td>\n",
       "      <td>274185.000000</td>\n",
       "      <td>271667.000000</td>\n",
       "      <td>274295.000000</td>\n",
       "      <td>273971.000000</td>\n",
       "      <td>274295.000000</td>\n",
       "      <td>274070.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.506315e+06</td>\n",
       "      <td>0.060765</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>0.012999</td>\n",
       "      <td>0.014232</td>\n",
       "      <td>0.296304</td>\n",
       "      <td>39.347088</td>\n",
       "      <td>1.462377</td>\n",
       "      <td>0.849427</td>\n",
       "      <td>0.595225</td>\n",
       "      <td>...</td>\n",
       "      <td>47.854559</td>\n",
       "      <td>4.546763</td>\n",
       "      <td>5.516848</td>\n",
       "      <td>38.422912</td>\n",
       "      <td>5.307593</td>\n",
       "      <td>52.255282</td>\n",
       "      <td>19.635848</td>\n",
       "      <td>0.063042</td>\n",
       "      <td>0.720872</td>\n",
       "      <td>0.210312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.449998e+06</td>\n",
       "      <td>0.062247</td>\n",
       "      <td>0.033286</td>\n",
       "      <td>0.012315</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.078481</td>\n",
       "      <td>3.387807</td>\n",
       "      <td>0.246330</td>\n",
       "      <td>0.201663</td>\n",
       "      <td>0.166096</td>\n",
       "      <td>...</td>\n",
       "      <td>8.524622</td>\n",
       "      <td>1.329243</td>\n",
       "      <td>1.829603</td>\n",
       "      <td>6.697692</td>\n",
       "      <td>3.906198</td>\n",
       "      <td>8.177621</td>\n",
       "      <td>4.196483</td>\n",
       "      <td>0.014560</td>\n",
       "      <td>0.248205</td>\n",
       "      <td>0.043535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000025e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.061282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303990</td>\n",
       "      <td>0.154320</td>\n",
       "      <td>0.064523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.100230</td>\n",
       "      <td>1.682500</td>\n",
       "      <td>4.318400</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>3.665400</td>\n",
       "      <td>0.005511</td>\n",
       "      <td>0.063553</td>\n",
       "      <td>0.067384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.251038e+06</td>\n",
       "      <td>0.029318</td>\n",
       "      <td>0.011571</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.011095</td>\n",
       "      <td>0.239170</td>\n",
       "      <td>37.263000</td>\n",
       "      <td>1.288000</td>\n",
       "      <td>0.705990</td>\n",
       "      <td>0.474210</td>\n",
       "      <td>...</td>\n",
       "      <td>41.650000</td>\n",
       "      <td>3.626550</td>\n",
       "      <td>4.288200</td>\n",
       "      <td>33.813000</td>\n",
       "      <td>3.016800</td>\n",
       "      <td>48.215000</td>\n",
       "      <td>16.731000</td>\n",
       "      <td>0.052981</td>\n",
       "      <td>0.539820</td>\n",
       "      <td>0.180060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.504206e+06</td>\n",
       "      <td>0.043116</td>\n",
       "      <td>0.014985</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>0.012905</td>\n",
       "      <td>0.287390</td>\n",
       "      <td>39.344000</td>\n",
       "      <td>1.439300</td>\n",
       "      <td>0.833995</td>\n",
       "      <td>0.578330</td>\n",
       "      <td>...</td>\n",
       "      <td>47.173000</td>\n",
       "      <td>4.454000</td>\n",
       "      <td>5.123400</td>\n",
       "      <td>38.109000</td>\n",
       "      <td>4.386100</td>\n",
       "      <td>53.365000</td>\n",
       "      <td>19.002000</td>\n",
       "      <td>0.061228</td>\n",
       "      <td>0.697580</td>\n",
       "      <td>0.205370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.760605e+06</td>\n",
       "      <td>0.068369</td>\n",
       "      <td>0.018960</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>41.427000</td>\n",
       "      <td>1.610500</td>\n",
       "      <td>0.974930</td>\n",
       "      <td>0.698723</td>\n",
       "      <td>...</td>\n",
       "      <td>53.271000</td>\n",
       "      <td>5.352400</td>\n",
       "      <td>6.304800</td>\n",
       "      <td>42.704500</td>\n",
       "      <td>6.472000</td>\n",
       "      <td>57.635000</td>\n",
       "      <td>21.910000</td>\n",
       "      <td>0.071066</td>\n",
       "      <td>0.875325</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.024110e+06</td>\n",
       "      <td>3.843400</td>\n",
       "      <td>1.800600</td>\n",
       "      <td>0.758200</td>\n",
       "      <td>0.421170</td>\n",
       "      <td>1.272600</td>\n",
       "      <td>72.014000</td>\n",
       "      <td>3.399800</td>\n",
       "      <td>2.462100</td>\n",
       "      <td>3.586700</td>\n",
       "      <td>...</td>\n",
       "      <td>99.301000</td>\n",
       "      <td>35.914000</td>\n",
       "      <td>77.593000</td>\n",
       "      <td>81.040000</td>\n",
       "      <td>99.989000</td>\n",
       "      <td>97.041000</td>\n",
       "      <td>48.023000</td>\n",
       "      <td>0.389410</td>\n",
       "      <td>2.767600</td>\n",
       "      <td>0.847660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                eid  3-Hydroxybutyrate        Acetate   Acetoacetate  \\\n",
       "count  2.742980e+05      269219.000000  274092.000000  274286.000000   \n",
       "mean   3.506315e+06           0.060765       0.017903       0.012999   \n",
       "std    1.449998e+06           0.062247       0.033286       0.012315   \n",
       "min    1.000025e+06           0.000000       0.000000       0.000000   \n",
       "25%    2.251038e+06           0.029318       0.011571       0.006238   \n",
       "50%    3.504206e+06           0.043116       0.014985       0.009833   \n",
       "75%    4.760605e+06           0.068369       0.018960       0.015646   \n",
       "max    6.024110e+06           3.843400       1.800600       0.758200   \n",
       "\n",
       "             Acetone        Alanine        Albumin  Apolipoprotein A1  \\\n",
       "count  274294.000000  274188.000000  274252.000000      274296.000000   \n",
       "mean        0.014232       0.296304      39.347088           1.462377   \n",
       "std         0.005623       0.078481       3.387807           0.246330   \n",
       "min         0.002328       0.061282       0.000000           0.303990   \n",
       "25%         0.011095       0.239170      37.263000           1.288000   \n",
       "50%         0.012905       0.287390      39.344000           1.439300   \n",
       "75%         0.015644       0.344213      41.427000           1.610500   \n",
       "max         0.421170       1.272600      72.014000           3.399800   \n",
       "\n",
       "       Apolipoprotein B  Apolipoprotein B to Apolipoprotein A1 ratio  ...  \\\n",
       "count     274296.000000                                274296.000000  ...   \n",
       "mean           0.849427                                     0.595225  ...   \n",
       "std            0.201663                                     0.166096  ...   \n",
       "min            0.154320                                     0.064523  ...   \n",
       "25%            0.705990                                     0.474210  ...   \n",
       "50%            0.833995                                     0.578330  ...   \n",
       "75%            0.974930                                     0.698723  ...   \n",
       "max            2.462100                                     3.586700  ...   \n",
       "\n",
       "       Triglycerides to Total Lipids in Medium VLDL percentage  \\\n",
       "count                                      274295.000000         \n",
       "mean                                           47.854559         \n",
       "std                                             8.524622         \n",
       "min                                             0.000158         \n",
       "25%                                            41.650000         \n",
       "50%                                            47.173000         \n",
       "75%                                            53.271000         \n",
       "max                                            99.301000         \n",
       "\n",
       "       Triglycerides to Total Lipids in Small HDL percentage  \\\n",
       "count                                      274295.000000       \n",
       "mean                                            4.546763       \n",
       "std                                             1.329243       \n",
       "min                                             0.100230       \n",
       "25%                                             3.626550       \n",
       "50%                                             4.454000       \n",
       "75%                                             5.352400       \n",
       "max                                            35.914000       \n",
       "\n",
       "       Triglycerides to Total Lipids in Small LDL percentage  \\\n",
       "count                                      274295.000000       \n",
       "mean                                            5.516848       \n",
       "std                                             1.829603       \n",
       "min                                             1.682500       \n",
       "25%                                             4.288200       \n",
       "50%                                             5.123400       \n",
       "75%                                             6.304800       \n",
       "max                                            77.593000       \n",
       "\n",
       "       Triglycerides to Total Lipids in Small VLDL percentage  \\\n",
       "count                                      274295.000000        \n",
       "mean                                           38.422912        \n",
       "std                                             6.697692        \n",
       "min                                             4.318400        \n",
       "25%                                            33.813000        \n",
       "50%                                            38.109000        \n",
       "75%                                            42.704500        \n",
       "max                                            81.040000        \n",
       "\n",
       "       Triglycerides to Total Lipids in Very Large HDL percentage  \\\n",
       "count                                      274185.000000            \n",
       "mean                                            5.307593            \n",
       "std                                             3.906198            \n",
       "min                                             0.001715            \n",
       "25%                                             3.016800            \n",
       "50%                                             4.386100            \n",
       "75%                                             6.472000            \n",
       "max                                            99.989000            \n",
       "\n",
       "       Triglycerides to Total Lipids in Very Large VLDL percentage  \\\n",
       "count                                      271667.000000             \n",
       "mean                                           52.255282             \n",
       "std                                             8.177621             \n",
       "min                                             0.000184             \n",
       "25%                                            48.215000             \n",
       "50%                                            53.365000             \n",
       "75%                                            57.635000             \n",
       "max                                            97.041000             \n",
       "\n",
       "       Triglycerides to Total Lipids in Very Small VLDL percentage  \\\n",
       "count                                      274295.000000             \n",
       "mean                                           19.635848             \n",
       "std                                             4.196483             \n",
       "min                                             3.665400             \n",
       "25%                                            16.731000             \n",
       "50%                                            19.002000             \n",
       "75%                                            21.910000             \n",
       "max                                            48.023000             \n",
       "\n",
       "            Tyrosine  VLDL Cholesterol         Valine  \n",
       "count  273971.000000     274295.000000  274070.000000  \n",
       "mean        0.063042          0.720872       0.210312  \n",
       "std         0.014560          0.248205       0.043535  \n",
       "min         0.005511          0.063553       0.067384  \n",
       "25%         0.052981          0.539820       0.180060  \n",
       "50%         0.061228          0.697580       0.205370  \n",
       "75%         0.071066          0.875325       0.234800  \n",
       "max         0.389410          2.767600       0.847660  \n",
       "\n",
       "[8 rows x 252 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metabolism = pd.read_pickle(\n",
    "    \"/home/xutingfeng/ukb/ukbData/omics/metabolomics/parsed/2024/init_visit.pkl\"\n",
    ")\n",
    "\n",
    "metabolism.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000017</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-11.3690</td>\n",
       "      <td>3.56718</td>\n",
       "      <td>-1.975530</td>\n",
       "      <td>0.213937</td>\n",
       "      <td>-12.43420</td>\n",
       "      <td>-1.698380</td>\n",
       "      <td>-0.090687</td>\n",
       "      <td>-3.498190</td>\n",
       "      <td>4.762600</td>\n",
       "      <td>3.153210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000025</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-12.1620</td>\n",
       "      <td>2.77470</td>\n",
       "      <td>0.175048</td>\n",
       "      <td>2.554930</td>\n",
       "      <td>8.75958</td>\n",
       "      <td>-0.044124</td>\n",
       "      <td>-1.497300</td>\n",
       "      <td>0.052680</td>\n",
       "      <td>0.276735</td>\n",
       "      <td>2.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000038</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-12.8698</td>\n",
       "      <td>6.41566</td>\n",
       "      <td>-5.106100</td>\n",
       "      <td>-1.296310</td>\n",
       "      <td>-6.34291</td>\n",
       "      <td>-2.935870</td>\n",
       "      <td>1.690630</td>\n",
       "      <td>-1.932100</td>\n",
       "      <td>3.712410</td>\n",
       "      <td>-0.063338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000042</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.9437</td>\n",
       "      <td>-109.21600</td>\n",
       "      <td>74.692200</td>\n",
       "      <td>17.863400</td>\n",
       "      <td>-1.44577</td>\n",
       "      <td>-0.571180</td>\n",
       "      <td>-2.228180</td>\n",
       "      <td>1.646810</td>\n",
       "      <td>1.608430</td>\n",
       "      <td>5.003350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000056</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.7174</td>\n",
       "      <td>5.77507</td>\n",
       "      <td>0.620341</td>\n",
       "      <td>0.505251</td>\n",
       "      <td>-2.49160</td>\n",
       "      <td>1.052860</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>1.672830</td>\n",
       "      <td>-1.928450</td>\n",
       "      <td>-0.712658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502404</th>\n",
       "      <td>6024086</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.1845</td>\n",
       "      <td>4.08367</td>\n",
       "      <td>-0.006942</td>\n",
       "      <td>-0.325017</td>\n",
       "      <td>-5.32889</td>\n",
       "      <td>2.483810</td>\n",
       "      <td>-1.063800</td>\n",
       "      <td>-3.733520</td>\n",
       "      <td>3.016760</td>\n",
       "      <td>-0.309265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502405</th>\n",
       "      <td>6024098</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-13.3426</td>\n",
       "      <td>2.56658</td>\n",
       "      <td>-0.076882</td>\n",
       "      <td>6.048100</td>\n",
       "      <td>11.09400</td>\n",
       "      <td>1.417840</td>\n",
       "      <td>2.647870</td>\n",
       "      <td>1.042270</td>\n",
       "      <td>-2.291610</td>\n",
       "      <td>-1.093340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502406</th>\n",
       "      <td>6024103</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-12.2113</td>\n",
       "      <td>4.22902</td>\n",
       "      <td>-2.629170</td>\n",
       "      <td>4.489250</td>\n",
       "      <td>-2.29320</td>\n",
       "      <td>0.573617</td>\n",
       "      <td>1.350590</td>\n",
       "      <td>-1.911610</td>\n",
       "      <td>1.115080</td>\n",
       "      <td>0.535197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502407</th>\n",
       "      <td>6024110</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.5527</td>\n",
       "      <td>6.84118</td>\n",
       "      <td>-2.149580</td>\n",
       "      <td>-0.825010</td>\n",
       "      <td>-2.83187</td>\n",
       "      <td>-1.727010</td>\n",
       "      <td>1.742680</td>\n",
       "      <td>0.109792</td>\n",
       "      <td>-0.305446</td>\n",
       "      <td>-0.589371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502408</th>\n",
       "      <td>6024122</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.4469</td>\n",
       "      <td>2.66226</td>\n",
       "      <td>-5.098050</td>\n",
       "      <td>0.441097</td>\n",
       "      <td>-4.72467</td>\n",
       "      <td>-1.861650</td>\n",
       "      <td>0.624629</td>\n",
       "      <td>-3.139570</td>\n",
       "      <td>1.171670</td>\n",
       "      <td>-0.072533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>502409 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            eid   age  sex      PC1        PC2        PC3        PC4  \\\n",
       "0       1000017  56.0  1.0 -11.3690    3.56718  -1.975530   0.213937   \n",
       "1       1000025  62.0  1.0 -12.1620    2.77470   0.175048   2.554930   \n",
       "2       1000038  60.0  1.0 -12.8698    6.41566  -5.106100  -1.296310   \n",
       "3       1000042  60.0  1.0  72.9437 -109.21600  74.692200  17.863400   \n",
       "4       1000056  65.0  0.0 -10.7174    5.77507   0.620341   0.505251   \n",
       "...         ...   ...  ...      ...        ...        ...        ...   \n",
       "502404  6024086  66.0  0.0 -11.1845    4.08367  -0.006942  -0.325017   \n",
       "502405  6024098  68.0  1.0 -13.3426    2.56658  -0.076882   6.048100   \n",
       "502406  6024103  61.0  1.0 -12.2113    4.22902  -2.629170   4.489250   \n",
       "502407  6024110  66.0  1.0 -10.5527    6.84118  -2.149580  -0.825010   \n",
       "502408  6024122  66.0  0.0 -11.4469    2.66226  -5.098050   0.441097   \n",
       "\n",
       "             PC5       PC6       PC7       PC8       PC9      PC10  \n",
       "0      -12.43420 -1.698380 -0.090687 -3.498190  4.762600  3.153210  \n",
       "1        8.75958 -0.044124 -1.497300  0.052680  0.276735  2.118800  \n",
       "2       -6.34291 -2.935870  1.690630 -1.932100  3.712410 -0.063338  \n",
       "3       -1.44577 -0.571180 -2.228180  1.646810  1.608430  5.003350  \n",
       "4       -2.49160  1.052860  0.290698  1.672830 -1.928450 -0.712658  \n",
       "...          ...       ...       ...       ...       ...       ...  \n",
       "502404  -5.32889  2.483810 -1.063800 -3.733520  3.016760 -0.309265  \n",
       "502405  11.09400  1.417840  2.647870  1.042270 -2.291610 -1.093340  \n",
       "502406  -2.29320  0.573617  1.350590 -1.911610  1.115080  0.535197  \n",
       "502407  -2.83187 -1.727010  1.742680  0.109792 -0.305446 -0.589371  \n",
       "502408  -4.72467 -1.861650  0.624629 -3.139570  1.171670 -0.072533  \n",
       "\n",
       "[502409 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
