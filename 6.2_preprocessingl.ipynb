{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组织数据成GeneFormer支持的格式\n",
    "\n",
    "\n",
    "Geneformer需要loom格式的单细胞数据来进行tokenize，自己修改代码是可以，但是麻烦，选择套他的代码。\n",
    "\n",
    "- loom格式，行代表基因，列代表细胞，这里我们是人（为了符合GeneFormer需求）\n",
    "- ra 用记录蛋白信息的df；必须要有：ensembl_id、\n",
    "- ca 记录人的各种信息；`n_counts`需要；`filter_pass`可以需要，来表示该列是否需要保留，为0则会删除\n",
    "\n",
    "\n",
    ">其他需要的信息可以通过 传入一个字典来保留：`{\"cell_type\": \"cell_type\", \"organ_major\": \"organ\"}`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "from pathlib import Path\n",
    "\n",
    "tmpdir = \"tmp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_data\n",
    "\n",
    "save to loom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: eid信息缺失了，并且部分蛋白是用mean填充的，后续可以根本不需要填充，因为可以传入的时候没有他们\n",
    "\n",
    "train_imputed = pd.read_pickle(\"result/part1/train_imputed.pkl\")\n",
    "test_imputed = pd.read_pickle(\"result/part1/test_imputed.pkl\")\n",
    "\n",
    "\n",
    "protein_cols = test_imputed.columns[\n",
    "    test_imputed.columns.tolist().index(\"C3\") :\n",
    "].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed[\"incident_cad\"] = train_imputed[\"incident_cad\"].astype(int)\n",
    "test_imputed[\"incident_cad\"] = test_imputed[\"incident_cad\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 11:51:51.572214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 11:51:52.864806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from ppp_prediction.geneformer.in_silico_perturber_stats import (\n",
    "    GENE_NAME_ID_DICTIONARY_FILE,\n",
    ")\n",
    "import loompy\n",
    "\n",
    "\n",
    "def proteomics_to_loom(\n",
    "    data,\n",
    "    protein_cols,\n",
    "    ca_cols=None,\n",
    "    gene_name_id_dict_path=GENE_NAME_ID_DICTIONARY_FILE,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    Return: main_matrix, ra, ca\n",
    "    \"\"\"\n",
    "    gene_name_id_dict = pd.read_pickle(gene_name_id_dict_path)\n",
    "\n",
    "    ## check olink proteins in geneformer dict\n",
    "    in_geneforer_proteins = []\n",
    "    out_geneforer_proteins = []\n",
    "    for gene in protein_cols:\n",
    "        if gene not in gene_name_id_dict:\n",
    "            out_geneforer_proteins.append(gene)\n",
    "        else:\n",
    "            in_geneforer_proteins.append(gene)\n",
    "    print(f\"out_geneformer_proteins: {out_geneforer_proteins}\")\n",
    "    print(\n",
    "        f\"Successly found {len(in_geneforer_proteins)} proteins in geneformer, only {len(out_geneforer_proteins)} proteins are not found in geneformer\"\n",
    "    )\n",
    "\n",
    "    ## ca_attr_cols\n",
    "    ca_attr_cols = [col for col in data.columns if col not in protein_cols]\n",
    "    ca_df = data[ca_attr_cols]\n",
    "\n",
    "    main_df = data[in_geneforer_proteins].rename(columns=gene_name_id_dict).T\n",
    "\n",
    "    ra_df = main_df.index.to_frame().reset_index(drop=True)\n",
    "    ra_df.columns = [\"ensembl_id\"]\n",
    "\n",
    "    main_df = main_df.values\n",
    "    assert len(ra_df) == main_df.shape[0]\n",
    "    assert len(ca_df) == main_df.shape[1]\n",
    "    print(f\"finnal main_df shape: {main_df.shape}\")\n",
    "    ## gene_name => esmble_id\n",
    "\n",
    "    return main_df, ra_df.to_dict(\"list\"), ca_df.to_dict(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_geneformer_proteins: ['DEFB103A_DEFB103B', 'BTNL10', 'ARNTL', 'SKIV2L', 'DEFB104A_DEFB104B', 'GBA', 'MYLPF', 'BOLA2_BOLA2B', 'MENT', 'CTAG1A_CTAG1B', 'GPR15L', 'SARG', 'PALM2', 'BAP18', 'SPACA5_SPACA5B', 'CGB3_CGB5_CGB8', 'DDX58', 'CERT', 'DEFA1_DEFA1B', 'DEFB4A_DEFB4B', 'IL12A_IL12B', 'CKMT1A_CKMT1B', 'LGALS7_LGALS7B', 'MICB_MICA', 'NTproBNP', 'WARS', 'EBI3_IL27', 'FUT3_FUT5']\n",
      "Successly found 2883 proteins in geneformer, only 28 proteins are not found in geneformer\n",
      "finnal main_df shape: (2883, 36007)\n",
      "out_geneformer_proteins: ['DEFB103A_DEFB103B', 'BTNL10', 'ARNTL', 'SKIV2L', 'DEFB104A_DEFB104B', 'GBA', 'MYLPF', 'BOLA2_BOLA2B', 'MENT', 'CTAG1A_CTAG1B', 'GPR15L', 'SARG', 'PALM2', 'BAP18', 'SPACA5_SPACA5B', 'CGB3_CGB5_CGB8', 'DDX58', 'CERT', 'DEFA1_DEFA1B', 'DEFB4A_DEFB4B', 'IL12A_IL12B', 'CKMT1A_CKMT1B', 'LGALS7_LGALS7B', 'MICB_MICA', 'NTproBNP', 'WARS', 'EBI3_IL27', 'FUT3_FUT5']\n",
      "Successly found 2883 proteins in geneformer, only 28 proteins are not found in geneformer\n",
      "finnal main_df shape: (2883, 15432)\n"
     ]
    }
   ],
   "source": [
    "Path(tmpdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "main_matrix, ra, ca = proteomics_to_loom(train_imputed, protein_cols)\n",
    "\n",
    "loompy.create(f\"{tmpdir}/2_train_imputed.loom\", main_matrix, ra, ca)\n",
    "\n",
    "main_matrix, ra, ca = proteomics_to_loom(test_imputed, protein_cols)\n",
    "\n",
    "loompy.create(f\"{tmpdir}/2_test_imputed.loom\", main_matrix, ra, ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize + embedding => datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer.tokenizer import TOKEN_DICTIONARY_FILE\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from datasets import Dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")  # noqa\n",
    "import loompy as lp  # noqa\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import loompy as lp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rank_genes(gene_vector, gene_tokens):\n",
    "    \"\"\"\n",
    "    Rank gene expression vector.\n",
    "    \"\"\"\n",
    "    # sort by median-scaled gene values\n",
    "    sorted_indices = np.argsort(-gene_vector)\n",
    "    return gene_tokens[sorted_indices]\n",
    "\n",
    "\n",
    "def tokenize_ind(gene_vector, gene_tokens):\n",
    "    \"\"\"\n",
    "    Convert normalized gene expression vector to tokenized rank value encoding.\n",
    "    \"\"\"\n",
    "    # create array of gene vector with token indices\n",
    "    # mask undetected genes\n",
    "    nonzero_mask = np.nonzero(gene_vector)[0]\n",
    "    # rank by median-scaled gene values\n",
    "    return rank_genes(gene_vector[nonzero_mask], gene_tokens[nonzero_mask])\n",
    "\n",
    "\n",
    "class ProteomicsTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        custom_attr_name_dict=None,\n",
    "        nproc=1,\n",
    "        chunk_size=512,\n",
    "        model_input_size=2048,\n",
    "        special_token=False,\n",
    "        # gene_median_file=GENE_MEDIAN_FILE,\n",
    "        token_dictionary_file=TOKEN_DICTIONARY_FILE,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize tokenizer.\n",
    "\n",
    "        **Parameters:**\n",
    "\n",
    "        custom_attr_name_dict : None, dict\n",
    "            | Dictionary of custom attributes to be added to the dataset.\n",
    "            | Keys are the names of the attributes in the loom file.\n",
    "            | Values are the names of the attributes in the dataset.\n",
    "        nproc : int\n",
    "            | Number of processes to use for dataset mapping.\n",
    "        chunk_size : int = 512\n",
    "            | Chunk size for anndata tokenizer.\n",
    "        model_input_size : int = 2048\n",
    "            | Max input size of model to truncate input to.\n",
    "        special_token : bool = False\n",
    "            | Adds CLS token before and SEP token after rank value encoding.\n",
    "        # gene_median_file : Path\n",
    "        #     | Path to pickle file containing dictionary of non-zero median\n",
    "        #     | gene expression values across Genecorpus-30M.\n",
    "        token_dictionary_file : Path\n",
    "            | Path to pickle file containing token dictionary (Ensembl IDs:token).\n",
    "\n",
    "        \"\"\"\n",
    "        # dictionary of custom attributes {output dataset column name: input .loom column name}\n",
    "        self.custom_attr_name_dict = custom_attr_name_dict\n",
    "\n",
    "        # number of processes for dataset mapping\n",
    "        self.nproc = nproc\n",
    "\n",
    "        # chunk size for anndata tokenizer\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        # input size for tokenization\n",
    "        self.model_input_size = model_input_size\n",
    "\n",
    "        # add CLS and SEP tokens\n",
    "        self.special_token = special_token\n",
    "\n",
    "        # load dictionary of gene normalization factors\n",
    "        # (non-zero median value of expression across Genecorpus-30M)\n",
    "        # with open(gene_median_file, \"rb\") as f:\n",
    "        #     self.gene_median_dict = pickle.load(f)\n",
    "\n",
    "        # load token dictionary (Ensembl IDs:token)\n",
    "        with open(token_dictionary_file, \"rb\") as f:\n",
    "            self.gene_token_dict = pickle.load(f)\n",
    "\n",
    "        # gene keys for full vocabulary\n",
    "        self.gene_keys = list(self.gene_token_dict.keys())\n",
    "\n",
    "        # protein-coding and miRNA gene list dictionary for selecting .loom rows for tokenization\n",
    "        self.genelist_dict = dict(zip(self.gene_keys, [True] * len(self.gene_keys)))\n",
    "\n",
    "    def tokenize_loom(self, loom_file_path, target_sum=10_000):\n",
    "        if self.custom_attr_name_dict is not None:\n",
    "            file_ind_metadata = {\n",
    "                attr_key: [] for attr_key in self.custom_attr_name_dict.keys()\n",
    "            }\n",
    "\n",
    "        with lp.connect(str(loom_file_path)) as data:\n",
    "            # define coordinates of detected protein-coding or miRNA genes and vector of their normalization factors\n",
    "\n",
    "            coding_miRNA_loc = np.where(\n",
    "                [self.genelist_dict.get(i, False) for i in data.ra[\"ensembl_id\"]]\n",
    "            )[0]\n",
    "\n",
    "            # norm_factor_vector = np.array(\n",
    "            #     [\n",
    "            #         self.gene_median_dict[i]\n",
    "            #         for i in data.ra[\"ensembl_id\"][coding_miRNA_loc]\n",
    "            #     ]\n",
    "            # )\n",
    "            coding_miRNA_ids = data.ra[\"ensembl_id\"][coding_miRNA_loc]\n",
    "\n",
    "            not_in_gene_ids = set(data.ra[\"ensembl_id\"]) - set(self.gene_keys)\n",
    "            print(\n",
    "                f\"{len(not_in_gene_ids)} genes not in gene token dictionary, skipping them, some are: {list(not_in_gene_ids)[:5]}\"\n",
    "            )\n",
    "\n",
    "            coding_miRNA_tokens = np.array(\n",
    "                [self.gene_token_dict[i] for i in coding_miRNA_ids]\n",
    "            )\n",
    "\n",
    "            # define coordinates of individual passing filters for inclusion (e.g. QC)\n",
    "            try:\n",
    "                data.ca[\"filter_pass\"]\n",
    "            except AttributeError:\n",
    "                var_exists = False\n",
    "            else:\n",
    "                var_exists = True\n",
    "\n",
    "            if var_exists:\n",
    "                filter_pass_loc = np.where([i == 1 for i in data.ca[\"filter_pass\"]])[0]\n",
    "            elif not var_exists:\n",
    "                print(\n",
    "                    f\"{loom_file_path} has no column attribute 'filter_pass'; tokenizing all inds.\"\n",
    "                )\n",
    "                filter_pass_loc = np.array([i for i in range(data.shape[1])])\n",
    "\n",
    "            # scan through .loom files and tokenize inds\n",
    "            tokenized_ind = []\n",
    "            for _ix, _selection, view in data.scan(\n",
    "                items=filter_pass_loc, axis=1, batch_size=self.chunk_size\n",
    "            ):\n",
    "                # select subview with protein-coding and miRNA genes\n",
    "                subview = view.view[coding_miRNA_loc, :]\n",
    "                # Currently do not norm ,as the values is NPX by UKB\n",
    "\n",
    "                # tokenize subview gene vectors\n",
    "                tokenized_ind += [\n",
    "                    tokenize_ind(subview[:, i], coding_miRNA_tokens)\n",
    "                    for i in range(subview.shape[1])\n",
    "                ]\n",
    "\n",
    "                # add custom attributes for subview to dict\n",
    "                if self.custom_attr_name_dict is not None:\n",
    "                    for k in file_ind_metadata.keys():\n",
    "                        file_ind_metadata[k] += subview.ca[k].tolist()\n",
    "                else:\n",
    "                    file_ind_metadata = None\n",
    "\n",
    "        return tokenized_ind, file_ind_metadata\n",
    "\n",
    "    def create_dataset(\n",
    "        self,\n",
    "        tokenized_inds,\n",
    "        ind_metadata,\n",
    "        use_generator=False,\n",
    "        keep_uncropped_input_ids=False,\n",
    "    ):\n",
    "        print(\"Creating dataset.\")\n",
    "        # create dict for dataset creation\n",
    "        dataset_dict = {\"input_ids\": tokenized_inds}\n",
    "        if self.custom_attr_name_dict is not None:\n",
    "            dataset_dict.update(ind_metadata)\n",
    "\n",
    "        # create dataset\n",
    "        if use_generator:\n",
    "\n",
    "            def dict_generator():\n",
    "                for i in range(len(tokenized_inds)):\n",
    "                    yield {k: dataset_dict[k][i] for k in dataset_dict.keys()}\n",
    "\n",
    "            output_dataset = Dataset.from_generator(dict_generator, num_proc=self.nproc)\n",
    "        else:\n",
    "            output_dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "        def format_ind_features(example):\n",
    "            # Store original uncropped input_ids in separate feature\n",
    "            if keep_uncropped_input_ids:\n",
    "                example[\"input_ids_uncropped\"] = example[\"input_ids\"]\n",
    "                example[\"length_uncropped\"] = len(example[\"input_ids\"])\n",
    "\n",
    "            # Truncate/Crop input_ids to input size\n",
    "            if self.special_token:\n",
    "                example[\"input_ids\"] = example[\"input_ids\"][\n",
    "                    0 : self.model_input_size - 2\n",
    "                ]  # truncate to leave space for CLS and SEP token\n",
    "                example[\"input_ids\"] = np.insert(\n",
    "                    example[\"input_ids\"], 0, self.gene_token_dict.get(\"<cls>\")\n",
    "                )\n",
    "                example[\"input_ids\"] = np.insert(\n",
    "                    example[\"input_ids\"],\n",
    "                    len(example[\"input_ids\"]),\n",
    "                    self.gene_token_dict.get(\"<sep>\"),\n",
    "                )\n",
    "            else:\n",
    "                # Truncate/Crop input_ids to input size\n",
    "                example[\"input_ids\"] = example[\"input_ids\"][0 : self.model_input_size]\n",
    "            example[\"length\"] = len(example[\"input_ids\"])\n",
    "\n",
    "            return example\n",
    "\n",
    "        output_dataset_truncated = output_dataset.map(\n",
    "            format_ind_features, num_proc=self.nproc\n",
    "        )\n",
    "        return output_dataset_truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(\"ctheodoris/Geneformer\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mctheodoris/Geneformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(example):\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# inputs = torch.Tensor(example[\"input_ids\"]).unsqueeze(0).long()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# print(example)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# example[\"a\"] = np.random.normal(size=(len(example[\"input_ids\"]), 10, 10))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:488\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/transformers/utils/hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1628\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/huggingface_hub/file_download.py:408\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:67\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/urllib3/connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/urllib3/connectionpool.py:1097\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1097\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1101\u001b[0m         (\n\u001b[1;32m   1102\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconn\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1108\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/urllib3/connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    613\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ctheodoris/Geneformer\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"ctheodoris/Geneformer\").to(device)\n",
    "\n",
    "\n",
    "def get_embeddings(example):\n",
    "\n",
    "    # inputs = torch.Tensor(example[\"input_ids\"]).unsqueeze(0).long()\n",
    "    # outputs = model.bert(inputs)\n",
    "    # embeddings = outputs.last_hidden_state.cpu().detach().numpy()\n",
    "\n",
    "    # example[\"embeddings\"] = embeddings\n",
    "    # print(example)\n",
    "    # example[\"a\"] = np.random.normal(size=(len(example[\"input_ids\"]), 10, 10))\n",
    "    inputs = torch.Tensor(example[\"input_ids\"]).long().to(device)\n",
    "    outputs = model.bert(inputs).last_hidden_state.cpu().detach().numpy()\n",
    "    example[\"embeddings\"] = outputs\n",
    "\n",
    "    return example\n",
    "\n",
    "loom_dir = \"\"\n",
    "# train\n",
    "loom_file_path = f\"{tmpdir}/2_train_imputed.loom\"\n",
    "outputpath = f\"{tmpdir}/imputed_200/train\"\n",
    "Path(outputpath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "proteomics_tokenizer = ProteomicsTokenizer(\n",
    "    {\"incident_cad\": \"incident_cad\", \"eid\": \"eid\"},\n",
    "    model_input_size=200,\n",
    "    special_token=False,  # TODO: <cls> not in the dictionary\n",
    ")  # TODO: model_input_size may be larger if it is ok; special_token=True if we want to add CLS and SEP tokens\n",
    "tokenized_ind, file_ = proteomics_tokenizer.tokenize_loom(loom_file_path)  # toknize\n",
    "output_dataset_truncated = proteomics_tokenizer.create_dataset(\n",
    "    tokenized_ind, file_\n",
    ")  # create dataset\n",
    "\n",
    "output_dataset_truncated = output_dataset_truncated.map(\n",
    "    get_embeddings, batched=True, batch_size=256, num_proc=4\n",
    ")\n",
    "output_dataset_truncated.save_to_disk(outputpath)  # save to disk\n",
    "\n",
    "\n",
    "# test\n",
    "\n",
    "loom_file_path = f\"{tmpdir}/2_test_imputed.loom\"\n",
    "outputpath = f\"{tmpdir}/imputed_200/test\"\n",
    "Path(outputpath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "proteomics_tokenizer = ProteomicsTokenizer(\n",
    "    {\"incident_cad\": \"incident_cad\", \"eid\": \"eid\"},\n",
    "    model_input_size=200,\n",
    "    special_token=False,  # TODO: <cls> not in the dictionary\n",
    ")  # TODO: model_input_size may be larger if it is ok; special_token=True if we want to add CLS and SEP tokens\n",
    "tokenized_ind, file_ = proteomics_tokenizer.tokenize_loom(loom_file_path)  # toknize\n",
    "output_dataset_truncated = proteomics_tokenizer.create_dataset(\n",
    "    tokenized_ind, file_\n",
    ")  # create dataset\n",
    "\n",
    "output_dataset_truncated = output_dataset_truncated.map(\n",
    "    get_embeddings, batched=True, batch_size=256, num_proc=1\n",
    ")\n",
    "output_dataset_truncated.save_to_disk(outputpath)  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'incident_cad', 'eid', 'length'],\n",
       "    num_rows: 36007\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "train_Dataset = datasets.load_from_disk(f\"{tmpdir}/imputed_200/train\")\n",
    "train_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dataset_truncated.save_to_disk(outputpath)  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 genes not in gene token dictionary, skipping them, some are: ['ENSG00000275841', 'ENSG00000291237', 'ENSG00000228789', 'ENSG00000266200', 'ENSG00000248546']\n",
      "tmp//2_test_imputed.loom has no column attribute 'filter_pass'; tokenizing all inds.\n",
      "Creating dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3706ed631c974207816e7ba6c37d35a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15432 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c5b2acebb04eebadbbd5a1f4ff5026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15432 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loom_file_path = f\"{tmpdir}/2_test_imputed.loom\"\n",
    "outputpath = f\"{tmpdir}/imputed_200/test\"\n",
    "Path(outputpath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "proteomics_tokenizer = ProteomicsTokenizer(\n",
    "    {\"incident_cad\": \"incident_cad\"}, model_input_size=200, special_token=False\n",
    ")  # TODO: model_input_size may be larger if it is ok; special_token=True if we want to add CLS and SEP tokens\n",
    "tokenized_ind, file_ = proteomics_tokenizer.tokenize_loom(loom_file_path)  # toknize\n",
    "output_dataset_truncated = proteomics_tokenizer.create_dataset(\n",
    "    tokenized_ind, file_\n",
    ")  # create dataset\n",
    "output_dataset_truncated.save_to_disk(outputpath)  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'incident_cad', 'length'],\n",
       "    num_rows: 15432\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dataset_truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !source ~/vpn.sh\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ctheodoris/Geneformer\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"ctheodoris/Geneformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(f\"{tmpdir}/imputed_200/test\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a62c851300b4929855e7d8e9a8cdf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/15432 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_embeddings(example):\n",
    "\n",
    "    # inputs = torch.Tensor(example[\"input_ids\"]).unsqueeze(0).long()\n",
    "    # outputs = model.bert(inputs)\n",
    "    # embeddings = outputs.last_hidden_state.cpu().detach().numpy()\n",
    "\n",
    "    # example[\"embeddings\"] = embeddings\n",
    "    # print(example)\n",
    "    # example[\"a\"] = np.random.normal(size=(len(example[\"input_ids\"]), 10, 10))\n",
    "    inputs = torch.Tensor(example[\"input_ids\"]).long()\n",
    "    outputs = model.bert(inputs).last_hidden_state.cpu().detach().numpy()\n",
    "    example[\"embeddings\"] = outputs\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "test = dataset.map(get_embeddings, batched=True, batch_size=128, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "test[0][\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.443294683019979,\n",
       "  -0.28211410571610945,\n",
       "  1.2625037373664558,\n",
       "  -1.4698392975720649,\n",
       "  0.412663431492597,\n",
       "  -0.6720643058804685,\n",
       "  0.6370625505509272,\n",
       "  0.4383611692883055,\n",
       "  -0.9469268261102411,\n",
       "  0.1866431795129508],\n",
       " [2.018076132952118,\n",
       "  -1.396561747879193,\n",
       "  0.3363108759124171,\n",
       "  -0.9575563130223151,\n",
       "  0.06698016665688715,\n",
       "  -0.30448374389087024,\n",
       "  -0.390177235468002,\n",
       "  0.996180871902987,\n",
       "  0.6756530429210247,\n",
       "  1.483116023083148],\n",
       " [2.213912828257484,\n",
       "  -0.9047191237407438,\n",
       "  -0.16463655938118052,\n",
       "  0.5839528701611253,\n",
       "  -1.2072897546288042,\n",
       "  1.1303511706142206,\n",
       "  0.29084594055370894,\n",
       "  2.5040704770002056,\n",
       "  -1.330320759409782,\n",
       "  -0.2883352069137411],\n",
       " [-0.1312388478695491,\n",
       "  0.365759316566558,\n",
       "  -1.289969055516189,\n",
       "  1.6630244192019943,\n",
       "  0.1718978327084064,\n",
       "  -1.3203740999948013,\n",
       "  0.23104629638514015,\n",
       "  -0.6317994372823559,\n",
       "  2.6457231413562967,\n",
       "  1.3812916242164794],\n",
       " [-0.9094756971678113,\n",
       "  0.7200806249987156,\n",
       "  -0.21279409621144432,\n",
       "  1.0593773664464585,\n",
       "  -0.7921709715597298,\n",
       "  1.0725748325364728,\n",
       "  0.3748722525975901,\n",
       "  0.06627985289724342,\n",
       "  0.48594951991607777,\n",
       "  -1.2491122289679353],\n",
       " [-0.3130794875536191,\n",
       "  -0.661243975846399,\n",
       "  -1.061904363259633,\n",
       "  -0.5445656409059048,\n",
       "  -0.7161358694757013,\n",
       "  -0.5157919257106555,\n",
       "  -0.1983906153378191,\n",
       "  0.2740618569259018,\n",
       "  -0.30044455805736175,\n",
       "  -0.6259083222754881],\n",
       " [-1.4041014374623446,\n",
       "  -0.5052046199628285,\n",
       "  -1.3639327255404894,\n",
       "  0.4273033838720233,\n",
       "  0.3911159274970543,\n",
       "  1.4122284065934496,\n",
       "  -0.08983349341875171,\n",
       "  0.5130505330598687,\n",
       "  -1.708730543328933,\n",
       "  -1.686519046637552],\n",
       " [0.6961946396902916,\n",
       "  1.146635345301131,\n",
       "  0.6307154585916862,\n",
       "  -1.0162539042807655,\n",
       "  0.3291784932575339,\n",
       "  -2.1543606484523714,\n",
       "  -0.8429880675100664,\n",
       "  -0.9132825119219339,\n",
       "  -0.5291750476008511,\n",
       "  0.5128422212197162],\n",
       " [-1.0503994373442236,\n",
       "  1.1969375912572529,\n",
       "  0.2866500996762855,\n",
       "  0.23676149122456955,\n",
       "  -0.48299658097075104,\n",
       "  -0.8640875725976942,\n",
       "  -0.10357139161028009,\n",
       "  -0.16585342098084233,\n",
       "  -1.2437264439218014,\n",
       "  0.37483734871239377],\n",
       " [-1.7679906033554615,\n",
       "  -0.4421140312528047,\n",
       "  1.3805641923261083,\n",
       "  0.2248497261328456,\n",
       "  0.061082834651405064,\n",
       "  0.9621225547060457,\n",
       "  -0.8451869670003661,\n",
       "  0.39227110732571485,\n",
       "  0.18506945595163546,\n",
       "  -0.5721383315064483]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0][\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "# def get_embeddings(example):\n",
    "#     inputs = torch.Tensor(example[\"input_ids\"]).unsqueeze(0).long()\n",
    "#     outputs = model.bert(inputs)\n",
    "#     embeddings = outputs.last_hidden_state.cpu().detach().numpy()\n",
    "\n",
    "#     example[\"embeddings\"] = embeddings\n",
    "#     print(example)\n",
    "#     return example\n",
    "\n",
    "\n",
    "# dataset.map(get_embeddings, batched=True, batch_size=32, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# o = model(torch.Tensor(dataset[0][\"input_ids\"]).unsqueeze(0).long())\n",
    "# o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "o = model.bert(torch.Tensor(dataset[0][\"input_ids\"]).unsqueeze(0).long())\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_imputed\n",
    "protein_cols = protein_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_name_id_dict_geneformer = pd.read_pickle(GENE_NAME_ID_DICTIONARY_FILE)\n",
    "\n",
    "## check olink proteins in geneformer dict\n",
    "in_geneforer_proteins = []\n",
    "out_geneforer_proteins = []\n",
    "for gene in protein_cols:\n",
    "    if gene not in gene_name_id_dict_geneformer:\n",
    "        out_geneforer_proteins.append(gene)\n",
    "    else:\n",
    "        in_geneforer_proteins.append(gene)\n",
    "print(f\"out_geneformer_proteins: {out_geneforer_proteins}\")\n",
    "print(\n",
    "    f\"Successly found {len(in_geneforer_proteins)} proteins in geneformer, only {len(out_geneforer_proteins)} proteins are not found in geneformer\"\n",
    ")\n",
    "\n",
    "\n",
    "## ca_attr_cols\n",
    "ca_attr_cols = [col for col in data.columns if col not in protein_cols]\n",
    "ca_df = data[ca_attr_cols]\n",
    "\n",
    "main_df = data[in_geneforer_proteins].rename(columns=gene_name_id_dict_geneformer).T\n",
    "\n",
    "ra_df = main_df.index.to_frame().reset_index(drop=True)\n",
    "ra_df.columns = [\"ensembl_id\"]\n",
    "\n",
    "main_df = main_df.values\n",
    "assert len(ra_df) == main_df.shape[0]\n",
    "assert len(ca_df) == main_df.shape[1]\n",
    "print(f\"finnal main_df shape: {main_df.shape}\")\n",
    "## gene_name => esmble_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loompy\n",
    "\n",
    "loompy.create(\n",
    "    \"2_test_imputed.loom\", main_df, ra_df.to_dict(\"list\"), ca_df.to_dict(\"list\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ENSG00000175164\" in list(gene_name_id_dict_geneformer.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed[protein_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyranges as pr\n",
    "\n",
    "ensemble_anno_hs_path = \"/home/xutingfeng/ukb/externel/Homo_sapiens.GRCh38.111.gff3.gz\"\n",
    "ensemble_anno_hs = pr.read_gff3(ensemble_anno_hs_path)  # load gff3\n",
    "ensemble_anno_hs_gene = ensemble_anno_hs[ensemble_anno_hs.Feature == \"gene\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_anno_hs_gene[ensemble_anno_hs_gene.Name == \"ABO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_name_id_dict_geneformer = pd.read_pickle(\n",
    "    \"/home/xutingfeng/github_code/others/Geneformer/geneformer/gene_name_id_dict.pkl\"\n",
    ")\n",
    "\n",
    "gene_name_id_dict_geneformer[\"ABO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSG00000175164"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
