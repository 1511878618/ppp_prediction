{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/dask/array/chunk_types.py:110: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ppp_prediction.model_v2.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "from ppp_prediction.model import run_glmnet\n",
    "from ppp_prediction.cox import run_cox\n",
    "from ppp_prediction.metrics import cal_binary_metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from plotnine import *\n",
    "from sklearn.metrics import brier_score_loss, roc_curve, auc\n",
    "from dcurves import dca\n",
    "from functools import reduce, partial\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "\n",
    "def config_dict_to_df(config_dict, index_name):\n",
    "    \"\"\"\n",
    "    1) convert the config_dict to a dataframe\n",
    "    Example1\n",
    "            combination_dict = OrderedDict(\n",
    "        {\n",
    "            (\"PANEL\", \"Lasso\"): {\n",
    "                \"xvar\": [\"Age\", \"Sex\"],\n",
    "                \"model\": run_glmnet,\n",
    "                \"config\": {\"cv\": 6},\n",
    "            },\n",
    "            (\"PANEL\", \"xgboost\"): {\n",
    "                \"xvar\": [\"Age\", \"Sex\"],\n",
    "            },\n",
    "            (\"AgeSex\", \"xgboost\"): {\n",
    "                \"xvar\": [\"Age\", \"Sex\"],\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    config_dict_to_df(combination_dict, (\"combination\", \"model\"))\n",
    "\n",
    "                            xvar                                    model  \\\n",
    "    combination model                                                          \n",
    "    PANEL       Lasso    [Age, Sex]  <function run_glmnet at 0x7ff7aa182f80>   \n",
    "                xgboost  [Age, Sex]                                      NaN   \n",
    "    AgeSex      xgboost  [Age, Sex]                                      NaN   \n",
    "\n",
    "                            config  \n",
    "    combination model               \n",
    "    PANEL       Lasso    {'cv': 6}  \n",
    "                xgboost        NaN  \n",
    "    AgeSex      xgboost        NaN \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    config_df = pd.DataFrame(config_dict).T\n",
    "    config_df.index.set_names(index_name, inplace=True)\n",
    "    config_df.columns = pd.MultiIndex.from_tuples(\n",
    "        [(\"param\", col) for col in config_df.columns]\n",
    "    )\n",
    "\n",
    "    return config_df\n",
    "\n",
    "\n",
    "def update_concat_df(df1, df2, duplicate_replace=False, show_warning=True):\n",
    "    \"\"\"\n",
    "    update the df1 with df2, if duplicate_replace is True, then replace the duplicate rows\n",
    "\n",
    "    This will copy df1 and df2 to avoid modify the original df1 and df2\n",
    "\n",
    "    Update the df1 with df2, if duplicate_replace is True, then replace the duplicate rows.\n",
    "    This function copies df1 and df2 to avoid modifying the original dataframes.\n",
    "    Parameters:\n",
    "    - df1 (pandas.DataFrame): The first dataframe to be updated.\n",
    "    - df2 (pandas.DataFrame): The second dataframe used for updating df1.\n",
    "    - duplicate_replace (bool, optional): If True, replace duplicate rows in df1 with df2. Default is False.\n",
    "    - show_warning (bool, optional): If True, show warning messages. Default is True.\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): The updated dataframe.\n",
    "    Example:\n",
    "    df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "    df2 = pd.DataFrame({'A': [7, 8, 9], 'B': [10, 11, 12]})\n",
    "    updated_df = update_concat_df(df1, df2, duplicate_replace=True, show_warning=False)\n",
    "\n",
    "    \"\"\"\n",
    "    # WARNING: this will copy the df1 and df2\n",
    "    df1, df2 = df1.copy(), df2.copy()\n",
    "\n",
    "    new_adds = df2.index.difference(df1.index)\n",
    "    inter = df2.index.intersection(df1.index)\n",
    "\n",
    "    if duplicate_replace:\n",
    "        df1.drop(inter, inplace=True)\n",
    "        warning_duplicated = (\n",
    "            f\"Duplicate_replace is True, will replace the model_config with {inter}\"\n",
    "        )\n",
    "    else:\n",
    "        warning_duplicated = (\n",
    "            f\"Duplicate_replace is False, will skip the model_config with {inter}\"\n",
    "        )\n",
    "\n",
    "    if len(new_adds) > 0:\n",
    "        warning_new_add = (\n",
    "            f\"new model_config {new_adds} not in original model_config, will add them\"\n",
    "        )\n",
    "    else:\n",
    "        warning_new_add = \"No new model_config found\"\n",
    "\n",
    "    if show_warning:\n",
    "        logging.warning(warning_new_add) if len(warning_new_add) > 0 else None\n",
    "        logging.warning(warning_duplicated) if len(warning_duplicated) > 0 else None\n",
    "    df = pd.concat([df1, df2])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_risk_strat_df(data=None, y_true=None, y_pred=None, k=10, n_resample=1000):\n",
    "    \"\"\"\n",
    "    TODO: Add iris as an example\n",
    "    \"\"\"\n",
    "    if data is not None:\n",
    "        y_true = data[y_true]\n",
    "        y_pred = data[y_pred]\n",
    "    elif isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n",
    "        pass\n",
    "    elif isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray):\n",
    "        y_true = pd.Series(y_true)\n",
    "        y_pred = pd.Series(y_pred)\n",
    "    elif isinstance(y_true, list) and isinstance(y_pred, list):\n",
    "        y_true = pd.Series(y_true)\n",
    "        y_pred = pd.Series(y_pred)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"data should be a DataFrame or y_true and y_pred should be Series or list or numpy array\"\n",
    "        )\n",
    "\n",
    "    plt_df = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred}).dropna()\n",
    "    try:\n",
    "        plt_df[\"y_pred_bins\"] = pd.qcut(\n",
    "            plt_df[\"y_pred\"],\n",
    "            k,\n",
    "            labels=[f\"{i:.0f}%\" for i in (np.linspace(0, 1, k + 1) * 100)[1:]],\n",
    "        )\n",
    "    except ValueError:\n",
    "        raise ValueError(\"input data have many values are same and cannot be binned\")\n",
    "    if not n_resample:\n",
    "        plt_df_group = (\n",
    "            plt_df.groupby(\"y_pred_bins\")\n",
    "            .apply(lambda x: pd.Series({\"mean_true\": x.y_true.mean()}))\n",
    "            .reset_index(drop=False)\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        # 定义一个函数来计算均值\n",
    "        def mean_bootstrap(data):\n",
    "            # 使用bootstrap计算均值的置信区间\n",
    "            res = bootstrap(data=(data,), statistic=np.mean, n_resamples=n_resample)\n",
    "\n",
    "            return (\n",
    "                np.mean(data),\n",
    "                res.confidence_interval.low,\n",
    "                res.confidence_interval.high,\n",
    "            )\n",
    "\n",
    "        # 对每个分位数进行bootstrap抽样\n",
    "\n",
    "        plt_df_group = (\n",
    "            plt_df.groupby(\"y_pred_bins\")\n",
    "            .apply(\n",
    "                lambda x: pd.Series(\n",
    "                    list(mean_bootstrap(x[\"y_true\"])) + [x[\"y_pred\"].mean()],\n",
    "                    index=[\"mean_true\", \"ci_low\", \"ci_high\", \"mean_pred\"],\n",
    "                ).T\n",
    "            )\n",
    "            .reset_index(drop=False)\n",
    "        )\n",
    "\n",
    "    return plt_df_group\n",
    "\n",
    "\n",
    "def get_calibration_df(\n",
    "    data,\n",
    "    obs,\n",
    "    pred,\n",
    "    followup=None,\n",
    "    group=None,\n",
    "    n_bins=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO: Add iris as an example\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "\n",
    "    if followup is None:\n",
    "        followup = \"followup\"\n",
    "        data[followup] = 1\n",
    "\n",
    "    if group is not None:\n",
    "\n",
    "        data = data.groupby(group).apply(\n",
    "            lambda x: x.assign(decile=pd.qcut(x[pred], n_bins, labels=False))\n",
    "        )\n",
    "        data = (\n",
    "            data.groupby([group, \"decile\"])\n",
    "            .apply(\n",
    "                lambda x: pd.Series(\n",
    "                    {\n",
    "                        \"obsRate\": (x[obs] / x[followup]).mean(),\n",
    "                        \"obsRate_SE\": (x[obs] / x[followup]).std() / np.sqrt(len(x)),\n",
    "                        \"obsNo\": x[obs].sum(),\n",
    "                        \"predMean\": x[pred].mean(),\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        data = data.assign(decile=pd.qcut(data[pred], n_bins, labels=False))\n",
    "        data = (\n",
    "            data.groupby(\"decile\")\n",
    "            .apply(\n",
    "                lambda x: pd.Series(\n",
    "                    {\n",
    "                        \"obsRate\": (x[obs] / x[followup]).mean(),\n",
    "                        \"obsRate_SE\": (x[obs] / x[followup]).std() / np.sqrt(len(x)),\n",
    "                        \"obsNo\": x[obs].sum(),\n",
    "                        \"predMean\": x[pred].mean(),\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "    data[\"obsRate_UCI\"] = np.clip(\n",
    "        data[\"obsRate\"] + 1.96 * data[\"obsRate_SE\"], a_max=1, a_min=None\n",
    "    )\n",
    "    data[\"obsRate_LCI\"] = np.clip(\n",
    "        data[\"obsRate\"] - 1.96 * data[\"obsRate_SE\"], a_min=0, a_max=None\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "def calibration_score(\n",
    "    raw_train_pred,\n",
    "    raw_test_pred,\n",
    "    train_y,\n",
    "    method=\"isotonic\",\n",
    "    return_model=False,\n",
    "    need_scale=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO: Add iris as an example\n",
    "    \"\"\"\n",
    "    if method == \"isotonic\":\n",
    "        model = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "\n",
    "        if need_scale:\n",
    "            model = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "        else:\n",
    "            model = Pipeline([(\"model\", model)])\n",
    "\n",
    "        model.fit(raw_train_pred, train_y)\n",
    "\n",
    "        pred_train_calibrated = model.predict(raw_train_pred)\n",
    "        pred_test_calibrated = model.predict(raw_test_pred)\n",
    "    elif method == \"logitstic\":\n",
    "        model = LogisticRegression(\n",
    "            # class_weight=\"balanced\",\n",
    "            max_iter=5000,\n",
    "            random_state=1,\n",
    "        )\n",
    "        if need_scale:\n",
    "            model = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "        else:\n",
    "            model = Pipeline([(\"model\", model)])\n",
    "\n",
    "        raw_train_pred = (\n",
    "            raw_train_pred.values\n",
    "            if isinstance(raw_train_pred, pd.Series)\n",
    "            else raw_train_pred\n",
    "        )\n",
    "        raw_test_pred = (\n",
    "            raw_test_pred.values\n",
    "            if isinstance(raw_test_pred, pd.Series)\n",
    "            else raw_test_pred\n",
    "        )\n",
    "        model.fit(raw_train_pred.reshape(-1, 1), train_y)\n",
    "        pred_train_calibrated = model.predict_proba(raw_train_pred.reshape(-1, 1))[:, 1]\n",
    "        pred_test_calibrated = model.predict_proba(raw_test_pred.reshape(-1, 1))[:, 1]\n",
    "    else:\n",
    "        raise ValueError(\"method should be isotonic or logitstic\")\n",
    "    if return_model:\n",
    "        return pred_train_calibrated, pred_test_calibrated, model\n",
    "    else:\n",
    "        return pred_train_calibrated, pred_test_calibrated\n",
    "\n",
    "\n",
    "class DiseaseScoreModel_V2:\n",
    "    def __init__(\n",
    "        self,\n",
    "        disease_df,\n",
    "        model_table,\n",
    "        label,\n",
    "        disease_name=None,\n",
    "        train_eid=None,\n",
    "        test_eid=None,\n",
    "        eid=\"eid\",\n",
    "        other_keep_cols=None,\n",
    "        E=None,\n",
    "        T=None,\n",
    "        test_size=0.2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            TODO: Add iris as an example\n",
    "\n",
    "            meta_index_col is the index to record model summary information by a structure of DataFrame\n",
    "\n",
    "                model_config:{\n",
    "                    \"AgeSex\": {\n",
    "                        \"xvar\":[\"age\", \"sex\"]\n",
    "                        }\n",
    "                    \"KidneyImage\": {\n",
    "                        \"xvar\":KidneyImage\n",
    "                        \"model\":a function accept (train= train, test=test,xvar, y, **kwargs) and return (model, *others)\n",
    "                        \"config\":{\n",
    "                            \"cv\":5\n",
    "                            ...\n",
    "                        } # other config\n",
    "                        }\n",
    "        }\n",
    "                }\n",
    "                \n",
    "        model_table:\n",
    "\n",
    "                          param                                           \\\n",
    "                           xvar                                    model   \n",
    "combination model                                                          \n",
    "PANEL       Lasso    [Age, Sex]  <function run_glmnet at 0x7ff7aa182f80>   \n",
    "            xgboost  [Age, Sex]                                      NaN   \n",
    "AgeSex      xgboost  [Age, Sex]                                      NaN   \n",
    "\n",
    "                                \n",
    "                        config  \n",
    "combination model               \n",
    "PANEL       Lasso    {'cv': 6}  \n",
    "            xgboost        NaN  \n",
    "AgeSex      xgboost        NaN  \n",
    "\n",
    "\n",
    "    \n",
    "                other_keep_cols: other columns to keep in the final dataframe\n",
    "                eid : the column name of the unique identifier\n",
    "\n",
    "        \"\"\"\n",
    "        self.disease_df = disease_df\n",
    "\n",
    "        # step1 split data; can be down by train_eid, test_eid or random split or user run train_test_split\n",
    "        if train_eid is not None:\n",
    "            self.train = disease_df.query(f\"{eid} in @train_eid\")\n",
    "        if test_eid is not None:\n",
    "            self.test = disease_df.query(f\"{eid} in @test_eid\")\n",
    "        if train_eid is None and test_eid is None:\n",
    "            logging.warning(f\"Random split data with test_size: {test_size:.2f}\")\n",
    "            self.train, self.test = train_test_split(disease_df, test_size=test_size)\n",
    "\n",
    "        self.train.reset_index(drop=True, inplace=True)\n",
    "        self.test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        self.label = label\n",
    "        self.disease_name = disease_name or label\n",
    "        self.eid = eid\n",
    "\n",
    "        self.other_keep_cols = other_keep_cols if other_keep_cols else []\n",
    "\n",
    "        logging.info(\n",
    "            f\"Loading data with train cases {self.train[label].sum()} and test cases {self.test[label].sum()} of {self.disease_name}, while {len(self.train.columns)} columns\"\n",
    "        )\n",
    "\n",
    "        # E and T for cox model or C-index\n",
    "        self.E = E\n",
    "        self.T = T\n",
    "        if self.E and (self.E != self.label):\n",
    "            self.other_keep_cols.append(self.E)\n",
    "        if self.T:\n",
    "            self.other_keep_cols.append(self.T)\n",
    "        ## drop na by label, E and T\n",
    "        if self.E and self.T:\n",
    "            self.train.dropna(subset=[self.label, self.E, self.T], inplace=True)\n",
    "            self.test.dropna(subset=[self.label, self.E, self.T], inplace=True)\n",
    "        else:\n",
    "            self.train.dropna(subset=[self.label], inplace=True)\n",
    "            self.test.dropna(subset=[self.label], inplace=True)\n",
    "\n",
    "        logging.info(\n",
    "            f\"Drop NA by {self.label} and {self.E} and {self.T} in train and test and left {len(self.train)} and {len(self.test)} with train cases {self.train[self.label].sum()} and test cases {self.test[self.label].sum()}\"\n",
    "        )\n",
    "\n",
    "        # step2 update information to score_dict\n",
    "        # step2 save all infomation on a dataframe\n",
    "\n",
    "        self.model_table = model_table.copy()\n",
    "\n",
    "        self.train_score, self.test_score = (\n",
    "            self.train[[self.eid, self.label, *self.other_keep_cols]].copy(),\n",
    "            self.test[[self.eid, self.label, *self.other_keep_cols]].copy(),\n",
    "        )\n",
    "\n",
    "        # # keep the fitted model\n",
    "        # self.fitted_model_dict = OrderedDict()\n",
    "\n",
    "        # # keep the metrics\n",
    "        # self.metrics_dict = {}\n",
    "\n",
    "    # def get_metrics_df(self):\n",
    "    #     c_index_df_list = []\n",
    "    #     auc_df_list = []\n",
    "\n",
    "    #     for score_name, metrics in self.metrics_dict.items():\n",
    "    #         c_index = metrics.get(\"c_index\", None)\n",
    "    #         auc_metrics = metrics.get(\"auc_metrics\", None)\n",
    "\n",
    "    #         if c_index is not None:\n",
    "    #             c_index_df_list.append(c_index)\n",
    "    #         if auc_metrics is not None:\n",
    "    #             auc_df_list.append(auc_metrics)\n",
    "    #     c_index_df = pd.concat(c_index_df_list)\n",
    "    #     auc_df = pd.DataFrame(auc_df_list)\n",
    "    #     return c_index_df, auc_df\n",
    "\n",
    "    # def re_cal_metrics(self):\n",
    "    #     \"\"\"\n",
    "    #     re-calculate the metrics only AUC and C-Index\n",
    "    #     \"\"\"\n",
    "    #     for combination_name in self.fitted_model_dict.keys():\n",
    "    #         # cal metrics\n",
    "    #         need_cols = [self.label, combination_name]\n",
    "\n",
    "    #         ## E may equal to T\n",
    "    #         if self.E and self.T:\n",
    "    #             if self.E != self.label:\n",
    "    #                 need_cols.append(self.E)\n",
    "    #             need_cols.append(self.T)\n",
    "\n",
    "    #         to_cal_df = self.test_score[need_cols].copy().dropna()\n",
    "\n",
    "    #         c_index = run_cox(\n",
    "    #             to_cal_df, var=combination_name, E=self.E, T=self.T, ci=True\n",
    "    #         )\n",
    "    #         auc_metrics = cal_binary_metrics(\n",
    "    #             to_cal_df[self.label], to_cal_df[combination_name], ci=True\n",
    "    #         )\n",
    "\n",
    "    #         self.metrics_dict[combination_name] = {\n",
    "    #             \"c_index\": c_index,\n",
    "    #             \"auc_metrics\": auc_metrics,\n",
    "    #         }\n",
    "\n",
    "    def update_model(self, new_model_table=None, duplicate_replace=False):\n",
    "        \"\"\"\n",
    "        fit the model with the new model_config, or\n",
    "        \"\"\"\n",
    "        # update the model_config\n",
    "        if new_model_table is not None:\n",
    "            self.model_table = update_concat_df(\n",
    "                self.model_table,\n",
    "                new_model_table,\n",
    "                duplicate_replace=duplicate_replace,\n",
    "            )\n",
    "\n",
    "        # fit model by model_table\n",
    "        # fitted model will add a status to show whether the model is fitted\n",
    "\n",
    "        for name, model_table_row in self.model_table.iterrows():\n",
    "            # unpack the params\n",
    "            params = model_table_row[\"param\"].to_dict()\n",
    "\n",
    "            # get xvar\n",
    "            xvar = params[\"xvar\"]\n",
    "\n",
    "            # get model\n",
    "            if \"model\" in params:\n",
    "                model_fn = params[\"model\"]\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    f\"model function not found in {name}, use default glmnet to run lasso\"\n",
    "                )\n",
    "                model_fn = run_glmnet\n",
    "\n",
    "            # get model config\n",
    "            model_fn_config = params.get(\"config\", {})\n",
    "            if pd.isna(model_fn_config):\n",
    "                model_fn_config = {}\n",
    "\n",
    "            # get score name alias or\n",
    "            score_name = params.get(\n",
    "                \"score_name\", name if isinstance(name, str) else \"_\".join(name)\n",
    "            )\n",
    "\n",
    "            # fit the model\n",
    "            self.fit(\n",
    "                xvar=xvar,\n",
    "                name=name,\n",
    "                score_name=score_name,\n",
    "                model_fn=model_fn,\n",
    "                **model_fn_config,\n",
    "            )\n",
    "\n",
    "    def fit(self, xvar, name, score_name, model_fn=run_glmnet, **model_fn_config):\n",
    "        \"\"\"\n",
    "        fit the model with the combination\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # step1 check whether the model is already fitted by name have (\"status\", \"fitted\") value\n",
    "        try:\n",
    "            status_fitted = self.model_table.loc[name, (\"status\", \"fitted\")]\n",
    "        except KeyError:\n",
    "            status_fitted = False\n",
    "\n",
    "        if status_fitted == 1:\n",
    "            logging.warning(f\"{name} already fitted, will skip it\")\n",
    "            return\n",
    "\n",
    "        # step2 fit the model\n",
    "\n",
    "        model, *_ = model_fn(\n",
    "            train=self.train,\n",
    "            test=self.test,\n",
    "            xvar=xvar,\n",
    "            label=self.label,\n",
    "            **model_fn_config,\n",
    "        )\n",
    "\n",
    "        # TODO: use model.predict(model=model, data=self.train, xvar = combination) to replace the following\n",
    "        self.train_score[score_name] = get_predict_v2_from_df(model, self.train, xvar)\n",
    "        self.test_score[score_name] = get_predict_v2_from_df(model, self.test, xvar)\n",
    "        ## add the score into train_score\n",
    "        self.train[score_name] = get_predict_v2_from_df(model, self.train, xvar)\n",
    "        self.test[score_name] = get_predict_v2_from_df(model, self.test, xvar)\n",
    "\n",
    "        # cal metrics\n",
    "        need_cols = [self.label, score_name]\n",
    "\n",
    "        ## E may equal to T\n",
    "        if self.E and self.T:\n",
    "            if self.E != self.label:\n",
    "                need_cols.append(self.E)\n",
    "            need_cols.append(self.T)\n",
    "\n",
    "        to_cal_df = self.test_score[need_cols].copy().dropna()\n",
    "\n",
    "        # zscore for correct OR and HR\n",
    "        to_cal_df_train = self.train_score[need_cols].copy().dropna()\n",
    "        train_mean = to_cal_df_train[score_name].mean()\n",
    "        train_std = to_cal_df_train[score_name].std()\n",
    "\n",
    "        to_cal_df[score_name] = (to_cal_df[score_name] - train_mean) / train_std\n",
    "\n",
    "        # cal c\n",
    "        if self.E and self.T:\n",
    "            c_index_df = run_cox(to_cal_df, var=score_name, E=self.E, T=self.T, ci=True)\n",
    "            c_index_dict = c_index_df.iloc[0].T.to_dict()\n",
    "            for metric_name, metric_value in c_index_dict.items():\n",
    "                self.model_table.loc[name, (\"c_index\", metric_name)] = metric_value\n",
    "\n",
    "        # cal auc\n",
    "        auc_metrics_dict = cal_binary_metrics(\n",
    "            to_cal_df[self.label], to_cal_df[score_name], ci=True\n",
    "        )\n",
    "        for metric_name, metric_value in auc_metrics_dict.items():\n",
    "            self.model_table.loc[name, (\"auc\", metric_name)] = metric_value\n",
    "\n",
    "        # update model into model_table\n",
    "        self.model_table.loc[name, (\"status\", \"fitted\")] = 1\n",
    "\n",
    "        self.model_table.loc[name, (\"model\", \"model\")] = model\n",
    "        self.model_table.loc[name, (\"basic\", \"score_name\")] = score_name\n",
    "\n",
    "    def calibrate(self, method=\"logitstic\"):\n",
    "        \"\"\"\n",
    "        calibrate the score\n",
    "\n",
    "        \"\"\"\n",
    "        # check fitted status\n",
    "        self._check_status()\n",
    "\n",
    "        self.train_score_calibrated, self.test_score_calibrated = (\n",
    "            self.train[[self.eid, self.label, *self.other_keep_cols]].copy(),\n",
    "            self.test[[self.eid, self.label, *self.other_keep_cols]].copy(),\n",
    "        )\n",
    "\n",
    "        # for score_name, score_model_config in self.model_config.items():\n",
    "        for name, score_name in self.model_table[(\"basic\", \"score_name\")].items():\n",
    "\n",
    "            from ppp_prediction.calibration import calibrate\n",
    "\n",
    "            raw_train_score = self.train_score[[self.label, score_name]].dropna()\n",
    "            raw_test_score = self.test_score[[self.label, score_name]].dropna()\n",
    "\n",
    "            calibrated_object = calibrate(\n",
    "                X_train=raw_train_score[score_name],\n",
    "                X_test=raw_test_score[score_name],\n",
    "                y_train=raw_train_score[self.label],\n",
    "                y_test=raw_test_score[self.label],\n",
    "                n_bins=10,\n",
    "                need_scale=True,\n",
    "            )\n",
    "\n",
    "            calibration_model = calibrated_object[\"best_clf\"]\n",
    "\n",
    "            # TODO: use model.predict(model=model, data=self.train, xvar = combination) to replace the following\n",
    "            self.train_score_calibrated[score_name] = get_predict_v2_from_df(\n",
    "                calibration_model, self.train_score, [score_name]\n",
    "            )\n",
    "            self.test_score_calibrated[score_name] = get_predict_v2_from_df(\n",
    "                calibration_model, self.test_score, [score_name]\n",
    "            )\n",
    "\n",
    "            self.model_table.loc[name, (\"model\", \"calibrated_model\")] = (\n",
    "                calibration_model\n",
    "            )\n",
    "\n",
    "    def get_score_names(self):\n",
    "        # return list(self.fitted_model_dict.keys())\n",
    "        return self.model_table[(\"basic\", \"score_name\")].values.tolist()\n",
    "\n",
    "    def get_score_names_df(self):\n",
    "\n",
    "        # TODO: if level is more than 2, may have problem\n",
    "        return (\n",
    "            self.model_table[[(\"basic\", \"score_name\")]]\n",
    "            .copy()\n",
    "            .droplevel(0, axis=1)\n",
    "            .reset_index()\n",
    "        )  #\n",
    "\n",
    "    def set_color_set(self, colorset=None):\n",
    "        # self.color\n",
    "        if colorset is None:\n",
    "            colorset = list(sns.color_palette(\"tab20\").as_hex())\n",
    "\n",
    "        self.method_colorset = {k: v for k, v in zip(self.get_score_names(), colorset)}\n",
    "\n",
    "    @property\n",
    "    def color_set(self):\n",
    "        if not hasattr(self, \"method_colorset\"):\n",
    "            self.set_color_set()\n",
    "        return self.method_colorset\n",
    "\n",
    "    def get_metrics_by_user_multi(\n",
    "        self, metrics_dict=None, use_calibrate=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        metrics_dict: a dict with key as the metrics_name and value as the metrics_fn\n",
    "        \"\"\"\n",
    "        metrics_list = []\n",
    "        for metrics_name, metrics_fn in metrics_dict.items():\n",
    "            metrics_df = self.get_metrics_by_user(\n",
    "                metrics_fn,\n",
    "                metrics_name=metrics_name,\n",
    "                use_calibrate=use_calibrate,\n",
    "                **kwargs,\n",
    "            )\n",
    "            metrics_list.append(metrics_df)\n",
    "\n",
    "        return reduce(lambda x, y: pd.merge(x, y), metrics_list)\n",
    "\n",
    "    def get_metrics_by_user(\n",
    "        self, metrics_fn, metrics_name=None, use_calibrate=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        metrics_fn: a function accept (y_true, y_prob, other_kwargs) and return a dict; note the first pos will be the label and the second pos will be the score\n",
    "        \"\"\"\n",
    "        metrics_name = metrics_name or metrics_fn.__name__\n",
    "\n",
    "        metrics_list = []\n",
    "        for row_idx, row in self.get_score_names_df().iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            score_name = row_dict[\"score_name\"]\n",
    "\n",
    "            if use_calibrate:\n",
    "\n",
    "                to_cal_df = self.test_score_calibrated[\n",
    "                    [self.label, score_name]\n",
    "                ].dropna()\n",
    "\n",
    "            else:\n",
    "                to_cal_df = self.test_score[[self.label, score_name]].dropna()\n",
    "\n",
    "            metrics_score = metrics_fn(\n",
    "                to_cal_df[self.label],\n",
    "                to_cal_df[score_name],\n",
    "                **kwargs,\n",
    "            )\n",
    "            if isinstance(metrics_score, dict):\n",
    "                metrics_list.append(\n",
    "                    {\n",
    "                        **row_dict,\n",
    "                        **metrics_score,\n",
    "                    }\n",
    "                )\n",
    "                logging.info(\n",
    "                    f\"metrics {metrics_name} return a dict, will unpack it to the dataframe\"\n",
    "                )\n",
    "            else:\n",
    "                metrics_list.append(\n",
    "                    {\n",
    "                        **row_dict,\n",
    "                        metrics_name: metrics_score,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return pd.DataFrame(metrics_list)\n",
    "\n",
    "    def _check_status(\n",
    "        self,\n",
    "    ):\n",
    "        if \"status\" in self.model_table.columns.get_level_values(0):\n",
    "\n",
    "            if self.model_table[(\"status\", \"fitted\")].isna().any():\n",
    "                error_flag = True\n",
    "            else:\n",
    "                error_flag = False\n",
    "\n",
    "        else:\n",
    "            error_flag = False\n",
    "\n",
    "        if error_flag:\n",
    "            raise ValueError(f\"model not fitted, run update_model first\")\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    @property\n",
    "    def brier_score(self):\n",
    "        if not hasattr(self, \"train_score_calibrated\"):\n",
    "            logging.warning(\"No calibrated model fitted, run calibrate first\")\n",
    "            return\n",
    "        return self.get_metrics_by_user(\n",
    "            brier_score_loss, use_calibrate=True, metrics_name=\"brier_score\"\n",
    "        )\n",
    "\n",
    "    # TODO：画图呈现逻辑，现在默认不用分面，全部都以score_name做\n",
    "\n",
    "    def calibration_plot(self, n_bins=10, return_df=False, by=\"test\", facet_fn=None):\n",
    "\n",
    "        # check fitted status\n",
    "        self._check_status()\n",
    "\n",
    "        if not hasattr(self, \"train_score_calibrated\"):\n",
    "            logging.warning(\"No calibrated model fitted, run calibrate first\")\n",
    "            return\n",
    "        if by == \"test\":\n",
    "            by_data = self.test_score_calibrated\n",
    "        elif by == \"train\":\n",
    "            by_data = self.train_score_calibrated\n",
    "        elif by == \"all\":\n",
    "            by_data = pd.concat(\n",
    "                [self.test_score_calibrated, self.train_score_calibrated]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"by should be test, train or all\")\n",
    "        # get calibration plot raw df\n",
    "        calibration_df_list = []\n",
    "\n",
    "        for row_idx, row in self.get_score_names_df().iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            score_name = row_dict[\"score_name\"]\n",
    "\n",
    "            score_calibrated = by_data[[self.label, score_name]].dropna().copy()\n",
    "\n",
    "            c_calibration_df = get_calibration_df(\n",
    "                data=score_calibrated,  # use train to test\n",
    "                obs=self.label,\n",
    "                pred=score_name,\n",
    "                n_bins=n_bins,\n",
    "            )\n",
    "\n",
    "            # assign others\n",
    "            for k, v in row_dict.items():\n",
    "                c_calibration_df[k] = v\n",
    "\n",
    "            c_calibration_df = c_calibration_df.set_index(\n",
    "                list(row_dict.keys())\n",
    "            ).reset_index()\n",
    "\n",
    "            calibration_df_list.append(c_calibration_df)\n",
    "\n",
    "        calibration_df = pd.concat(calibration_df_list)\n",
    "\n",
    "        lim_bound = max(\n",
    "            calibration_df[\"obsRate\"].max(), calibration_df[\"predMean\"].max()\n",
    "        )\n",
    "\n",
    "        # TODO: 统一绘图风格 theme\n",
    "        p = ggplot(\n",
    "            data=calibration_df,\n",
    "            mapping=aes(x=\"predMean\", y=\"obsRate\", color=\"score_name\"),\n",
    "        )\n",
    "        if facet_fn:\n",
    "            p = p + facet_fn\n",
    "\n",
    "        p = (\n",
    "            p\n",
    "            + geom_point(alpha=0.8, size=3)\n",
    "            + geom_line(alpha=0.8)\n",
    "            # + geom_line()\n",
    "            + geom_abline(intercept=0, slope=1, linetype=\"dashed\")\n",
    "            + theme_classic(base_family=\"Calibri\", base_size=12)  # 使用Tufte主题\n",
    "            + theme(axis_line=element_line())\n",
    "            + theme(\n",
    "                figure_size=(12, 12),\n",
    "                legend_position=\"top\",\n",
    "                axis_text_x=element_text(angle=90),\n",
    "                strip_background=element_blank(),\n",
    "                axis_text=element_text(size=12),  # 调整轴文字大小\n",
    "                axis_title=element_text(size=14),  # 调整轴标题大小和样式\n",
    "                legend_title=element_text(size=14),  # 调整图例标题大小和样式\n",
    "                legend_text=element_text(),  # 调整图例文字大小\n",
    "                strip_text=element_text(size=14),  # 调整分面标签的大小和样式\n",
    "                plot_title=element_text(size=16, hjust=0.5),  # 添加图表标题并居中\n",
    "                # plot_margin = margin(10, 10, 10, 10)  # 设置图表边距\n",
    "            )\n",
    "            + scale_color_manual(values=self.color_set)\n",
    "            + labs(\n",
    "                x=\"Predicted risk\",\n",
    "                y=\"Observed risk\",\n",
    "                title=\"Calibration plot\",\n",
    "                color=\"Score\",\n",
    "            )\n",
    "            + coord_cartesian(xlim=(0, lim_bound), ylim=(0, lim_bound))\n",
    "        )\n",
    "        if return_df:\n",
    "            return p, calibration_df\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    # TODO:接口统一\n",
    "    def plot_dca(self, return_df=False, by=\"test\"):\n",
    "\n",
    "        self._check_status()\n",
    "\n",
    "        if not hasattr(self, \"train_score_calibrated\"):\n",
    "            logging.warning(\"No calibrated model fitted, run calibrate first\")\n",
    "            return\n",
    "\n",
    "        if by == \"test\":\n",
    "            by_data = self.test_score_calibrated\n",
    "        elif by == \"train\":\n",
    "            by_data = self.train_score_calibrated\n",
    "        elif by == \"all\":\n",
    "            by_data = pd.concat(\n",
    "                [self.test_score_calibrated, self.train_score_calibrated]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"by should be test, train or all\")\n",
    "        # TODO: update to new code of get_dca_df\n",
    "        test = by_data[[self.label, *self.get_score_names()]].dropna().copy()\n",
    "        event_rate = test[self.label].sum() / len(test)\n",
    "        dca_df = dca(\n",
    "            data=test,\n",
    "            outcome=self.label,\n",
    "            modelnames=self.get_score_names(),\n",
    "            thresholds=np.linspace(0, event_rate, 1000),\n",
    "        )\n",
    "        dca_df[\"st_net_benefit\"] = dca_df[\"net_benefit\"] / event_rate\n",
    "        dca_df[\"disease\"] = self.disease_name\n",
    "\n",
    "        # TODO: 统一绘图风格 theme; by another function\n",
    "        # from dca_df\n",
    "        p = (\n",
    "            ggplot(\n",
    "                data=dca_df,\n",
    "                mapping=aes(x=\"threshold\", y=\"st_net_benefit\", color=\"score_name\"),\n",
    "            )\n",
    "            # + facet_wrap(\"disease\", scales=\"free\")\n",
    "            + geom_line()\n",
    "            + ylim(0, 1)\n",
    "            + theme_classic(base_family=\"Calibri\", base_size=12)  # 使用Tufte主题\n",
    "            + theme(axis_line=element_line())\n",
    "            + theme(\n",
    "                figure_size=(12, 12),\n",
    "                legend_position=\"top\",\n",
    "                axis_text_x=element_text(angle=90),\n",
    "                strip_background=element_blank(),\n",
    "                axis_text=element_text(size=12),  # 调整轴文字大小\n",
    "                axis_title=element_text(size=14),  # 调整轴标题大小和样式\n",
    "                legend_title=element_text(size=14),  # 调整图例标题大小和样式\n",
    "                legend_text=element_text(),  # 调整图例文字大小\n",
    "                strip_text=element_text(size=14),  # 调整分面标签的大小和样式\n",
    "                plot_title=element_text(size=16, hjust=0.5),  # 添加图表标题并居中\n",
    "                # plot_margin = margin(10, 10, 10, 10)  # 设置图表边距\n",
    "            )\n",
    "            # + scale_color_manual(values=c_color_dict)\n",
    "        )\n",
    "\n",
    "        if return_df:\n",
    "            return p, dca_df\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    def plot_auc(\n",
    "        self,\n",
    "        return_df=False,\n",
    "        by=\"test\",\n",
    "    ):\n",
    "        self._check_status()\n",
    "\n",
    "        if by == \"test\":\n",
    "            by_data = self.test_score\n",
    "        elif by == \"train\":\n",
    "            by_data = self.train_score\n",
    "        elif by == \"all\":\n",
    "            by_data = pd.concat([self.test_score, self.train_score])\n",
    "        else:\n",
    "            raise ValueError(\"by should be test, train or all\")\n",
    "\n",
    "        # get auc famhistory_df_list\n",
    "        auc_df_list = []\n",
    "\n",
    "        for row_idx, row in self.get_score_names_df().iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            score_name = row_dict[\"score_name\"]\n",
    "\n",
    "            to_cal_df = by_data[[self.label, score_name]].dropna()\n",
    "            fpr, tpr, _ = roc_curve(to_cal_df[self.label], to_cal_df[score_name])\n",
    "            roc_current_df = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"fpr\": fpr_,\n",
    "                        \"tpr\": tpr_,\n",
    "                    }\n",
    "                    for fpr_, tpr_ in zip(fpr, tpr)\n",
    "                ]\n",
    "            )\n",
    "            for k, v in row_dict.items():\n",
    "                roc_current_df[k] = v\n",
    "            roc_current_df = roc_current_df.set_index(\n",
    "                list(row_dict.keys())\n",
    "            ).reset_index()\n",
    "\n",
    "            auc_df_list.append(roc_current_df)\n",
    "        auc_df = pd.concat(auc_df_list)\n",
    "\n",
    "        # TODO: 统一绘图风格 theme\n",
    "        # from auc_df\n",
    "\n",
    "        p = (\n",
    "            ggplot(\n",
    "                data=auc_df,\n",
    "                mapping=aes(x=\"fpr\", y=\"tpr\", color=\"score_name\"),\n",
    "            )\n",
    "            + geom_line()\n",
    "            + geom_abline(intercept=0, slope=1, linetype=\"dashed\")\n",
    "            + theme_classic(base_family=\"Calibri\", base_size=12)  # 使用Tufte主题\n",
    "            + theme(axis_line=element_line())\n",
    "            + theme(\n",
    "                figure_size=(12, 12),\n",
    "                legend_position=\"top\",\n",
    "                axis_text_x=element_text(angle=90),\n",
    "                strip_background=element_blank(),\n",
    "                axis_text=element_text(size=12),  # 调整轴文字大小\n",
    "                axis_title=element_text(size=14),  # 调整轴标题大小和样式\n",
    "                legend_title=element_text(size=14),  # 调整图例标题大小和样式\n",
    "                legend_text=element_text(),  # 调整图例文字大小\n",
    "                strip_text=element_text(size=14),  # 调整分面标签的大小和样式\n",
    "                plot_title=element_text(size=16, hjust=0.5),  # 添加图表标题并居中\n",
    "                # plot_margin = margin(10, 10, 10, 10)  # 设置图表边距\n",
    "            )\n",
    "            + scale_color_manual(values=self.color_set)\n",
    "            + labs(\n",
    "                x=\"1 - Specificity\",\n",
    "                y=\"Sensitivity\",\n",
    "                title=\"ROC curve\",\n",
    "                color=\"score_name\",\n",
    "            )\n",
    "        )\n",
    "        if return_df:\n",
    "            return p, auc_df\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    def plot_risk_strat(\n",
    "        self,\n",
    "        return_df=False,\n",
    "        by=\"test\",\n",
    "        facet=False,\n",
    "        k=10,\n",
    "        show_ci=True,\n",
    "        n_resample=100,\n",
    "    ):\n",
    "        self._check_status()\n",
    "\n",
    "        if by == \"test\":\n",
    "            by_data = self.test_score\n",
    "        elif by == \"train\":\n",
    "            by_data = self.train_score\n",
    "        elif by == \"all\":\n",
    "            by_data = pd.concat([self.test_score, self.train_score])\n",
    "        else:\n",
    "            raise ValueError(\"by should be test, train or all\")\n",
    "\n",
    "        # get risk_strat_df\n",
    "        risk_strat_df_list = []\n",
    "        # for score_name in self.get_score_names():\n",
    "        for row_idx, row in self.get_score_names_df().iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            score_name = row_dict[\"score_name\"]\n",
    "\n",
    "            risk_strat_df = get_risk_strat_df(\n",
    "                data=by_data.copy(),\n",
    "                y_true=self.label,\n",
    "                y_pred=score_name,\n",
    "                k=k,\n",
    "                n_resample=n_resample,\n",
    "            )\n",
    "\n",
    "            for k, v in row_dict.items():\n",
    "                risk_strat_df[k] = v\n",
    "            risk_strat_df = risk_strat_df.set_index(list(row_dict.keys())).reset_index()\n",
    "\n",
    "            # risk_strat_df[\"model\"] = score_name\n",
    "            # risk_strat_df[\"disease\"] = self.disease_name\n",
    "            risk_strat_df_list.append(risk_strat_df)\n",
    "        risk_strat_df = pd.concat(risk_strat_df_list)\n",
    "\n",
    "        # TODO: 统一绘图风格 theme\n",
    "        # from risk_strat_df\n",
    "\n",
    "        dodge_width = 0.6\n",
    "        p = ggplot(\n",
    "            data=risk_strat_df,\n",
    "            mapping=aes(x=\"y_pred_bins\", y=\"mean_true\", color=\"score_name\"),\n",
    "        )\n",
    "        if facet:\n",
    "            p = p + facet_wrap(\"model\", scales=\"free_y\")\n",
    "\n",
    "        p = p + geom_point(\n",
    "            alpha=0.8,\n",
    "            size=2,\n",
    "            position=position_dodge(width=dodge_width),\n",
    "            na_rm=True,\n",
    "        )\n",
    "\n",
    "        if show_ci:\n",
    "            p = p + geom_linerange(\n",
    "                mapping=aes(ymin=\"ci_low\", ymax=\"ci_high\"),\n",
    "                size=1,\n",
    "                alpha=0.8,\n",
    "                position=position_dodge(width=dodge_width),\n",
    "                na_rm=True,\n",
    "            )\n",
    "\n",
    "        p = (\n",
    "            p\n",
    "            + theme_classic(base_family=\"Calibri\", base_size=12)  # 使用Tufte主题\n",
    "            + theme(axis_line=element_line())\n",
    "            + theme(\n",
    "                figure_size=(10, 5),\n",
    "                legend_position=\"top\",\n",
    "                axis_text_x=element_text(angle=90),\n",
    "                strip_background=element_blank(),\n",
    "                axis_text=element_text(size=12),  # 调整轴文字大小\n",
    "                axis_title=element_text(size=14),  # 调整轴标题大小和样式\n",
    "                legend_title=element_text(size=14),  # 调整图例标题大小和样式\n",
    "                legend_text=element_text(),  # 调整图例文字大小\n",
    "                strip_text=element_text(size=14),  # 调整分面标签的大小和样式\n",
    "                plot_title=element_text(size=16, hjust=0.5),  # 添加图表标题并居中\n",
    "                # plot_margin = margin(10, 10, 10, 10)  # 设置图表边距\n",
    "            )\n",
    "            + guides(color=guide_legend(nrow=1, title=\"\"))\n",
    "            + scale_color_manual(values=self.color_set)\n",
    "            + labs(\n",
    "                x=\"Risk Decile\",  # 设置X轴标签\n",
    "                y=\"Observed Events Rate\",  # 设置Y轴标签\n",
    "                # color=\"group\",  # 设置图例标题\n",
    "                # title=\"\",  # 添加图表标题\n",
    "            )\n",
    "            # + coord_flip()\n",
    "        )\n",
    "\n",
    "        if return_df:\n",
    "            return p, risk_strat_df\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    def compare_model(self, compare_list, by=\"test\", ci=True, n_resample=100):\n",
    "        \"\"\"\n",
    "        [\n",
    "        (ref1, new1)\n",
    "        (ref2, new2)\n",
    "        ]\n",
    "        \"\"\"\n",
    "        if by == \"test\":\n",
    "            by_data = self.test_score\n",
    "        elif by == \"train\":\n",
    "            by_data = self.train_score\n",
    "        elif by == \"all\":\n",
    "            by_data = pd.concat([self.test_score, self.train_score])\n",
    "        else:\n",
    "            raise ValueError(\"by should be test, train or all\")\n",
    "\n",
    "        compare_result_list = []\n",
    "        for ref, new in compare_list:\n",
    "            to_cal_df = by_data[[self.label, ref, new]].dropna().copy()\n",
    "\n",
    "            total = {}\n",
    "\n",
    "            total[\"ref\"] = ref\n",
    "            total[\"new\"] = new\n",
    "            total[\"disease\"] = self.disease_name\n",
    "\n",
    "            # NRI\n",
    "            NRI_res = NRI(\n",
    "                to_cal_df[self.label],\n",
    "                to_cal_df[ref],\n",
    "                to_cal_df[new],\n",
    "                ci=ci,\n",
    "                n_resamples=n_resample,\n",
    "            )\n",
    "            total.update(NRI_res)\n",
    "\n",
    "            # IDI\n",
    "            IDI_res = IDI(\n",
    "                to_cal_df[self.label],\n",
    "                to_cal_df[ref],\n",
    "                to_cal_df[new],\n",
    "                ci=ci,\n",
    "                n_resamples=n_resample,\n",
    "            )\n",
    "            total.update(IDI_res)\n",
    "\n",
    "            # AUC diff\n",
    "            auc_diff_res = roc_test(\n",
    "                to_cal_df[self.label], to_cal_df[ref], to_cal_df[new]\n",
    "            )\n",
    "            total.update(auc_diff_res)\n",
    "\n",
    "            # C diff\n",
    "            if self.E and self.T:\n",
    "                c_diff_res = compareC(\n",
    "                    to_cal_df[self.T],\n",
    "                    to_cal_df[self.label],\n",
    "                    to_cal_df[ref],\n",
    "                    to_cal_df[new],\n",
    "                )\n",
    "                total.update(c_diff_res)\n",
    "\n",
    "            compare_result_list.append(total)\n",
    "        return pd.DataFrame(compare_result_list)\n",
    "\n",
    "    def plot_performance(\n",
    "        self,\n",
    "        metric=\"auc\",\n",
    "        # or\n",
    "        metrics_fn=None,\n",
    "        metrics_name=None,\n",
    "        # return\n",
    "        return_df=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        if metric is a function, then use it to calculate the metrics; works like `get_metrics_by_user`\n",
    "        \"\"\"\n",
    "        # get metrics_df\n",
    "\n",
    "        if metric == \"c_index\":\n",
    "            plt_data = (\n",
    "                self.model_table[[\"basic\", metric]]\n",
    "                .copy()\n",
    "                .droplevel(0, axis=1)\n",
    "                .reset_index()\n",
    "            )\n",
    "            y = \"c_index\"\n",
    "            y_LCI = \"c_index_LCI\"\n",
    "            y_UCI = \"c_index_UCI\"\n",
    "            y_name = \"C-index\"\n",
    "\n",
    "        elif metric == \"auc\":\n",
    "            plt_data = (\n",
    "                self.model_table[[\"basic\", metric]]\n",
    "                .copy()\n",
    "                .droplevel(0, axis=1)\n",
    "                .reset_index()\n",
    "            )\n",
    "            y = \"AUC\"\n",
    "            y_LCI = \"AUC_LCI\"\n",
    "            y_UCI = \"AUC_UCI\"\n",
    "            y_name = \"AUC\"\n",
    "\n",
    "        elif metric == \"brier_score\":\n",
    "            plt_data = (\n",
    "                self.model_table[[\"basic\", metric]]\n",
    "                .copy()\n",
    "                .droplevel(0, axis=1)\n",
    "                .reset_index()\n",
    "            )\n",
    "            y = \"brier_score\"\n",
    "            y_LCI = None\n",
    "            y_UCI = None\n",
    "\n",
    "            y_name = \"Brier Score\"\n",
    "        elif metric is None and metrics_fn is not None:\n",
    "            if metrics_name is None:\n",
    "                if isinstance(metrics_fn, partial):\n",
    "                    raise ValueError(\n",
    "                        \"metrics_name should be provided when metrics_fn is a functools.partial\"\n",
    "                    )\n",
    "                metrics_name = metrics_fn.__name__\n",
    "\n",
    "            use_calibrate = kwargs.pop(\"use_calibrate\", False)\n",
    "            plt_data = self.get_metrics_by_user(\n",
    "                metrics_fn, metrics_name=metrics_name, use_calibrate=use_calibrate\n",
    "            )\n",
    "            y = metrics_name\n",
    "            if y not in plt_data.columns:\n",
    "                raise ValueError(\n",
    "                    f\"metrics_name {metrics_name} not found in the metrics_df, there are {plt_data.columns}\"\n",
    "                )\n",
    "            if f\"{y}_LCI\" in plt_data.columns:\n",
    "                y_LCI = f\"{y}_LCI\"\n",
    "                y_UCI = f\"{y}_UCI\"\n",
    "            else:\n",
    "                y_LCI = y_UCI = None\n",
    "            y_name = metrics_name\n",
    "        else:\n",
    "            raise ValueError(\"metric should be c_index or auc\")\n",
    "        p = (\n",
    "            ggplot(\n",
    "                data=plt_data,\n",
    "                mapping=aes(x=\"score_name\", y=y, color=\"score_name\"),\n",
    "            )\n",
    "            # + facet_wrap(\"disease\", scales=\"free_y\")\n",
    "            + geom_point(alpha=0.8, size=3, position=position_dodge(width=0.5))\n",
    "        )\n",
    "        if y_LCI is not None:\n",
    "            p = p + geom_linerange(\n",
    "                mapping=aes(ymin=y_LCI, ymax=y_UCI),\n",
    "                size=1,\n",
    "                alpha=0.8,\n",
    "                position=position_dodge(width=0.5),\n",
    "            )\n",
    "        p = (\n",
    "            p\n",
    "            + theme_classic(base_family=\"Calibri\", base_size=12)  # 使用Tufte主题\n",
    "            + theme(axis_line=element_line())\n",
    "            + theme(\n",
    "                figure_size=(12, 6),\n",
    "                legend_position=\"none\",\n",
    "                axis_text_x=element_text(angle=90),\n",
    "                strip_background=element_blank(),\n",
    "                axis_text=element_text(size=12),  # 调整轴文字大小\n",
    "                axis_title=element_text(size=14),  # 调整轴标题大小和样式\n",
    "                legend_title=element_text(size=14),  # 调整图例标题大小和样式\n",
    "                legend_text=element_text(),  # 调整图例文字大小\n",
    "                strip_text=element_text(size=14),  # 调整分面标签的大小和样式\n",
    "                plot_title=element_text(size=16, hjust=0.5),  # 添加图表标题并居中\n",
    "                # plot_margin = margin(10, 10, 10, 10)  # 设置图表边距\n",
    "            )\n",
    "            # + guides(color=False)\n",
    "            # + scale_color_manual(values=colorset)\n",
    "            + scale_color_manual(values=self.color_set)\n",
    "            + labs(\n",
    "                x=\"Method\",  # 设置X轴标签\n",
    "                # y=\"C-index\",  # 设置Y轴标签\n",
    "                y=y_name,\n",
    "                # color=\"Method\",  # 设置图例标题\n",
    "                title=\"Comparison of Methods\",  # 添加图表标题\n",
    "            )\n",
    "            # + coord_flip()\n",
    "        )\n",
    "        if return_df:\n",
    "            return p, plt_data\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "\n",
    "# save_fig("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load test dataset\n",
    "\n",
    "# import sklearn\n",
    "\n",
    "# # return dataframe\n",
    "# X_df = sklearn.datasets.load_breast_cancer(as_frame=True)[\"data\"]\n",
    "# y_df = sklearn.datasets.load_breast_cancer(as_frame=True)[\"target\"]\n",
    "\n",
    "# df = X_df.join(y_df).reset_index(drop=False, names=[\"eid\"])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "total_data = pd.read_feather(\n",
    "    \"/mnt/d/桌面/work/AAA_lifeStyle/V3/output/raw_data/score_df.feather\"\n",
    ")\n",
    "lifeStyle_cols = [\n",
    "    \"PhysicalActivity\",\n",
    "    \"HealthyDiet\",\n",
    "    \"Alcohol_consumption\",\n",
    "    \"Sedentary_behaviour\",\n",
    "    \"BMI\",\n",
    "    \"SleepPattern\",\n",
    "    \"SmokingStatus\",\n",
    "]\n",
    "to_dummpy_cols = []\n",
    "for col in total_data.columns:\n",
    "    if total_data[col].dtype == \"category\":\n",
    "        to_dummpy_cols.append(col)\n",
    "\n",
    "to_dummpy_cols\n",
    "# dummpy used the first category as the base category\n",
    "# so we need to drop the first category to avoid multicollinearity\n",
    "tmp = pd.DataFrame()\n",
    "dummpy_cols = []\n",
    "# for col in [\n",
    "#     \"Sex(M)\",\n",
    "#     \"Ethnicity\",\n",
    "#     # \"Educational_attainment\",\n",
    "#     # \"Family_income\",\n",
    "#     # \"Empolyment\",\n",
    "#     \"antihypertensives\",\n",
    "#     \"antihyperglycemic\",\n",
    "#     \"lipid_lowering\",\n",
    "#     \"History_of_Hypertension\",\n",
    "#     \"History_of_Diabetes\",\n",
    "#     \"familiy_history_heart_disease\",\n",
    "# ]:\n",
    "lifeStyleDummyCols = []\n",
    "\n",
    "for col in to_dummpy_cols:\n",
    "    # dummy variable\n",
    "    dummy = pd.get_dummies(\n",
    "        total_data[col], prefix=col, drop_first=True\n",
    "    )  # default version is True, False for test at 20250305\n",
    "    for i in dummy.columns:\n",
    "        dummy[i] = dummy[i].astype(\"int\")\n",
    "    dummpy_cols.extend(dummy.columns)\n",
    "    if col in lifeStyle_cols:\n",
    "        lifeStyleDummyCols.extend(dummy.columns)\n",
    "    for col in dummy.columns:\n",
    "        if col in total_data.columns:\n",
    "            dummy.drop(col, axis=1, inplace=True)\n",
    "    tmp = pd.concat([tmp, dummy], axis=1)\n",
    "total_data = pd.concat([total_data, tmp], axis=1)\n",
    "total_data\n",
    "for col in dummpy_cols:\n",
    "    total_data[col] = total_data[col].astype(float)\n",
    "total_data[\"PRS\"] = (total_data[\"PRS\"] - total_data[\"PRS\"].mean()) / total_data[\n",
    "    \"PRS\"\n",
    "].std()\n",
    "\n",
    "RF = [\n",
    "    \"Age_at_recruitment\",\n",
    "    \"Sex_M\",\n",
    "    \"HbA1C\",\n",
    "    \"LDL_cholesterol\",\n",
    "    \"HDL_cholesterol\",\n",
    "    \"Cholesterol\",\n",
    "    \"Triglycerides\",\n",
    "    \"SBP\",\n",
    "    \"eGFR\",\n",
    "    \"antihypertensives\",\n",
    "    \"antihyperglycemic\",\n",
    "    \"lipid_lowering\",\n",
    "]\n",
    "lifeStyle_cols\n",
    "features = lifeStyle_cols + RF + [\"PRS\"]\n",
    "\n",
    "cat_cols = [\n",
    "    *lifeStyle_cols,\n",
    "    \"Sex_M\",\n",
    "    \"antihypertensives\",\n",
    "    \"antihyperglycemic\",\n",
    "    \"lipid_lowering\",\n",
    "    \"incident\",\n",
    "]\n",
    "qt_cols = [\n",
    "    \"Age_at_recruitment\",\n",
    "    \"HbA1C\",\n",
    "    \"LDL_cholesterol\",\n",
    "    \"HDL_cholesterol\",\n",
    "    \"Cholesterol\",\n",
    "    \"Triglycerides\",\n",
    "    \"SBP\",\n",
    "    \"eGFR\",\n",
    "    \"PRS\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex_M_M',\n",
       " 'Genetic_sex_M',\n",
       " 'Ethnicity_Black',\n",
       " 'Ethnicity_Chinese',\n",
       " 'Ethnicity_Missing',\n",
       " 'Ethnicity_Mixed',\n",
       " 'Ethnicity_White',\n",
       " 'Educational_attainment_College or university degree',\n",
       " 'Educational_attainment_Missing',\n",
       " 'Educational_attainment_NVQ/HND/HNC/Other professional',\n",
       " 'Educational_attainment_O levels/GCSEs/CSEs',\n",
       " 'Family_income_>31000',\n",
       " 'Family_income_Missing',\n",
       " 'Empolyment_Missing',\n",
       " 'Empolyment_Others',\n",
       " 'Empolyment_Retired',\n",
       " 'Empolyment_Unemployed',\n",
       " 'antihypertensives_1',\n",
       " 'antihyperglycemic_1',\n",
       " 'lipid_lowering_1',\n",
       " 'History_of_Hypertension_1',\n",
       " 'History_of_Diabetes_1',\n",
       " 'familiy_history_heart_disease_1',\n",
       " 'genotype_array_2',\n",
       " 'PhysicalActivity_600-3000 MET-min/week',\n",
       " 'PhysicalActivity_< 600 MET-min/week',\n",
       " 'HealthyDiet_Unfavourable',\n",
       " 'Alcohol_consumption_Unfavourable',\n",
       " 'Sedentary_behaviour_Intermediate',\n",
       " 'Sedentary_behaviour_Unfavourable',\n",
       " 'BMI_underweight',\n",
       " 'BMI_overweight',\n",
       " 'BMI_obese',\n",
       " 'SleepPattern_Intermediate',\n",
       " 'SleepPattern_Unfavourable',\n",
       " 'SmokingStatus_Previous',\n",
       " 'SmokingStatus_Current',\n",
       " 'AAA_AAA',\n",
       " 'GeneticsRiskGroup_Intermediate',\n",
       " 'GeneticsRiskGroup_High',\n",
       " 'lifeStyleGroup_Intermediate',\n",
       " 'lifeStyleGroup_Unfavorable',\n",
       " 'TotalHealthyFactors_1',\n",
       " 'TotalHealthyFactors_2',\n",
       " 'TotalHealthyFactors_3',\n",
       " 'TotalHealthyFactors_4',\n",
       " 'TotalHealthyFactors_5',\n",
       " 'TotalHealthyFactors_6',\n",
       " 'TotalHealthyFactors_7']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummpy_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict = {}\n",
    "# models = [\"TabNet\", \"ResNet\", \"MLP\", \"ExcelFormer\", \"FTTransformer\"]\n",
    "# for modelName in models:\n",
    "#     model = fit_tabular_dl(\n",
    "#         train=train_df,\n",
    "#         label=label,\n",
    "#         xvar=features,\n",
    "#         col_to_stype=col_to_stype,\n",
    "#         epochs=5,\n",
    "#         num_trials=1,\n",
    "#         model_type=\"TabNet\",\n",
    "#     )\n",
    "#     total_data[modelName] = get_predict_from_tb_dl_with_df(\n",
    "#         total_data,\n",
    "#         model,\n",
    "#         features,\n",
    "#         batch_size=2048,\n",
    "#         device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "#     )\n",
    "#     model_dict[modelName] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = [\n",
    "    \"Age_at_recruitment\",\n",
    "    \"Sex_M_M\",\n",
    "    \"HbA1C\",\n",
    "    \"LDL_cholesterol\",\n",
    "    \"HDL_cholesterol\",\n",
    "    \"Cholesterol\",\n",
    "    \"Triglycerides\",\n",
    "    \"SBP\",\n",
    "    \"eGFR\",\n",
    "    \"antihypertensives_1\",\n",
    "    \"antihyperglycemic_1\",\n",
    "    \"lipid_lowering_1\",\n",
    "]\n",
    "\n",
    "features = lifeStyleDummyCols + RF + [\"PRS\"]\n",
    "\n",
    "cat_cols = [\n",
    "    *lifeStyleDummyCols,\n",
    "    \"Sex_M_M\",\n",
    "    \"antihypertensives_1\",\n",
    "    \"antihyperglycemic_1\",\n",
    "    \"lipid_lowering_1\",\n",
    "]\n",
    "qt_cols = [\n",
    "    \"Age_at_recruitment\",\n",
    "    \"HbA1C\",\n",
    "    \"LDL_cholesterol\",\n",
    "    \"HDL_cholesterol\",\n",
    "    \"Cholesterol\",\n",
    "    \"Triglycerides\",\n",
    "    \"SBP\",\n",
    "    \"eGFR\",\n",
    "    \"PRS\",\n",
    "]\n",
    "\n",
    "label = \"incident\"\n",
    "# cat_cols += [label]\n",
    "qt_cols += [label]\n",
    "\n",
    "col_to_stype = {\n",
    "    **{k: stype.numerical for k in qt_cols},\n",
    "    **{k: stype.categorical for k in cat_cols},\n",
    "}\n",
    "# model = fit_tabular_dl(\n",
    "#     train=train_df,\n",
    "#     label=label,\n",
    "#     xvar=features,\n",
    "#     col_to_stype=col_to_stype,\n",
    "#     epochs=5,\n",
    "#     num_trials=1,\n",
    "#     model_type=\"TabNet\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PhysicalActivity_600-3000 MET-min/week',\n",
       " 'PhysicalActivity_< 600 MET-min/week',\n",
       " 'HealthyDiet_Unfavourable',\n",
       " 'Alcohol_consumption_Unfavourable',\n",
       " 'Sedentary_behaviour_Intermediate',\n",
       " 'Sedentary_behaviour_Unfavourable',\n",
       " 'BMI_underweight',\n",
       " 'BMI_overweight',\n",
       " 'BMI_obese',\n",
       " 'SleepPattern_Intermediate',\n",
       " 'SleepPattern_Unfavourable',\n",
       " 'SmokingStatus_Previous',\n",
       " 'SmokingStatus_Current',\n",
       " 'Age_at_recruitment',\n",
       " 'Sex_M_M',\n",
       " 'HbA1C',\n",
       " 'LDL_cholesterol',\n",
       " 'HDL_cholesterol',\n",
       " 'Cholesterol',\n",
       " 'Triglycerides',\n",
       " 'SBP',\n",
       " 'eGFR',\n",
       " 'antihypertensives_1',\n",
       " 'antihyperglycemic_1',\n",
       " 'lipid_lowering_1',\n",
       " 'PRS']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">param</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>xvar</th>\n",
       "      <th>model</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>[PhysicalActivity_600-3000 MET-min/week, Physi...</td>\n",
       "      <td>&lt;function fit_best_model_v2 at 0x7fee73c75ea0&gt;</td>\n",
       "      <td>{'cv': 10, 'engine': 'sklearn'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabPFN</th>\n",
       "      <td>[PhysicalActivity_600-3000 MET-min/week, Physi...</td>\n",
       "      <td>&lt;function fit_tabpfn at 0x7fee73c76710&gt;</td>\n",
       "      <td>{'device': 'cuda'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet</th>\n",
       "      <td>[PhysicalActivity_600-3000 MET-min/week, Physi...</td>\n",
       "      <td>&lt;function fit_tabular_dl at 0x7fee73c1aef0&gt;</td>\n",
       "      <td>{'col_to_stype': {'Age_at_recruitment': numeri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    param  \\\n",
       "                                                     xvar   \n",
       "model                                                       \n",
       "Lasso   [PhysicalActivity_600-3000 MET-min/week, Physi...   \n",
       "TabPFN  [PhysicalActivity_600-3000 MET-min/week, Physi...   \n",
       "TabNet  [PhysicalActivity_600-3000 MET-min/week, Physi...   \n",
       "\n",
       "                                                        \\\n",
       "                                                 model   \n",
       "model                                                    \n",
       "Lasso   <function fit_best_model_v2 at 0x7fee73c75ea0>   \n",
       "TabPFN         <function fit_tabpfn at 0x7fee73c76710>   \n",
       "TabNet     <function fit_tabular_dl at 0x7fee73c1aef0>   \n",
       "\n",
       "                                                           \n",
       "                                                   config  \n",
       "model                                                      \n",
       "Lasso                     {'cv': 10, 'engine': 'sklearn'}  \n",
       "TabPFN                                 {'device': 'cuda'}  \n",
       "TabNet  {'col_to_stype': {'Age_at_recruitment': numeri...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination_dict = OrderedDict(\n",
    "    {\n",
    "        (\"Lasso\"): {\n",
    "            \"xvar\": features,\n",
    "            \"model\": fit_best_model_v2,\n",
    "            \"config\": {\"cv\": 10, \"engine\": \"sklearn\"},\n",
    "        },\n",
    "        # (\"xgboost\"): {\n",
    "        #     \"xvar\": features,\n",
    "        #     \"model\": fit_xgboost,\n",
    "        # },\n",
    "        # (\"lightGBM\"): {\n",
    "        #     \"xvar\": features,\n",
    "        #     \"model\": fit_lightgbm,\n",
    "        # },\n",
    "        (\"TabPFN\"): {\n",
    "            \"xvar\": features,\n",
    "            \"model\": fit_tabpfn,\n",
    "            \"config\": {\"device\": \"cuda\"},\n",
    "        },\n",
    "        (\"TabNet\"): {\n",
    "            \"xvar\": features,\n",
    "            \"model\": fit_tabular_dl,\n",
    "            \"config\": dict(\n",
    "                col_to_stype=col_to_stype,\n",
    "                model_type=\"TabNet\",\n",
    "                epochs=10,\n",
    "                num_trials=1,\n",
    "                device=\"cuda\",\n",
    "            ),\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "model_table = config_dict_to_df(combination_dict, (\"model\"))\n",
    "model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = total_data[total_data[\"Type\"] == \"Train\"]\n",
    "test_df = total_data[total_data[\"Type\"] == \"Test\"]\n",
    "\n",
    "train_eid = train_df[[\"eid\"]]\n",
    "test_eid = test_df[[\"eid\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145332</th>\n",
       "      <td>1000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145333</th>\n",
       "      <td>1000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145334</th>\n",
       "      <td>1000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145335</th>\n",
       "      <td>1000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145336</th>\n",
       "      <td>1000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435991</th>\n",
       "      <td>6024071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435992</th>\n",
       "      <td>6024086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435993</th>\n",
       "      <td>6024103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435994</th>\n",
       "      <td>6024110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435995</th>\n",
       "      <td>6024122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290664 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            eid\n",
       "145332  1000017\n",
       "145333  1000025\n",
       "145334  1000038\n",
       "145335  1000056\n",
       "145336  1000061\n",
       "...         ...\n",
       "435991  6024071\n",
       "435992  6024086\n",
       "435993  6024103\n",
       "435994  6024110\n",
       "435995  6024122\n",
       "\n",
       "[290664 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data['RF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Random split data with test_size: 0.30\n",
      "INFO:root:Loading data with train cases 594.0 and test cases 252.0 of AAA, while 110 columns\n",
      "INFO:root:Drop NA by incident and None and None in train and test and left 101732 and 43600 with train cases 594.0 and test cases 252.0\n"
     ]
    }
   ],
   "source": [
    "targetModel = DiseaseScoreModel_V2(\n",
    "    disease_df=train_df,\n",
    "    model_table=model_table,\n",
    "    label=label,\n",
    "    disease_name=\"AAA\",\n",
    "    test_size=0.3,\n",
    "    # train_eid=train_eid.eid,\n",
    "    # test_eid=test_eid.eid,\n",
    "    other_keep_cols=[col for col in train_df.columns if col not in [label, \"eid\"]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_var: incident, X_var: ['PhysicalActivity_600-3000 MET-min/week', 'PhysicalActivity_< 600 MET-min/week', 'HealthyDiet_Unfavourable', 'Alcohol_consumption_Unfavourable', 'Sedentary_behaviour_Intermediate', 'Sedentary_behaviour_Unfavourable', 'BMI_underweight', 'BMI_overweight', 'BMI_obese', 'SleepPattern_Intermediate', 'SleepPattern_Unfavourable', 'SmokingStatus_Previous', 'SmokingStatus_Current', 'Age_at_recruitment', 'Sex_M_M', 'HbA1C', 'LDL_cholesterol', 'HDL_cholesterol', 'Cholesterol', 'Triglycerides', 'SBP', 'eGFR', 'antihypertensives_1', 'antihyperglycemic_1', 'lipid_lowering_1', 'PRS']\n",
      "train shape: (101732, 26),  test shape is (43600, 26)\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.635e-02, tolerance: 5.368e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.725e+02, tolerance: 5.210e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+02, tolerance: 5.240e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.553e-02, tolerance: 5.506e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.749e+01, tolerance: 5.418e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.064e+02, tolerance: 5.250e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.100e-01, tolerance: 5.220e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.452e+01, tolerance: 5.210e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.770e-01, tolerance: 5.418e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.837e-01, tolerance: 5.250e-02\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: Lasso\tBest parameters: {'model__alpha': 0.00046415888336127773}, with auc: 0.853448320055776\n",
      "train_nums: 101732, will downsample to 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "/home/xutingfeng/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "[I 2025-03-24 21:42:45,932] A new study created in memory with name: no-name-ae3beda1-e014-47a1-b9dc-4ab12e37e9a6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter search via Optuna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 49/49 [00:01<00:00, 30.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0087, Val: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 49/49 [00:01<00:00, 37.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 49/49 [00:01<00:00, 36.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 49/49 [00:01<00:00, 37.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 49/49 [00:01<00:00, 37.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0056, Val: 0.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 49/49 [00:01<00:00, 37.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 49/49 [00:01<00:00, 35.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0056, Val: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 49/49 [00:01<00:00, 37.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0056, Val: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 49/49 [00:01<00:00, 35.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0056, Val: 0.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 49/49 [00:01<00:00, 37.03it/s]\n",
      "[I 2025-03-24 21:43:01,653] Trial 0 finished with value: 0.07483989000320435 and parameters: {'split_attn_channels': 128, 'split_feat_channels': 256, 'gamma': 1.0, 'num_layers': 8, 'batch_size': 2048, 'base_lr': 0.01, 'gamma_rate': 0.9}. Best is trial 0 with value: 0.07483989000320435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0056, Val: 0.0750\n",
      "Best val: 0.0748\n",
      "Hyper-parameter search done. Found the best config.\n",
      "config {'batch_size': 2048, 'gamma_rate': 0.9, 'base_lr': 0.01} and model config {'split_attn_channels': 128, 'split_feat_channels': 256, 'gamma': 1.0, 'num_layers': 8}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 49/49 [00:01<00:00, 36.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0066, Val: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 49/49 [00:01<00:00, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 49/49 [00:01<00:00, 37.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 49/49 [00:01<00:00, 37.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 49/49 [00:01<00:00, 36.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 49/49 [00:01<00:00, 36.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 49/49 [00:01<00:00, 37.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0056, Val: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 49/49 [00:01<00:00, 37.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0056, Val: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 49/49 [00:01<00:00, 37.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 49/49 [00:01<00:00, 34.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0056, Val: 0.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 74.30it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 58.40it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 65.44it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 82.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">param</th>\n",
       "      <th colspan=\"15\" halign=\"left\">auc</th>\n",
       "      <th>status</th>\n",
       "      <th>model</th>\n",
       "      <th>basic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>xvar</th>\n",
       "      <th>model</th>\n",
       "      <th>config</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUC_UCI</th>\n",
       "      <th>AUC_LCI</th>\n",
       "      <th>ACC</th>\n",
       "      <th>ACC_UCI</th>\n",
       "      <th>ACC_LCI</th>\n",
       "      <th>Macro_F1</th>\n",
       "      <th>...</th>\n",
       "      <th>Specificity_LCI</th>\n",
       "      <th>APR</th>\n",
       "      <th>APR_UCI</th>\n",
       "      <th>APR_LCI</th>\n",
       "      <th>N</th>\n",
       "      <th>N_case</th>\n",
       "      <th>N_control</th>\n",
       "      <th>fitted</th>\n",
       "      <th>model</th>\n",
       "      <th>score_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>[PhysicalActivity_600-3000 MET-min/week, Physi...</td>\n",
       "      <td>&lt;function fit_best_model_v2 at 0x7fee73c75ea0&gt;</td>\n",
       "      <td>{'cv': 10, 'engine': 'sklearn'}</td>\n",
       "      <td>0.853448</td>\n",
       "      <td>0.875718</td>\n",
       "      <td>0.833671</td>\n",
       "      <td>0.827179</td>\n",
       "      <td>0.830428</td>\n",
       "      <td>0.823677</td>\n",
       "      <td>0.476129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824875</td>\n",
       "      <td>0.058221</td>\n",
       "      <td>0.074501</td>\n",
       "      <td>0.033701</td>\n",
       "      <td>43600.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>43348.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(StandardScaler(), Lasso(alpha=0.0004641588833...</td>\n",
       "      <td>Lasso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabPFN</th>\n",
       "      <td>[PhysicalActivity_600-3000 MET-min/week, Physi...</td>\n",
       "      <td>&lt;function fit_tabpfn at 0x7fee73c76710&gt;</td>\n",
       "      <td>{'device': 'cuda'}</td>\n",
       "      <td>0.871995</td>\n",
       "      <td>0.891780</td>\n",
       "      <td>0.847205</td>\n",
       "      <td>0.802959</td>\n",
       "      <td>0.806227</td>\n",
       "      <td>0.798544</td>\n",
       "      <td>0.467426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799346</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0.092134</td>\n",
       "      <td>0.042931</td>\n",
       "      <td>43600.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>43348.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TabPFNClassifier(device='cuda:0', ignore_pretr...</td>\n",
       "      <td>TabPFN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet</th>\n",
       "      <td>[PhysicalActivity_600-3000 MET-min/week, Physi...</td>\n",
       "      <td>&lt;function fit_tabular_dl at 0x7fee73c1aef0&gt;</td>\n",
       "      <td>{'col_to_stype': {'Age_at_recruitment': numeri...</td>\n",
       "      <td>0.830596</td>\n",
       "      <td>0.858762</td>\n",
       "      <td>0.807121</td>\n",
       "      <td>0.816147</td>\n",
       "      <td>0.819712</td>\n",
       "      <td>0.812840</td>\n",
       "      <td>0.470981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813634</td>\n",
       "      <td>0.054683</td>\n",
       "      <td>0.068912</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>43600.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>43348.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TabNet(\\n  (feature_encoder): StypeWiseFeature...</td>\n",
       "      <td>TabNet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    param  \\\n",
       "                                                     xvar   \n",
       "model                                                       \n",
       "Lasso   [PhysicalActivity_600-3000 MET-min/week, Physi...   \n",
       "TabPFN  [PhysicalActivity_600-3000 MET-min/week, Physi...   \n",
       "TabNet  [PhysicalActivity_600-3000 MET-min/week, Physi...   \n",
       "\n",
       "                                                        \\\n",
       "                                                 model   \n",
       "model                                                    \n",
       "Lasso   <function fit_best_model_v2 at 0x7fee73c75ea0>   \n",
       "TabPFN         <function fit_tabpfn at 0x7fee73c76710>   \n",
       "TabNet     <function fit_tabular_dl at 0x7fee73c1aef0>   \n",
       "\n",
       "                                                                auc            \\\n",
       "                                                   config       AUC   AUC_UCI   \n",
       "model                                                                           \n",
       "Lasso                     {'cv': 10, 'engine': 'sklearn'}  0.853448  0.875718   \n",
       "TabPFN                                 {'device': 'cuda'}  0.871995  0.891780   \n",
       "TabNet  {'col_to_stype': {'Age_at_recruitment': numeri...  0.830596  0.858762   \n",
       "\n",
       "                                                          ...                  \\\n",
       "         AUC_LCI       ACC   ACC_UCI   ACC_LCI  Macro_F1  ... Specificity_LCI   \n",
       "model                                                     ...                   \n",
       "Lasso   0.833671  0.827179  0.830428  0.823677  0.476129  ...        0.824875   \n",
       "TabPFN  0.847205  0.802959  0.806227  0.798544  0.467426  ...        0.799346   \n",
       "TabNet  0.807121  0.816147  0.819712  0.812840  0.470981  ...        0.813634   \n",
       "\n",
       "                                                               status  \\\n",
       "             APR   APR_UCI   APR_LCI        N N_case N_control fitted   \n",
       "model                                                                   \n",
       "Lasso   0.058221  0.074501  0.033701  43600.0  252.0   43348.0    1.0   \n",
       "TabPFN  0.068531  0.092134  0.042931  43600.0  252.0   43348.0    1.0   \n",
       "TabNet  0.054683  0.068912  0.030409  43600.0  252.0   43348.0    1.0   \n",
       "\n",
       "                                                    model      basic  \n",
       "                                                    model score_name  \n",
       "model                                                                 \n",
       "Lasso   (StandardScaler(), Lasso(alpha=0.0004641588833...      Lasso  \n",
       "TabPFN  TabPFNClassifier(device='cuda:0', ignore_pretr...     TabPFN  \n",
       "TabNet  TabNet(\\n  (feature_encoder): StypeWiseFeature...     TabNet  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetModel.update_model()\n",
    "targetModel.model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">param</th>\n",
       "      <th colspan=\"15\" halign=\"left\">auc</th>\n",
       "      <th>status</th>\n",
       "      <th>model</th>\n",
       "      <th>basic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>xvar</th>\n",
       "      <th>model</th>\n",
       "      <th>config</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUC_UCI</th>\n",
       "      <th>AUC_LCI</th>\n",
       "      <th>ACC</th>\n",
       "      <th>ACC_UCI</th>\n",
       "      <th>ACC_LCI</th>\n",
       "      <th>Macro_F1</th>\n",
       "      <th>...</th>\n",
       "      <th>Specificity_LCI</th>\n",
       "      <th>APR</th>\n",
       "      <th>APR_UCI</th>\n",
       "      <th>APR_LCI</th>\n",
       "      <th>N</th>\n",
       "      <th>N_case</th>\n",
       "      <th>N_control</th>\n",
       "      <th>fitted</th>\n",
       "      <th>model</th>\n",
       "      <th>score_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>[PhysicalActivity_600-3000 MET-min/week, Physi...</td>\n",
       "      <td>&lt;function fit_best_model_v2 at 0x7f388cd6e200&gt;</td>\n",
       "      <td>{'cv': 10, 'engine': 'sklearn'}</td>\n",
       "      <td>0.849107</td>\n",
       "      <td>0.870700</td>\n",
       "      <td>0.831604</td>\n",
       "      <td>0.777683</td>\n",
       "      <td>0.781799</td>\n",
       "      <td>0.774652</td>\n",
       "      <td>0.457335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773605</td>\n",
       "      <td>0.060989</td>\n",
       "      <td>0.075374</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>43600.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>43334.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(StandardScaler(), Lasso(alpha=0.0004641588833...</td>\n",
       "      <td>Lasso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet</th>\n",
       "      <td>[PhysicalActivity_600-3000 MET-min/week, Physi...</td>\n",
       "      <td>&lt;function fit_tabular_dl at 0x7f388cd27250&gt;</td>\n",
       "      <td>{'col_to_stype': {'Age_at_recruitment': numeri...</td>\n",
       "      <td>0.834170</td>\n",
       "      <td>0.863948</td>\n",
       "      <td>0.808652</td>\n",
       "      <td>0.778991</td>\n",
       "      <td>0.782861</td>\n",
       "      <td>0.774917</td>\n",
       "      <td>0.458147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774006</td>\n",
       "      <td>0.060678</td>\n",
       "      <td>0.078666</td>\n",
       "      <td>0.038387</td>\n",
       "      <td>43600.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>43334.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TabNet(\\n  (feature_encoder): StypeWiseFeature...</td>\n",
       "      <td>TabNet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    param  \\\n",
       "                                                     xvar   \n",
       "model                                                       \n",
       "Lasso   [PhysicalActivity_600-3000 MET-min/week, Physi...   \n",
       "TabNet  [PhysicalActivity_600-3000 MET-min/week, Physi...   \n",
       "\n",
       "                                                        \\\n",
       "                                                 model   \n",
       "model                                                    \n",
       "Lasso   <function fit_best_model_v2 at 0x7f388cd6e200>   \n",
       "TabNet     <function fit_tabular_dl at 0x7f388cd27250>   \n",
       "\n",
       "                                                                auc            \\\n",
       "                                                   config       AUC   AUC_UCI   \n",
       "model                                                                           \n",
       "Lasso                     {'cv': 10, 'engine': 'sklearn'}  0.849107  0.870700   \n",
       "TabNet  {'col_to_stype': {'Age_at_recruitment': numeri...  0.834170  0.863948   \n",
       "\n",
       "                                                          ...                  \\\n",
       "         AUC_LCI       ACC   ACC_UCI   ACC_LCI  Macro_F1  ... Specificity_LCI   \n",
       "model                                                     ...                   \n",
       "Lasso   0.831604  0.777683  0.781799  0.774652  0.457335  ...        0.773605   \n",
       "TabNet  0.808652  0.778991  0.782861  0.774917  0.458147  ...        0.774006   \n",
       "\n",
       "                                                               status  \\\n",
       "             APR   APR_UCI   APR_LCI        N N_case N_control fitted   \n",
       "model                                                                   \n",
       "Lasso   0.060989  0.075374  0.038956  43600.0  266.0   43334.0    1.0   \n",
       "TabNet  0.060678  0.078666  0.038387  43600.0  266.0   43334.0    1.0   \n",
       "\n",
       "                                                    model      basic  \n",
       "                                                    model score_name  \n",
       "model                                                                 \n",
       "Lasso   (StandardScaler(), Lasso(alpha=0.0004641588833...      Lasso  \n",
       "TabNet  TabNet(\\n  (feature_encoder): StypeWiseFeature...     TabNet  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetModel.model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Linear, Module, ModuleList\n",
    "\n",
    "from torch_frame import TensorFrame, stype\n",
    "from torch_frame.nn.conv import TabTransformerConv\n",
    "from torch_frame.nn.encoder import (\n",
    "    EmbeddingEncoder,\n",
    "    LinearEncoder,\n",
    "    StypeWiseFeatureEncoder,\n",
    ")\n",
    "\n",
    "\n",
    "class ExampleTransformer(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        out_channels,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        col_stats,\n",
    "        col_names_dict,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = StypeWiseFeatureEncoder(\n",
    "            out_channels=channels,\n",
    "            col_stats=col_stats,\n",
    "            col_names_dict=col_names_dict,\n",
    "            stype_encoder_dict={\n",
    "                stype.categorical: EmbeddingEncoder(),\n",
    "                stype.numerical: LinearEncoder(),\n",
    "            },\n",
    "        )\n",
    "        self.convs = ModuleList(\n",
    "            [\n",
    "                TabTransformerConv(\n",
    "                    channels=channels,\n",
    "                    num_heads=num_heads,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.decoder = Linear(channels, out_channels)\n",
    "\n",
    "    def forward(self, tf: TensorFrame) -> Tensor:\n",
    "        x, _ = self.encoder(tf)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        out = self.decoder(x.mean(dim=1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Control\n",
       "1         Control\n",
       "2         Control\n",
       "3         Control\n",
       "4         Control\n",
       "           ...   \n",
       "435991    Control\n",
       "435992    Control\n",
       "435993        AAA\n",
       "435994    Control\n",
       "435995    Control\n",
       "Name: AAA, Length: 435996, dtype: category\n",
       "Categories (2, object): ['Control', 'AAA']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data[\"AAA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_frame.data import Dataset\n",
    "from torch_frame import TensorFrame, stype\n",
    "\n",
    "# train_df = df.query(\"eid in @train_eid.eid\")\n",
    "# test_df = df.query(\"eid in @test_eid.eid\")\n",
    "# train_df\n",
    "\n",
    "# split train, val and tes\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=42)\n",
    "train_df = total_data[total_data[\"Type\"] == \"Train\"]\n",
    "test_df = total_data[total_data[\"Type\"] == \"Test\"]\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.groupby(\"AAA\").sample(n=3000, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import math\n",
    "# import os\n",
    "# import os.path as osp\n",
    "# import time\n",
    "# from typing import Any, Optional\n",
    "\n",
    "# import numpy as np\n",
    "# import optuna\n",
    "# import torch\n",
    "# from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, Module, MSELoss\n",
    "# from torch.optim.lr_scheduler import ExponentialLR\n",
    "# from torchmetrics import AUROC, Accuracy, MeanSquaredError\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from torch_frame import stype\n",
    "# from torch_frame.data import DataLoader\n",
    "# from torch_frame.datasets import DataFrameBenchmark\n",
    "# from torch_frame.gbdt import CatBoost, LightGBM, XGBoost\n",
    "# from torch_frame.nn.encoder import EmbeddingEncoder, LinearBucketEncoder\n",
    "# from torch_frame.nn.models import (\n",
    "#     MLP,\n",
    "#     ExcelFormer,\n",
    "#     FTTransformer,\n",
    "#     ResNet,\n",
    "#     TabNet,\n",
    "#     TabTransformer,\n",
    "#     Trompt,\n",
    "# )\n",
    "# from torch_frame.typing import TaskType\n",
    "\n",
    "# # class Args:\n",
    "# #     def __init__(self, **kwargs):\n",
    "# #         # 使用字典存储键值对\n",
    "# #         self.__dict__.update(kwargs)\n",
    "\n",
    "# # # 创建 Args 对象\n",
    "\n",
    "# # args = Args(\n",
    "# #     model_type = \"TabNet\", # TabNet, TabTransformer, ExcelFormer, MLP, ResNet, Trompt, LightGBM, CatBoost, XGBoost\n",
    "# #     task_type = \"binary_classification\", # binary_classification, multiclass_classification, regression\n",
    "# #     scale = \"small\",\n",
    "# #     idx = 0,\n",
    "# # )\n",
    "\n",
    "# model_type = \"TabNet\"\n",
    "# train_dataset = TrainDataSet\n",
    "# test_dataset = TestDataSet\n",
    "# val_dataset = ValDataSet\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# TRAIN_CONFIG_KEYS = [\"batch_size\", \"gamma_rate\", \"base_lr\"]\n",
    "# task_type = \"binary_classification\"  # binary_classification, multiclass_classification, regression\n",
    "# sacle = \"small\"  # small, medium, large\n",
    "# epochs = 50\n",
    "# num_trials = 20  # Number of Optuna-based hyper-parameter tuning.\n",
    "# num_repeats = 5  # Number of repeated training and eval on the best config\n",
    "# seed = 42\n",
    "# result_path = \"./test\"\n",
    "\n",
    "\n",
    "# def fit_tabular_dl(\n",
    "#     model_type=\"TabNet\",\n",
    "#     train_dataset=TrainDataSet,\n",
    "#     test_dataset=TestDataSet,\n",
    "#     val_dataset=ValDataSet,\n",
    "#     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "#     TRAIN_CONFIG_KEYS=[\"batch_size\", \"gamma_rate\", \"base_lr\"],\n",
    "#     # task_type = \"binary_classification\",  # binary_classification, multiclass_classification, regression\n",
    "#     # sacle = \"small\" , # small, medium, large\n",
    "#     epochs=50,\n",
    "#     num_trials=20,  # Number of Optuna-based hyper-parameter tuning.\n",
    "#     num_repeats=5,  # Number of repeated training and eval on the best config\n",
    "#     seed=42,\n",
    "#     result_path=\"./test\",\n",
    "# ):\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "\n",
    "#     train_tensor_frame = train_dataset.tensor_frame\n",
    "#     val_tensor_frame = val_dataset.tensor_frame\n",
    "#     test_tensor_frame = test_dataset.tensor_frame\n",
    "\n",
    "#     if train_dataset.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "#         out_channels = 1\n",
    "#         loss_fun = BCEWithLogitsLoss()\n",
    "#         metric_computer = AUROC(task=\"binary\").to(device)\n",
    "#         higher_is_better = True\n",
    "#     elif train_dataset.task_type == TaskType.MULTICLASS_CLASSIFICATION:\n",
    "#         out_channels = train_dataset.num_classes\n",
    "#         loss_fun = CrossEntropyLoss()\n",
    "#         metric_computer = Accuracy(\n",
    "#             task=\"multiclass\", num_classes=train_dataset.num_classes\n",
    "#         ).to(device)\n",
    "#         higher_is_better = True\n",
    "#     elif train_dataset.task_type == TaskType.REGRESSION:\n",
    "#         out_channels = 1\n",
    "#         loss_fun = MSELoss()\n",
    "#         metric_computer = MeanSquaredError(squared=False).to(device)\n",
    "#         higher_is_better = False\n",
    "\n",
    "#     # To be set for each model\n",
    "#     model_cls = None\n",
    "#     col_stats = None\n",
    "\n",
    "#     # Set up model specific search space\n",
    "#     if model_type == \"TabNet\":\n",
    "#         model_search_space = {\n",
    "#             \"split_attn_channels\": [64, 128, 256],\n",
    "#             \"split_feat_channels\": [64, 128, 256],\n",
    "#             \"gamma\": [1.0, 1.2, 1.5],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [2048, 4096],\n",
    "#             \"base_lr\": [0.001, 0.01],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = TabNet\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"FTTransformer\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 256],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = FTTransformer\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"FTTransformerBucket\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 256],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = FTTransformer\n",
    "\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"ResNet\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 256],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = ResNet\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"MLP\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 256],\n",
    "#             \"num_layers\": [1, 2, 4],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = MLP\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"TabTransformer\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [16, 32, 64, 128],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#             \"num_heads\": [4, 8],\n",
    "#             \"encoder_pad_size\": [2, 4],\n",
    "#             \"attn_dropout\": [0, 0.2],\n",
    "#             \"ffn_dropout\": [0, 0.2],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [128, 256],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = TabTransformer\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"Trompt\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 192],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#             \"num_prompts\": [64, 128, 192],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [128, 256],\n",
    "#             \"base_lr\": [0.01, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         if train_tensor_frame.num_cols > 20:\n",
    "#             # Reducing the model size to avoid GPU OOM\n",
    "#             model_search_space[\"channels\"] = [64, 128]\n",
    "#             model_search_space[\"num_prompts\"] = [64, 128]\n",
    "#         elif train_tensor_frame.num_cols > 50:\n",
    "#             model_search_space[\"channels\"] = [64]\n",
    "#             model_search_space[\"num_prompts\"] = [64]\n",
    "#         model_cls = Trompt\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"ExcelFormer\":\n",
    "#         from torch_frame.transforms import (\n",
    "#             CatToNumTransform,\n",
    "#             MutualInformationSort,\n",
    "#         )\n",
    "\n",
    "#         categorical_transform = CatToNumTransform()\n",
    "#         categorical_transform.fit(train_dataset.tensor_frame, train_dataset.col_stats)\n",
    "#         train_tensor_frame = categorical_transform(train_tensor_frame)\n",
    "#         # val_tensor_frame = categorical_transform(val_tensor_frame)\n",
    "#         # test_tensor_frame = categorical_transform(test_tensor_frame)\n",
    "#         col_stats = categorical_transform.transformed_stats\n",
    "\n",
    "#         mutual_info_sort = MutualInformationSort(task_type=train_dataset.task_type)\n",
    "#         mutual_info_sort.fit(train_tensor_frame, col_stats)\n",
    "#         train_tensor_frame = mutual_info_sort(train_tensor_frame)\n",
    "#         # val_tensor_frame = mutual_info_sort(val_tensor_frame)\n",
    "#         # test_tensor_frame = mutual_info_sort(test_tensor_frame)\n",
    "\n",
    "#         model_search_space = {\n",
    "#             \"in_channels\": [128, 256],\n",
    "#             \"num_heads\": [8, 16, 32],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#             \"diam_dropout\": [0, 0.2],\n",
    "#             \"residual_dropout\": [0, 0.2],\n",
    "#             \"aium_dropout\": [0, 0.2],\n",
    "#             \"mixup\": [None, \"feature\", \"hidden\"],\n",
    "#             \"beta\": [0.5],\n",
    "#             \"num_cols\": [train_tensor_frame.num_cols],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = ExcelFormer\n",
    "\n",
    "#     assert model_cls is not None\n",
    "#     assert col_stats is not None\n",
    "#     assert set(train_search_space.keys()) == set(TRAIN_CONFIG_KEYS)\n",
    "#     col_names_dict = train_tensor_frame.col_names_dict\n",
    "\n",
    "#     def train(\n",
    "#         model: Module,\n",
    "#         loader: DataLoader,\n",
    "#         optimizer: torch.optim.Optimizer,\n",
    "#         epoch: int,\n",
    "#     ) -> float:\n",
    "#         model.train()\n",
    "#         loss_accum = total_count = 0\n",
    "\n",
    "#         for tf in tqdm(loader, desc=f\"Epoch: {epoch}\"):\n",
    "#             tf = tf.to(device)\n",
    "#             y = tf.y\n",
    "#             if isinstance(model, ExcelFormer):\n",
    "#                 # Train with FEAT-MIX or HIDDEN-MIX\n",
    "#                 pred, y = model(tf, mixup_encoded=True)\n",
    "#             elif isinstance(model, Trompt):\n",
    "#                 # Trompt uses the layer-wise loss\n",
    "#                 pred = model(tf)\n",
    "#                 num_layers = pred.size(1)\n",
    "#                 # [batch_size * num_layers, num_classes]\n",
    "#                 pred = pred.view(-1, out_channels)\n",
    "#                 y = tf.y.repeat_interleave(num_layers)\n",
    "#             else:\n",
    "#                 pred = model(tf)\n",
    "\n",
    "#             if pred.size(1) == 1:\n",
    "#                 pred = pred.view(\n",
    "#                     -1,\n",
    "#                 )\n",
    "#             if train_dataset.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "#                 y = y.to(torch.float)\n",
    "#             loss = loss_fun(pred, y)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             loss_accum += float(loss) * len(tf.y)\n",
    "#             print(tf.y)\n",
    "#             total_count += len(tf.y)\n",
    "#             optimizer.step()\n",
    "#         return loss_accum / total_count\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def test(\n",
    "#         model: Module,\n",
    "#         loader: DataLoader,\n",
    "#     ) -> float:\n",
    "#         model.eval()\n",
    "#         metric_computer.reset()\n",
    "#         for tf in loader:\n",
    "#             tf = tf.to(device)\n",
    "#             pred = model(tf)\n",
    "#             if isinstance(model, Trompt):\n",
    "#                 pred = pred.mean(dim=1)\n",
    "#             if train_dataset.task_type == TaskType.MULTICLASS_CLASSIFICATION:\n",
    "#                 pred = pred.argmax(dim=-1)\n",
    "#             elif train_dataset.task_type == TaskType.REGRESSION:\n",
    "#                 pred = pred.view(\n",
    "#                     -1,\n",
    "#                 )\n",
    "#             metric_computer.update(pred, tf.y)\n",
    "#         return metric_computer.compute().item()\n",
    "\n",
    "#     def train_and_eval_with_cfg(\n",
    "#         model_cfg: dict[str, Any],\n",
    "#         train_cfg: dict[str, Any],\n",
    "#         trial: Optional[optuna.trial.Trial] = None,\n",
    "#     ) -> tuple[float, float]:\n",
    "#         # Use model_cfg to set up training procedure\n",
    "#         if model_type == \"FTTransformerBucket\":\n",
    "#             # Use LinearBucketEncoder instead\n",
    "#             stype_encoder_dict = {\n",
    "#                 stype.categorical: EmbeddingEncoder(),\n",
    "#                 stype.numerical: LinearBucketEncoder(),\n",
    "#             }\n",
    "#             model_cfg[\"stype_encoder_dict\"] = stype_encoder_dict\n",
    "#         model = model_cls(\n",
    "#             **model_cfg,\n",
    "#             out_channels=out_channels,\n",
    "#             col_stats=col_stats,\n",
    "#             col_names_dict=col_names_dict,\n",
    "#         ).to(device)\n",
    "#         model.reset_parameters()\n",
    "#         # Use train_cfg to set up training procedure\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=train_cfg[\"base_lr\"])\n",
    "#         lr_scheduler = ExponentialLR(optimizer, gamma=train_cfg[\"gamma_rate\"])\n",
    "#         train_loader = DataLoader(\n",
    "#             train_tensor_frame,\n",
    "#             batch_size=train_cfg[\"batch_size\"],\n",
    "#             shuffle=True,\n",
    "#             drop_last=True,\n",
    "#         )\n",
    "#         val_loader = DataLoader(val_tensor_frame, batch_size=train_cfg[\"batch_size\"])\n",
    "#         test_loader = DataLoader(test_tensor_frame, batch_size=train_cfg[\"batch_size\"])\n",
    "\n",
    "#         if higher_is_better:\n",
    "#             best_val_metric = 0\n",
    "#         else:\n",
    "#             best_val_metric = math.inf\n",
    "\n",
    "#         for epoch in range(1, epochs + 1):\n",
    "#             train_loss = train(model, train_loader, optimizer, epoch)\n",
    "#             val_metric = test(model, val_loader)\n",
    "\n",
    "#             if higher_is_better:\n",
    "#                 if val_metric > best_val_metric:\n",
    "#                     best_val_metric = val_metric\n",
    "#                     best_test_metric = test(model, test_loader)\n",
    "#             else:\n",
    "#                 if val_metric < best_val_metric:\n",
    "#                     best_val_metric = val_metric\n",
    "#                     best_test_metric = test(model, test_loader)\n",
    "#             lr_scheduler.step()\n",
    "#             print(f\"Train Loss: {train_loss:.4f}, Val: {val_metric:.4f}\")\n",
    "\n",
    "#             if trial is not None:\n",
    "#                 trial.report(val_metric, epoch)\n",
    "#                 if trial.should_prune():\n",
    "#                     raise optuna.TrialPruned()\n",
    "\n",
    "#         print(f\"Best val: {best_val_metric:.4f}, Best test: {best_test_metric:.4f}\")\n",
    "#         return best_val_metric, best_test_metric\n",
    "\n",
    "#     def objective(trial: optuna.trial.Trial) -> float:\n",
    "#         model_cfg = {}\n",
    "#         for name, search_list in model_search_space.items():\n",
    "#             model_cfg[name] = trial.suggest_categorical(name, search_list)\n",
    "#         train_cfg = {}\n",
    "#         for name, search_list in train_search_space.items():\n",
    "#             train_cfg[name] = trial.suggest_categorical(name, search_list)\n",
    "\n",
    "#         best_val_metric, _ = train_and_eval_with_cfg(\n",
    "#             model_cfg=model_cfg, train_cfg=train_cfg, trial=trial\n",
    "#         )\n",
    "#         return best_val_metric\n",
    "\n",
    "\n",
    "#     # Hyper-parameter optimization with Optuna\n",
    "#     print(\"Hyper-parameter search via Optuna\")\n",
    "#     start_time = time.time()\n",
    "#     study = optuna.create_study(\n",
    "#         pruner=optuna.pruners.MedianPruner(),\n",
    "#         direction=\"maximize\" if higher_is_better else \"minimize\",\n",
    "#     )\n",
    "#     study.optimize(objective, n_trials=num_trials)\n",
    "#     end_time = time.time()\n",
    "#     search_time = end_time - start_time\n",
    "#     print(\"Hyper-parameter search done. Found the best config.\")\n",
    "#     params = study.best_params\n",
    "#     best_train_cfg = {}\n",
    "#     for train_cfg_key in TRAIN_CONFIG_KEYS:\n",
    "#         best_train_cfg[train_cfg_key] = params.pop(train_cfg_key)\n",
    "#     best_model_cfg = params\n",
    "\n",
    "#     print(\n",
    "#         f\"Repeat experiments {num_repeats} times with the best train \"\n",
    "#         f\"config {best_train_cfg} and model config {best_model_cfg}.\"\n",
    "#     )\n",
    "#     start_time = time.time()\n",
    "#     best_val_metrics = []\n",
    "#     best_test_metrics = []\n",
    "#     for _ in range(num_repeats):\n",
    "#         best_val_metric, best_test_metric = train_and_eval_with_cfg(\n",
    "#             best_model_cfg, best_train_cfg\n",
    "#         )\n",
    "#         best_val_metrics.append(best_val_metric)\n",
    "#         best_test_metrics.append(best_test_metric)\n",
    "#     end_time = time.time()\n",
    "#     final_model_time = (end_time - start_time) / num_repeats\n",
    "#     best_val_metrics = np.array(best_val_metrics)\n",
    "#     best_test_metrics = np.array(best_test_metrics)\n",
    "\n",
    "#     result_dict = {\n",
    "#         # 'args': __dict__,\n",
    "#         \"best_val_metrics\": best_val_metrics,\n",
    "#         \"best_test_metrics\": best_test_metrics,\n",
    "#         \"best_val_metric\": best_val_metrics.mean(),\n",
    "#         \"best_test_metric\": best_test_metrics.mean(),\n",
    "#         \"best_train_cfg\": best_train_cfg,\n",
    "#         \"best_model_cfg\": best_model_cfg,\n",
    "#         \"search_time\": search_time,\n",
    "#         \"final_model_time\": final_model_time,\n",
    "#         \"total_time\": search_time + final_model_time,\n",
    "#     }\n",
    "#     print(result_dict)\n",
    "#     # Save results\n",
    "#     if result_path != \"\":\n",
    "#         os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "#         torch.save(result_dict, result_path)\n",
    "#     return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import math\n",
    "# import os\n",
    "# import os.path as osp\n",
    "# import time\n",
    "# from typing import Any, Optional\n",
    "\n",
    "# import numpy as np\n",
    "# import optuna\n",
    "# import torch\n",
    "# from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, Module, MSELoss\n",
    "# from torch.optim.lr_scheduler import ExponentialLR\n",
    "# from torchmetrics import AUROC, Accuracy, MeanSquaredError\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from torch_frame import stype\n",
    "# from torch_frame.data import DataLoader\n",
    "# from torch_frame.datasets import DataFrameBenchmark\n",
    "# from torch_frame.gbdt import CatBoost, LightGBM, XGBoost\n",
    "# from torch_frame.nn.encoder import EmbeddingEncoder, LinearBucketEncoder\n",
    "# from torch_frame.nn.models import (\n",
    "#     MLP,\n",
    "#     ExcelFormer,\n",
    "#     FTTransformer,\n",
    "#     ResNet,\n",
    "#     TabNet,\n",
    "#     TabTransformer,\n",
    "#     Trompt,\n",
    "# )\n",
    "# from torch_frame.typing import TaskType\n",
    "# from copy import deepcopy\n",
    "\n",
    "\n",
    "# def fit_tabular_dl(\n",
    "#     xvar,\n",
    "#     train,\n",
    "#     label,\n",
    "#     test=None,  # Test is validation\n",
    "#     cv=10,\n",
    "#     verbose=1,\n",
    "#     y_type=\"bt\",\n",
    "#     need_scale=False,\n",
    "#     test_size=0.2,\n",
    "#     col_to_stype=None,  # cat_cols and qt_cols shouldb e stype.numerical\n",
    "#     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "#     epochs=50,\n",
    "#     num_trials=10,\n",
    "#     # num_repeats=5,\n",
    "#     seed=42,\n",
    "#     model_type=\"TabNet\",  # TabNet, TabTransformer, ExcelFormer, MLP, ResNet, Trompt, LightGBM, CatBoost, XGBoost\n",
    "#     **kwargs,\n",
    "# ):\n",
    "#     if col_to_stype is None:\n",
    "#         col_to_stype = {}\n",
    "#         for col in xvar:\n",
    "#             if train[col].dtype in [\"object\", \"category\"]:\n",
    "#                 if train[col].nunique() <= 2:\n",
    "#                     col_to_stype[col] = stype.categorical\n",
    "#                 else:\n",
    "#                     col_to_stype[col] = stype.multicategorical\n",
    "#             else:\n",
    "#                 col_to_stype[col] = stype.numerical\n",
    "\n",
    "#     train_df = train[xvar + [label]].copy().dropna().reset_index(drop=True)\n",
    "\n",
    "#     if test is not None:\n",
    "#         test_df = test[xvar + [label]].copy().dropna()\n",
    "#     else:\n",
    "#         print(\"No test data provided, will split the train data into train and test\")\n",
    "#         train_df, test_df = train_test_split(\n",
    "#             train_df, test_size=test_size, random_state=42\n",
    "#         )\n",
    "\n",
    "#     train_dataset = Dataset(\n",
    "#         df=train_df,\n",
    "#         col_to_stype=col_to_stype,\n",
    "#         target_col=label,\n",
    "#     )\n",
    "#     train_dataset.materialize()\n",
    "#     val_dataset = Dataset(\n",
    "#         df=test_df,\n",
    "#         col_to_stype=col_to_stype,\n",
    "#         target_col=label,\n",
    "#     )\n",
    "#     val_dataset.materialize()\n",
    "\n",
    "#     # model_type = \"MLP\"\n",
    "#     # train_dataset = TrainDataSet\n",
    "#     # test_dataset = TestDataSet\n",
    "#     # val_dataset = ValDataSet\n",
    "#     # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     TRAIN_CONFIG_KEYS = [\"batch_size\", \"gamma_rate\", \"base_lr\"]\n",
    "#     # task_type = \"binary_classification\"  # binary_classification, multiclass_classification, regression\n",
    "#     # sacle = \"small\"  # small, medium, large\n",
    "#     # epochs = 10\n",
    "#     # num_trials = 3  # Number of Optuna-based hyper-parameter tuning.\n",
    "#     # num_repeats = 5  # Number of repeated training and eval on the best config\n",
    "#     # seed = 42\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "\n",
    "#     train_tensor_frame = train_dataset.tensor_frame\n",
    "#     val_tensor_frame = val_dataset.tensor_frame\n",
    "#     # test_tensor_frame = test_dataset.tensor_frame\n",
    "\n",
    "#     if train_dataset.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "#         out_channels = 1\n",
    "#         loss_fun = BCEWithLogitsLoss()\n",
    "#         metric_computer = AUROC(task=\"binary\").to(device)\n",
    "#         higher_is_better = True\n",
    "#     elif train_dataset.task_type == TaskType.MULTICLASS_CLASSIFICATION:\n",
    "#         out_channels = train_dataset.num_classes\n",
    "#         loss_fun = CrossEntropyLoss()\n",
    "#         metric_computer = Accuracy(\n",
    "#             task=\"multiclass\", num_classes=train_dataset.num_classes\n",
    "#         ).to(device)\n",
    "#         higher_is_better = True\n",
    "#     elif train_dataset.task_type == TaskType.REGRESSION:\n",
    "#         out_channels = 1\n",
    "#         loss_fun = MSELoss()\n",
    "#         metric_computer = MeanSquaredError(squared=False).to(device)\n",
    "#         higher_is_better = False\n",
    "\n",
    "#     # To be set for each model\n",
    "#     model_cls = None\n",
    "#     col_stats = None\n",
    "\n",
    "#     # Set up model specific search space\n",
    "#     if model_type == \"TabNet\":\n",
    "#         model_search_space = {\n",
    "#             \"split_attn_channels\": [64, 128, 256],\n",
    "#             \"split_feat_channels\": [64, 128, 256],\n",
    "#             \"gamma\": [1.0, 1.2, 1.5],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [\n",
    "#                 2048,\n",
    "#                 4096,\n",
    "#             ],  # Note if you have a small data, you may want to reduce it, also low gpu memory\n",
    "#             # \"batch_size\": [128, 256],\n",
    "#             \"base_lr\": [0.001, 0.01],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = TabNet\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"FTTransformer\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 256],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = FTTransformer\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"FTTransformerBucket\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 256],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = FTTransformer\n",
    "\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"ResNet\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 256],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = ResNet\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"MLP\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 256],\n",
    "#             \"num_layers\": [1, 2, 4],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = MLP\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"TabTransformer\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [16, 32, 64, 128],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#             \"num_heads\": [4, 8],\n",
    "#             \"encoder_pad_size\": [2, 4],\n",
    "#             \"attn_dropout\": [0, 0.2],\n",
    "#             \"ffn_dropout\": [0, 0.2],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [128, 256],\n",
    "#             \"base_lr\": [0.0001, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = TabTransformer\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"Trompt\":\n",
    "#         model_search_space = {\n",
    "#             \"channels\": [64, 128, 192],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#             \"num_prompts\": [64, 128, 192],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [128, 256],\n",
    "#             \"base_lr\": [0.01, 0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         if train_tensor_frame.num_cols > 20:\n",
    "#             # Reducing the model size to avoid GPU OOM\n",
    "#             model_search_space[\"channels\"] = [64, 128]\n",
    "#             model_search_space[\"num_prompts\"] = [64, 128]\n",
    "#         elif train_tensor_frame.num_cols > 50:\n",
    "#             model_search_space[\"channels\"] = [64]\n",
    "#             model_search_space[\"num_prompts\"] = [64]\n",
    "#         model_cls = Trompt\n",
    "#         col_stats = train_dataset.col_stats\n",
    "#     elif model_type == \"ExcelFormer\":\n",
    "#         from torch_frame.transforms import (\n",
    "#             CatToNumTransform,\n",
    "#             MutualInformationSort,\n",
    "#         )\n",
    "\n",
    "#         categorical_transform = CatToNumTransform()\n",
    "#         categorical_transform.fit(train_dataset.tensor_frame, train_dataset.col_stats)\n",
    "#         train_tensor_frame = categorical_transform(train_tensor_frame)\n",
    "#         # val_tensor_frame = categorical_transform(val_tensor_frame)\n",
    "#         # test_tensor_frame = categorical_transform(test_tensor_frame)\n",
    "#         col_stats = categorical_transform.transformed_stats\n",
    "\n",
    "#         mutual_info_sort = MutualInformationSort(task_type=train_dataset.task_type)\n",
    "#         mutual_info_sort.fit(train_tensor_frame, col_stats)\n",
    "#         train_tensor_frame = mutual_info_sort(train_tensor_frame)\n",
    "#         # val_tensor_frame = mutual_info_sort(val_tensor_frame)\n",
    "#         # test_tensor_frame = mutual_info_sort(test_tensor_frame)\n",
    "\n",
    "#         model_search_space = {\n",
    "#             \"in_channels\": [128, 256],\n",
    "#             \"num_heads\": [8, 16, 32],\n",
    "#             \"num_layers\": [4, 6, 8],\n",
    "#             \"diam_dropout\": [0, 0.2],\n",
    "#             \"residual_dropout\": [0, 0.2],\n",
    "#             \"aium_dropout\": [0, 0.2],\n",
    "#             \"mixup\": [None, \"feature\", \"hidden\"],\n",
    "#             \"beta\": [0.5],\n",
    "#             \"num_cols\": [train_tensor_frame.num_cols],\n",
    "#         }\n",
    "#         train_search_space = {\n",
    "#             \"batch_size\": [256, 512],\n",
    "#             \"base_lr\": [0.001],\n",
    "#             \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "#         }\n",
    "#         model_cls = ExcelFormer\n",
    "\n",
    "#     assert model_cls is not None\n",
    "#     assert col_stats is not None\n",
    "#     assert set(train_search_space.keys()) == set(TRAIN_CONFIG_KEYS)\n",
    "#     col_names_dict = train_tensor_frame.col_names_dict\n",
    "\n",
    "#     def train(\n",
    "#         model: Module,\n",
    "#         loader: DataLoader,\n",
    "#         optimizer: torch.optim.Optimizer,\n",
    "#         epoch: int,\n",
    "#     ) -> float:\n",
    "#         model.train()\n",
    "#         loss_accum = total_count = 0\n",
    "\n",
    "#         for tf in tqdm(loader, desc=f\"Epoch: {epoch}\"):\n",
    "#             tf = tf.to(device)\n",
    "#             y = tf.y\n",
    "#             if isinstance(model, ExcelFormer):\n",
    "#                 # Train with FEAT-MIX or HIDDEN-MIX\n",
    "#                 pred, y = model(tf, mixup_encoded=True)\n",
    "#             elif isinstance(model, Trompt):\n",
    "#                 # Trompt uses the layer-wise loss\n",
    "#                 pred = model(tf)\n",
    "#                 num_layers = pred.size(1)\n",
    "#                 # [batch_size * num_layers, num_classes]\n",
    "#                 pred = pred.view(-1, out_channels)\n",
    "#                 y = tf.y.repeat_interleave(num_layers)\n",
    "#             else:\n",
    "#                 pred = model(tf)\n",
    "\n",
    "#             if pred.size(1) == 1:\n",
    "#                 pred = pred.view(\n",
    "#                     -1,\n",
    "#                 )\n",
    "#             if train_dataset.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "#                 y = y.to(torch.float)\n",
    "#             loss = loss_fun(pred, y)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             loss_accum += float(loss) * len(tf.y)\n",
    "#             total_count += len(tf.y)\n",
    "#             optimizer.step()\n",
    "#         return loss_accum / total_count\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def test(\n",
    "#         model: Module,\n",
    "#         loader: DataLoader,\n",
    "#     ) -> float:\n",
    "#         model.eval()\n",
    "#         metric_computer.reset()\n",
    "#         for tf in loader:\n",
    "#             tf = tf.to(device)\n",
    "#             pred = model(tf)\n",
    "#             if isinstance(model, Trompt):\n",
    "#                 pred = pred.mean(dim=1)\n",
    "#             if train_dataset.task_type == TaskType.MULTICLASS_CLASSIFICATION:\n",
    "#                 pred = pred.argmax(dim=-1)\n",
    "#             elif train_dataset.task_type == TaskType.REGRESSION:\n",
    "#                 pred = pred.view(\n",
    "#                     -1,\n",
    "#                 )\n",
    "#             metric_computer.update(pred, tf.y)\n",
    "#         return metric_computer.compute().item()\n",
    "\n",
    "#     def train_and_eval_with_cfg(\n",
    "#         model_cfg: dict[str, Any],\n",
    "#         train_cfg: dict[str, Any],\n",
    "#         trial: Optional[optuna.trial.Trial] = None,\n",
    "#     ) -> tuple[float, float]:\n",
    "#         # Use model_cfg to set up training procedure\n",
    "#         if model_type == \"FTTransformerBucket\":\n",
    "#             # Use LinearBucketEncoder instead\n",
    "#             stype_encoder_dict = {\n",
    "#                 stype.categorical: EmbeddingEncoder(),\n",
    "#                 stype.numerical: LinearBucketEncoder(),\n",
    "#             }\n",
    "#             model_cfg[\"stype_encoder_dict\"] = stype_encoder_dict\n",
    "#         model = model_cls(\n",
    "#             **model_cfg,\n",
    "#             out_channels=out_channels,\n",
    "#             col_stats=col_stats,\n",
    "#             col_names_dict=col_names_dict,\n",
    "#         ).to(device)\n",
    "#         model.reset_parameters()\n",
    "#         # Use train_cfg to set up training procedure\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=train_cfg[\"base_lr\"])\n",
    "#         lr_scheduler = ExponentialLR(optimizer, gamma=train_cfg[\"gamma_rate\"])\n",
    "#         train_loader = DataLoader(\n",
    "#             train_tensor_frame,\n",
    "#             batch_size=train_cfg[\"batch_size\"],\n",
    "#             shuffle=True,\n",
    "#             drop_last=True,\n",
    "#         )\n",
    "#         val_loader = DataLoader(val_tensor_frame, batch_size=train_cfg[\"batch_size\"])\n",
    "#         # test_loader = DataLoader(test_tensor_frame, batch_size=train_cfg[\"batch_size\"])\n",
    "\n",
    "#         if higher_is_better:\n",
    "#             best_val_metric = 0\n",
    "#         else:\n",
    "#             best_val_metric = math.inf\n",
    "\n",
    "#         for epoch in range(1, epochs + 1):\n",
    "#             train_loss = train(model, train_loader, optimizer, epoch)\n",
    "#             val_metric = test(model, val_loader)\n",
    "\n",
    "#             if higher_is_better:\n",
    "#                 if val_metric > best_val_metric:\n",
    "#                     best_val_metric = val_metric\n",
    "#                     # best_test_metric = test(model, test_loader)\n",
    "#             else:\n",
    "#                 if val_metric < best_val_metric:\n",
    "#                     best_val_metric = val_metric\n",
    "#                     # best_test_metric = test(model, test_loader)\n",
    "#             lr_scheduler.step()\n",
    "#             print(f\"Train Loss: {train_loss:.4f}, Val: {val_metric:.4f}\")\n",
    "\n",
    "#             if trial is not None:\n",
    "#                 trial.report(val_metric, epoch)\n",
    "#                 if trial.should_prune():\n",
    "#                     raise optuna.TrialPruned()\n",
    "\n",
    "#         # print(f\"Best val: {best_val_metric:.4f}, Best test: {best_test_metric:.4f}\")\n",
    "#         # return best_val_metric, best_test_metric\n",
    "#         print(f\"Best val: {best_val_metric:.4f}\")\n",
    "#         return best_val_metric, 0\n",
    "\n",
    "#     def objective(trial: optuna.trial.Trial) -> float:\n",
    "#         model_cfg = {}\n",
    "#         for name, search_list in model_search_space.items():\n",
    "#             model_cfg[name] = trial.suggest_categorical(name, search_list)\n",
    "#         train_cfg = {}\n",
    "#         for name, search_list in train_search_space.items():\n",
    "#             train_cfg[name] = trial.suggest_categorical(name, search_list)\n",
    "\n",
    "#         best_val_metric, _ = train_and_eval_with_cfg(\n",
    "#             model_cfg=model_cfg, train_cfg=train_cfg, trial=trial\n",
    "#         )\n",
    "#         return best_val_metric\n",
    "\n",
    "#     # Hyper-parameter optimization with Optuna\n",
    "#     print(\"Hyper-parameter search via Optuna\")\n",
    "#     start_time = time.time()\n",
    "#     study = optuna.create_study(\n",
    "#         pruner=optuna.pruners.MedianPruner(),\n",
    "#         direction=\"maximize\" if higher_is_better else \"minimize\",\n",
    "#     )\n",
    "#     study.optimize(objective, n_trials=num_trials)\n",
    "#     end_time = time.time()\n",
    "#     search_time = end_time - start_time\n",
    "#     print(\"Hyper-parameter search done. Found the best config.\")\n",
    "#     params = study.best_params\n",
    "#     best_train_cfg = {}\n",
    "#     for train_cfg_key in TRAIN_CONFIG_KEYS:\n",
    "#         best_train_cfg[train_cfg_key] = params.pop(train_cfg_key)\n",
    "#     best_model_cfg = params\n",
    "\n",
    "#     print(\n",
    "#         # f\"Repeat experiments {num_repeats} times with the best train \"\n",
    "#         f\"config {best_train_cfg} and model config {best_model_cfg}.\"\n",
    "#     )\n",
    "\n",
    "#     # retrain model\n",
    "#     if model_type == \"FTTransformerBucket\":\n",
    "#         # Use LinearBucketEncoder instead\n",
    "#         stype_encoder_dict = {\n",
    "#             stype.categorical: EmbeddingEncoder(),\n",
    "#             stype.numerical: LinearBucketEncoder(),\n",
    "#         }\n",
    "#         best_model_cfg[\"stype_encoder_dict\"] = stype_encoder_dict\n",
    "\n",
    "#     model = model_cls(\n",
    "#         **best_model_cfg,\n",
    "#         out_channels=out_channels,\n",
    "#         col_stats=col_stats,\n",
    "#         col_names_dict=col_names_dict,\n",
    "#     ).to(device)\n",
    "#     model.reset_parameters()\n",
    "#     # Use train_cfg to set up training procedure\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=best_train_cfg[\"base_lr\"])\n",
    "#     lr_scheduler = ExponentialLR(optimizer, gamma=best_train_cfg[\"gamma_rate\"])\n",
    "#     train_loader = DataLoader(\n",
    "#         train_tensor_frame,\n",
    "#         batch_size=best_train_cfg[\"batch_size\"],\n",
    "#         shuffle=True,\n",
    "#         drop_last=True,\n",
    "#     )\n",
    "#     val_loader = DataLoader(val_tensor_frame, batch_size=best_train_cfg[\"batch_size\"])\n",
    "#     # test_loader = DataLoader(test_tensor_frame, batch_size=best_train_cfg[\"batch_size\"])\n",
    "\n",
    "#     if higher_is_better:\n",
    "#         best_val_metric = 0\n",
    "#     else:\n",
    "#         best_val_metric = math.inf\n",
    "\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         train_loss = train(model, train_loader, optimizer, epoch)\n",
    "#         val_metric = test(model, val_loader)\n",
    "\n",
    "#         if higher_is_better:\n",
    "#             if val_metric > best_val_metric:\n",
    "#                 best_val_metric = val_metric\n",
    "#                 # best_test_metric = test(model, test_loader)\n",
    "#         else:\n",
    "#             if val_metric < best_val_metric:\n",
    "#                 best_val_metric = val_metric\n",
    "#                 # best_test_metric = test(model, test_loader)\n",
    "#         lr_scheduler.step()\n",
    "#         print(f\"Train Loss: {train_loss:.4f}, Val: {val_metric:.4f}\")\n",
    "\n",
    "#     # get_predict\n",
    "\n",
    "#     result_dict = {\n",
    "#         # 'args': __dict__,\n",
    "#         \"model\": model,\n",
    "#         \"best_val_metric\": best_val_metric,\n",
    "#         # \"best_test_metric\": best_test_metric,\n",
    "#         \"best_train_cfg\": best_train_cfg,\n",
    "#         \"best_model_cfg\": best_model_cfg,\n",
    "#         \"search_time\": search_time,\n",
    "#     }\n",
    "#     col_to_stype_save = {k: v for k, v in deepcopy(col_to_stype).items() if k != label}\n",
    "#     model.col_to_stype = col_to_stype_save\n",
    "#     return model\n",
    "\n",
    "# def get_predict_from_tb_dl_with_df(\n",
    "#     df,\n",
    "#     model,\n",
    "#     x_var,\n",
    "#     batch_size=2048,\n",
    "#     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "# ):\n",
    "#     from torch_frame.data import Dataset\n",
    "#     from torch_frame.data import DataLoader\n",
    "\n",
    "#     # dataset\n",
    "#     df = df[x_var]\n",
    "#     col_to_stype = model.col_to_stype\n",
    "#     DataSetUsed = Dataset(\n",
    "#         df=df,\n",
    "#         col_to_stype=col_to_stype,\n",
    "#         # target_col=target_col,\n",
    "#     )\n",
    "#     DataSetUsed.materialize()\n",
    "#     DataSetUsedLoader = DataLoader(DataSetUsed, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     #\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         pred = []\n",
    "#         for batch in tqdm(DataSetUsedLoader):\n",
    "#             batch = batch.to(device)\n",
    "#             pred.append(model(batch).cpu().numpy())\n",
    "#         pred = np.concatenate(pred)\n",
    "#     return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test data provided, will split the train data into train and test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 19:58:15,684] A new study created in memory with name: no-name-1c30b1ab-fcdd-4d02-9d31-b7a8d9d1087e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter search via Optuna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 21.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0063, Val: 0.0731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 27.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 27.78it/s]\n",
      "[I 2025-03-24 19:58:19,868] Trial 0 finished with value: 0.07204770296812057 and parameters: {'split_attn_channels': 256, 'split_feat_channels': 128, 'gamma': 1.2, 'num_layers': 6, 'batch_size': 4096, 'base_lr': 0.001, 'gamma_rate': 0.95}. Best is trial 0 with value: 0.07204770296812057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0721\n",
      "Best val: 0.0720\n",
      "Hyper-parameter search done. Found the best config.\n",
      "config {'batch_size': 4096, 'gamma_rate': 0.95, 'base_lr': 0.001} and model config {'split_attn_channels': 256, 'split_feat_channels': 128, 'gamma': 1.2, 'num_layers': 6}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 27.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0061, Val: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 27.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 27.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 27.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 26.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0719\n"
     ]
    }
   ],
   "source": [
    "RF = [\n",
    "    \"Age_at_recruitment\",\n",
    "    \"Sex_M\",\n",
    "    \"HbA1C\",\n",
    "    \"LDL_cholesterol\",\n",
    "    \"HDL_cholesterol\",\n",
    "    \"Cholesterol\",\n",
    "    \"Triglycerides\",\n",
    "    \"SBP\",\n",
    "    \"eGFR\",\n",
    "    \"antihypertensives\",\n",
    "    \"antihyperglycemic\",\n",
    "    \"lipid_lowering\",\n",
    "]\n",
    "lifeStyle_cols\n",
    "features = lifeStyle_cols + RF + [\"PRS\"]\n",
    "\n",
    "cat_cols = [\n",
    "    *lifeStyle_cols,\n",
    "    \"Sex_M\",\n",
    "    \"antihypertensives\",\n",
    "    \"antihyperglycemic\",\n",
    "    \"lipid_lowering\",\n",
    "]\n",
    "qt_cols = [\n",
    "    \"Age_at_recruitment\",\n",
    "    \"HbA1C\",\n",
    "    \"LDL_cholesterol\",\n",
    "    \"HDL_cholesterol\",\n",
    "    \"Cholesterol\",\n",
    "    \"Triglycerides\",\n",
    "    \"SBP\",\n",
    "    \"eGFR\",\n",
    "    \"PRS\",\n",
    "]\n",
    "\n",
    "label = \"incident\"\n",
    "# cat_cols += [label]\n",
    "qt_cols += [label]\n",
    "\n",
    "col_to_stype = {\n",
    "    **{k: stype.numerical for k in qt_cols},\n",
    "    **{k: stype.categorical for k in cat_cols},\n",
    "}\n",
    "\n",
    "model = fit_tabular_dl(\n",
    "    train=train_df,\n",
    "    label=label,\n",
    "    xvar=features,\n",
    "    col_to_stype=col_to_stype,\n",
    "    epochs=5,\n",
    "    num_trials=1,\n",
    "    model_type=\"TabNet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:01<00:00, 98.95it/s] \n",
      "/tmp/ipykernel_29921/1021034837.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.7933615086426051,\n",
       " 'ACC': 0.7812181763135442,\n",
       " 'Macro_F1': 0.45681458092015004,\n",
       " 'Sensitivity': 0.7170474516695958,\n",
       " 'Specificity': 0.7816007225988642,\n",
       " 'APR': 0.0329010602889096}"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def get_predict_from_tb_dl_with_df(\n",
    "    df,\n",
    "    model,\n",
    "    x_var,\n",
    "    batch_size=2048,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    from torch_frame.data import Dataset\n",
    "    from torch_frame.data import DataLoader\n",
    "\n",
    "    # dataset\n",
    "    df = df[x_var]\n",
    "    col_to_stype = model.col_to_stype\n",
    "    DataSetUsed = Dataset(\n",
    "        df=df,\n",
    "        col_to_stype=col_to_stype,\n",
    "        # target_col=target_col,\n",
    "    )\n",
    "    DataSetUsed.materialize()\n",
    "    DataSetUsedLoader = DataLoader(DataSetUsed, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = []\n",
    "        for batch in tqdm(DataSetUsedLoader):\n",
    "            batch = batch.to(device)\n",
    "            pred.append(model(batch).cpu().numpy())\n",
    "        pred = np.concatenate(pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "pred = get_predict_from_tb_dl_with_df(\n",
    "    test_df,\n",
    "    model,\n",
    "    features,\n",
    "    batch_size=2048,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")\n",
    "\n",
    "from ppp_prediction.metrics import cal_binary_metrics\n",
    "\n",
    "test_df[\"pred\"] = pred\n",
    "test_df\n",
    "cal_binary_metrics(\n",
    "    # train_to_cal[label], train_to_cal[\n",
    "    test_df[label],\n",
    "    test_df[f\"pred\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290664, 1)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"TabNet\", \"ResNet\", \"MLP\", \"ExcelFormer\", \"FTTransformer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test data provided, will split the train data into train and test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 19:50:47,775] A new study created in memory with name: no-name-f0d8239e-b331-4ef4-9e7d-eac2126d7d66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter search via Optuna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 26.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 26.93it/s]\n",
      "[I 2025-03-24 19:50:52,074] Trial 0 finished with value: 0.07166009396314621 and parameters: {'split_attn_channels': 256, 'split_feat_channels': 64, 'gamma': 1.0, 'num_layers': 8, 'batch_size': 4096, 'base_lr': 0.001, 'gamma_rate': 1.0}. Best is trial 0 with value: 0.07166009396314621.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0718\n",
      "Best val: 0.0717\n",
      "Hyper-parameter search done. Found the best config.\n",
      "config {'batch_size': 4096, 'gamma_rate': 1.0, 'base_lr': 0.001} and model config {'split_attn_channels': 256, 'split_feat_channels': 64, 'gamma': 1.0, 'num_layers': 8}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 26.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060, Val: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 26.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 26.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 26.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [00:02<00:00, 101.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test data provided, will split the train data into train and test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 19:51:25,171] A new study created in memory with name: no-name-ba8070ce-78fa-40dd-9bf1-f323cba52906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter search via Optuna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 39/39 [00:01<00:00, 32.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060, Val: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 39/39 [00:01<00:00, 38.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 39/39 [00:01<00:00, 37.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 39/39 [00:01<00:00, 38.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 39/39 [00:01<00:00, 37.93it/s]\n",
      "[I 2025-03-24 19:51:31,114] Trial 0 finished with value: 0.07211237400770187 and parameters: {'split_attn_channels': 256, 'split_feat_channels': 128, 'gamma': 1.5, 'num_layers': 8, 'batch_size': 2048, 'base_lr': 0.001, 'gamma_rate': 0.9}. Best is trial 0 with value: 0.07211237400770187.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0721\n",
      "Best val: 0.0721\n",
      "Hyper-parameter search done. Found the best config.\n",
      "config {'batch_size': 2048, 'gamma_rate': 0.9, 'base_lr': 0.001} and model config {'split_attn_channels': 256, 'split_feat_channels': 128, 'gamma': 1.5, 'num_layers': 8}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 39/39 [00:01<00:00, 37.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060, Val: 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 39/39 [00:01<00:00, 37.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060, Val: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 39/39 [00:01<00:00, 36.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060, Val: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 39/39 [00:01<00:00, 34.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 39/39 [00:01<00:00, 36.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [00:02<00:00, 93.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test data provided, will split the train data into train and test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 19:52:06,114] A new study created in memory with name: no-name-699b0547-53ae-4365-bd60-6a9632f839fe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter search via Optuna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 31.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060, Val: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 53.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 51.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 47.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 49.24it/s]\n",
      "[I 2025-03-24 19:52:08,513] Trial 0 finished with value: 0.07165241241455078 and parameters: {'split_attn_channels': 64, 'split_feat_channels': 64, 'gamma': 1.0, 'num_layers': 6, 'batch_size': 4096, 'base_lr': 0.001, 'gamma_rate': 1.0}. Best is trial 0 with value: 0.07165241241455078.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0718\n",
      "Best val: 0.0717\n",
      "Hyper-parameter search done. Found the best config.\n",
      "config {'batch_size': 4096, 'gamma_rate': 1.0, 'base_lr': 0.001} and model config {'split_attn_channels': 64, 'split_feat_channels': 64, 'gamma': 1.0, 'num_layers': 6}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 49.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0078, Val: 0.0737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 48.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 47.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 50.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 48.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [00:02<00:00, 89.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test data provided, will split the train data into train and test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 19:52:42,187] A new study created in memory with name: no-name-320c9237-37d5-4f87-96b3-31eb526a5356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter search via Optuna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 25.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0062, Val: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 29.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 33.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 36.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Val: 0.0716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 32.54it/s]\n",
      "[I 2025-03-24 19:52:45,686] Trial 0 finished with value: 0.07154668122529984 and parameters: {'split_attn_channels': 128, 'split_feat_channels': 64, 'gamma': 1.0, 'num_layers': 8, 'batch_size': 4096, 'base_lr': 0.01, 'gamma_rate': 0.9}. Best is trial 0 with value: 0.07154668122529984.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0716\n",
      "Best val: 0.0715\n",
      "Hyper-parameter search done. Found the best config.\n",
      "config {'batch_size': 4096, 'gamma_rate': 0.9, 'base_lr': 0.01} and model config {'split_attn_channels': 128, 'split_feat_channels': 64, 'gamma': 1.0, 'num_layers': 8}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 31.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0068, Val: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 36.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 39.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 34.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 36.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0058, Val: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [00:02<00:00, 87.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test data provided, will split the train data into train and test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 19:53:20,912] A new study created in memory with name: no-name-fc4590b2-39d2-4c69-9628-e8d2bcfda4f3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter search via Optuna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 23.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0067, Val: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 36.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 33.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060, Val: 0.0723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 32.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 35.40it/s]\n",
      "[I 2025-03-24 19:53:24,360] Trial 0 finished with value: 0.07197567820549011 and parameters: {'split_attn_channels': 64, 'split_feat_channels': 128, 'gamma': 1.5, 'num_layers': 8, 'batch_size': 4096, 'base_lr': 0.001, 'gamma_rate': 0.9}. Best is trial 0 with value: 0.07197567820549011.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0059, Val: 0.0720\n",
      "Best val: 0.0720\n",
      "Hyper-parameter search done. Found the best config.\n",
      "config {'batch_size': 4096, 'gamma_rate': 0.9, 'base_lr': 0.001} and model config {'split_attn_channels': 64, 'split_feat_channels': 128, 'gamma': 1.5, 'num_layers': 8}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 19/19 [00:00<00:00, 33.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0072, Val: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 19/19 [00:00<00:00, 34.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0062, Val: 0.0739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 19/19 [00:00<00:00, 30.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0061, Val: 0.0731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 19/19 [00:00<00:00, 30.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060, Val: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 19/19 [00:00<00:00, 32.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060, Val: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [00:01<00:00, 109.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dict = {}\n",
    "\n",
    "for modelName in models:\n",
    "    model = fit_tabular_dl(\n",
    "        train=train_df,\n",
    "        label=label,\n",
    "        xvar=features,\n",
    "        col_to_stype=col_to_stype,\n",
    "        epochs=5,\n",
    "        num_trials=1,\n",
    "        model_type=\"TabNet\",\n",
    "    )\n",
    "    total_data[modelName] = get_predict_from_tb_dl_with_df(\n",
    "        total_data,\n",
    "        model,\n",
    "        features,\n",
    "        batch_size=2048,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    )\n",
    "    model_dict[modelName] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[368], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modelName \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m      5\u001b[0m     to_cal_df \u001b[38;5;241m=\u001b[39m total_data[total_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m     res_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcal_binary_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# train_to_cal[label], train_to_cal[\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_cal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_cal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodelName\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mci\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     res_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m modelName\n\u001b[1;32m     13\u001b[0m     res_metrics_list\u001b[38;5;241m.\u001b[39mappend(res_metrics)\n",
      "File \u001b[0;32m~/ukb/project/ppp_prediction/ppp_prediction/metrics/common.py:84\u001b[0m, in \u001b[0;36mcal_binary_metrics\u001b[0;34m(y, y_pred, ci, n_resamples)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m\"\u001b[39m: AUC,\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACC\u001b[39m\u001b[38;5;124m\"\u001b[39m: ACC,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPR\u001b[39m\u001b[38;5;124m\"\u001b[39m: APR,\n\u001b[1;32m     82\u001b[0m     }\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ci:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcal_binary_metrics_bootstrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mci_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_resamples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_resamples\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ukb/project/ppp_prediction/ppp_prediction/metrics/common.py:197\u001b[0m, in \u001b[0;36mcal_binary_metrics_bootstrap\u001b[0;34m(y, y_pred, ci_kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m y_pred_binary \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m>\u001b[39m optim_threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# cal metrics\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m AUC, AUC_CI \u001b[38;5;241m=\u001b[39m \u001b[43mci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mci_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m ACC, ACC_CI \u001b[38;5;241m=\u001b[39m ci\u001b[38;5;241m.\u001b[39maccuracy_score(y, y_pred_binary, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mci_params)\n\u001b[1;32m    199\u001b[0m macro_f1, macro_f1_CI \u001b[38;5;241m=\u001b[39m ci\u001b[38;5;241m.\u001b[39mf1_score(y, y_pred_binary, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mci_params)\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/confidenceinterval/auc.py:40\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_pred, confidence_level, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m auc, ci\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m bootstrap_methods:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mroc_auc_score_bootstrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/confidenceinterval/auc.py:16\u001b[0m, in \u001b[0;36mroc_auc_score_bootstrap\u001b[0;34m(y_true, y_pred, confidence_level, method, n_resamples, random_state)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroc_auc_score_bootstrap\u001b[39m(y_true: List,\n\u001b[1;32m     11\u001b[0m                             y_pred: List,\n\u001b[1;32m     12\u001b[0m                             confidence_level: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m     13\u001b[0m                             method: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap_bca\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m                             n_resamples: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9999\u001b[39m,\n\u001b[1;32m     15\u001b[0m                             random_state: Callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbootstrap_ci\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroc_auc_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconfidence_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mn_resamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_resamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/confidenceinterval/bootstrap.py:31\u001b[0m, in \u001b[0;36mbootstrap_ci\u001b[0;34m(y_true, y_pred, metric, confidence_level, n_resamples, method, random_state)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m bootstrap_methods, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBootstrap ci method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbootstrap_methods\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m indices \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(y_true)), )\n\u001b[0;32m---> 31\u001b[0m bootstrap_res \u001b[38;5;241m=\u001b[39m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mstatistic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatistic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mn_resamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_resamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mconfidence_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbootstrap_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m result \u001b[38;5;241m=\u001b[39m metric(y_true, y_pred)\n\u001b[1;32m     38\u001b[0m ci \u001b[38;5;241m=\u001b[39m bootstrap_res\u001b[38;5;241m.\u001b[39mconfidence_interval\u001b[38;5;241m.\u001b[39mlow, bootstrap_res\u001b[38;5;241m.\u001b[39mconfidence_interval\u001b[38;5;241m.\u001b[39mhigh\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/scipy/stats/_resampling.py:630\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m(data, statistic, n_resamples, batch, vectorized, paired, axis, confidence_level, alternative, method, bootstrap_result, random_state)\u001b[0m\n\u001b[1;32m    627\u001b[0m         resampled_data\u001b[38;5;241m.\u001b[39mappend(resample)\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# Compute bootstrap distribution of statistic\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     theta_hat_b\u001b[38;5;241m.\u001b[39mappend(\u001b[43mstatistic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresampled_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    632\u001b[0m theta_hat_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(theta_hat_b, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# Calculate percentile interval\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/scipy/stats/_resampling.py:38\u001b[0m, in \u001b[0;36m_vectorize_statistic.<locals>.stat_nd\u001b[0;34m(axis, *data)\u001b[0m\n\u001b[1;32m     35\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msplit(z, split_indices)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m statistic(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstat_1d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m[()]\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/numpy/lib/shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m buff[ind0] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[0;32m--> 402\u001b[0m     buff[ind] \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43mfunc1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minarr_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, matrix):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     buff \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39m__array_wrap__(buff)\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/scipy/stats/_resampling.py:36\u001b[0m, in \u001b[0;36m_vectorize_statistic.<locals>.stat_nd.<locals>.stat_1d\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstat_1d\u001b[39m(z):\n\u001b[1;32m     35\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msplit(z, split_indices)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstatistic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/confidenceinterval/bootstrap.py:26\u001b[0m, in \u001b[0;36mbootstrap_ci.<locals>.statistic\u001b[0;34m(*indices)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstatistic\u001b[39m(\u001b[38;5;241m*\u001b[39mindices):\n\u001b[1;32m     25\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(indices)[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:640\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    639\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[0;32m--> 640\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_binarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    642\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    643\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:528\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[0;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos_switch:\n\u001b[1;32m    526\u001b[0m     pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mneg_label\n\u001b[0;32m--> 528\u001b[0m y_type \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m y_type:\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultioutput target data is not supported with label binarization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/multiclass.py:423\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name, raise_unknown)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(first_row_or_val):\n\u001b[1;32m    422\u001b[0m     first_row_or_val \u001b[38;5;241m=\u001b[39m first_row_or_val\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m--> 423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcached_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first_row_or_val) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/_unique.py:105\u001b[0m, in \u001b[0;36mcached_unique\u001b[0;34m(xp, *ys)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_unique\u001b[39m(\u001b[38;5;241m*\u001b[39mys, xp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the unique values of ys.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    Use the cached values from dtype.metadata if present.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        Unique values of ys.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_cached_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/_unique.py:105\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_unique\u001b[39m(\u001b[38;5;241m*\u001b[39mys, xp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the unique values of ys.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    Use the cached values from dtype.metadata if present.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        Unique values of ys.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43m_cached_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/_unique.py:78\u001b[0m, in \u001b[0;36m_cached_unique\u001b[0;34m(y, xp)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     77\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/sklearn/utils/_array_api.py:416\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.unique_values\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rapids-24.02/lib/python3.10/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ppp_prediction.metrics import cal_binary_metrics\n",
    "\n",
    "res_metrics_list = []\n",
    "for modelName in models:\n",
    "    to_cal_df = total_data[total_data[\"Type\"] == \"Test\"]\n",
    "    res_metrics = cal_binary_metrics(\n",
    "        # train_to_cal[label], train_to_cal[\n",
    "        to_cal_df[label],\n",
    "        to_cal_df[modelName],\n",
    "        ci=False,\n",
    "    )\n",
    "    res_metrics[\"model\"] = modelName\n",
    "    res_metrics_list.append(res_metrics)\n",
    "res_metrics_df = pd.DataFrame(res_metrics_list)\n",
    "res_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUC_UCI</th>\n",
       "      <th>AUC_LCI</th>\n",
       "      <th>ACC</th>\n",
       "      <th>ACC_UCI</th>\n",
       "      <th>ACC_LCI</th>\n",
       "      <th>Macro_F1</th>\n",
       "      <th>Macro_F1_UCI</th>\n",
       "      <th>Macro_F1_LCI</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>...</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Specificity_UCI</th>\n",
       "      <th>Specificity_LCI</th>\n",
       "      <th>APR</th>\n",
       "      <th>APR_UCI</th>\n",
       "      <th>APR_LCI</th>\n",
       "      <th>N</th>\n",
       "      <th>N_case</th>\n",
       "      <th>N_control</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825122</td>\n",
       "      <td>0.836434</td>\n",
       "      <td>0.817335</td>\n",
       "      <td>0.764708</td>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.763061</td>\n",
       "      <td>0.451333</td>\n",
       "      <td>0.452475</td>\n",
       "      <td>0.450249</td>\n",
       "      <td>0.762742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764719</td>\n",
       "      <td>0.766133</td>\n",
       "      <td>0.763209</td>\n",
       "      <td>0.047334</td>\n",
       "      <td>0.052851</td>\n",
       "      <td>0.039482</td>\n",
       "      <td>290664</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>288957.0</td>\n",
       "      <td>TabNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.800796</td>\n",
       "      <td>0.811705</td>\n",
       "      <td>0.787975</td>\n",
       "      <td>0.780788</td>\n",
       "      <td>0.782120</td>\n",
       "      <td>0.779274</td>\n",
       "      <td>0.456471</td>\n",
       "      <td>0.457745</td>\n",
       "      <td>0.455258</td>\n",
       "      <td>0.709432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781210</td>\n",
       "      <td>0.782702</td>\n",
       "      <td>0.779535</td>\n",
       "      <td>0.035901</td>\n",
       "      <td>0.040207</td>\n",
       "      <td>0.030247</td>\n",
       "      <td>290664</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>288957.0</td>\n",
       "      <td>ResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.799604</td>\n",
       "      <td>0.812685</td>\n",
       "      <td>0.785161</td>\n",
       "      <td>0.789083</td>\n",
       "      <td>0.790611</td>\n",
       "      <td>0.787636</td>\n",
       "      <td>0.460049</td>\n",
       "      <td>0.461219</td>\n",
       "      <td>0.458981</td>\n",
       "      <td>0.719977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789491</td>\n",
       "      <td>0.790918</td>\n",
       "      <td>0.787973</td>\n",
       "      <td>0.043817</td>\n",
       "      <td>0.047928</td>\n",
       "      <td>0.038819</td>\n",
       "      <td>290664</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>288957.0</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AUC   AUC_UCI   AUC_LCI       ACC   ACC_UCI   ACC_LCI  Macro_F1  \\\n",
       "0  0.825122  0.836434  0.817335  0.764708  0.766007  0.763061  0.451333   \n",
       "1  0.800796  0.811705  0.787975  0.780788  0.782120  0.779274  0.456471   \n",
       "2  0.799604  0.812685  0.785161  0.789083  0.790611  0.787636  0.460049   \n",
       "\n",
       "   Macro_F1_UCI  Macro_F1_LCI  Sensitivity  ...  Specificity  Specificity_UCI  \\\n",
       "0      0.452475      0.450249     0.762742  ...     0.764719         0.766133   \n",
       "1      0.457745      0.455258     0.709432  ...     0.781210         0.782702   \n",
       "2      0.461219      0.458981     0.719977  ...     0.789491         0.790918   \n",
       "\n",
       "   Specificity_LCI       APR   APR_UCI   APR_LCI       N  N_case  N_control  \\\n",
       "0         0.763209  0.047334  0.052851  0.039482  290664  1707.0   288957.0   \n",
       "1         0.779535  0.035901  0.040207  0.030247  290664  1707.0   288957.0   \n",
       "2         0.787973  0.043817  0.047928  0.038819  290664  1707.0   288957.0   \n",
       "\n",
       "    model  \n",
       "0  TabNet  \n",
       "1  ResNet  \n",
       "2     MLP  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>Macro_F1</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>APR</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825122</td>\n",
       "      <td>0.764708</td>\n",
       "      <td>0.451333</td>\n",
       "      <td>0.763327</td>\n",
       "      <td>0.764719</td>\n",
       "      <td>0.047334</td>\n",
       "      <td>TabNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.800796</td>\n",
       "      <td>0.780788</td>\n",
       "      <td>0.456471</td>\n",
       "      <td>0.710018</td>\n",
       "      <td>0.781210</td>\n",
       "      <td>0.035901</td>\n",
       "      <td>ResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.799604</td>\n",
       "      <td>0.789083</td>\n",
       "      <td>0.460049</td>\n",
       "      <td>0.720562</td>\n",
       "      <td>0.789491</td>\n",
       "      <td>0.043817</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.794367</td>\n",
       "      <td>0.803089</td>\n",
       "      <td>0.464902</td>\n",
       "      <td>0.690100</td>\n",
       "      <td>0.803760</td>\n",
       "      <td>0.042007</td>\n",
       "      <td>ExcelFormer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.753385</td>\n",
       "      <td>0.691420</td>\n",
       "      <td>0.421466</td>\n",
       "      <td>0.709432</td>\n",
       "      <td>0.691317</td>\n",
       "      <td>0.022235</td>\n",
       "      <td>FTTransformer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AUC       ACC  Macro_F1  Sensitivity  Specificity       APR  \\\n",
       "0  0.825122  0.764708  0.451333     0.763327     0.764719  0.047334   \n",
       "1  0.800796  0.780788  0.456471     0.710018     0.781210  0.035901   \n",
       "2  0.799604  0.789083  0.460049     0.720562     0.789491  0.043817   \n",
       "3  0.794367  0.803089  0.464902     0.690100     0.803760  0.042007   \n",
       "4  0.753385  0.691420  0.421466     0.709432     0.691317  0.022235   \n",
       "\n",
       "           model  \n",
       "0         TabNet  \n",
       "1         ResNet  \n",
       "2            MLP  \n",
       "3    ExcelFormer  \n",
       "4  FTTransformer  "
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_predict_from_tb_dl_with_df(df, model, x_var, col_to_stype, target_col, batch_size=2048, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    from torch_frame.data import Dataset\n",
    "    from torch_frame.data import DataLoader\n",
    "    # dataset\n",
    "    DataSetUsed = Dataset(df = df[x_var],\n",
    "                          col_to_stype = col_to_stype,\n",
    "                          target_col = target_col,\n",
    "                          )\n",
    "    DataSetUsed.materialize()\n",
    "    DataSetUsedLoader = DataLoader(DataSetUsed, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = []\n",
    "        for batch in tqdm(DataSetUsedLoader):\n",
    "            batch = batch.to(device)\n",
    "            pred.append(model(batch).cpu().numpy())\n",
    "        pred = np.concatenate(pred)\n",
    "    return pred\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_to_stype = {\n",
    "#     **{k: stype.numerical for k in train_df.columns if k not in [\"eid\", \"target\"]},\n",
    "#     **{\"target\": stype.categorical},\n",
    "# }\n",
    "\n",
    "col_to_stype = {\n",
    "    **{k: stype.numerical for k in qt_cols},\n",
    "    **{k: stype.categorical for k in cat_cols},\n",
    "}\n",
    "\n",
    "TrainDataSet = Dataset(\n",
    "    df=train_df,\n",
    "    col_to_stype=col_to_stype,\n",
    "    target_col=\"incident\",\n",
    ")\n",
    "TrainDataSet.materialize()\n",
    "\n",
    "ValDataSet = Dataset(\n",
    "    df=val_df,\n",
    "    col_to_stype=col_to_stype,\n",
    "    target_col=\"incident\",\n",
    ")\n",
    "ValDataSet.materialize()\n",
    "\n",
    "TestDataSet = Dataset(\n",
    "    df=test_df,\n",
    "    col_to_stype=col_to_stype,\n",
    "    target_col=\"incident\",\n",
    ")\n",
    "TestDataSet.materialize()\n",
    "from torch_frame.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(TrainDataSet.tensor_frame, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(ValDataSet.tensor_frame, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(TestDataSet.tensor_frame, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TaskType.BINARY_CLASSIFICATION: 'binary_classification'>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "from typing import Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, Module, MSELoss\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torchmetrics import AUROC, Accuracy, MeanSquaredError\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_frame import stype\n",
    "from torch_frame.data import DataLoader\n",
    "from torch_frame.datasets import DataFrameBenchmark\n",
    "from torch_frame.gbdt import CatBoost, LightGBM, XGBoost\n",
    "from torch_frame.nn.encoder import EmbeddingEncoder, LinearBucketEncoder\n",
    "from torch_frame.nn.models import (\n",
    "    MLP,\n",
    "    ExcelFormer,\n",
    "    FTTransformer,\n",
    "    ResNet,\n",
    "    TabNet,\n",
    "    TabTransformer,\n",
    "    Trompt,\n",
    ")\n",
    "from torch_frame.typing import TaskType\n",
    "\n",
    "# class Args:\n",
    "#     def __init__(self, **kwargs):\n",
    "#         # 使用字典存储键值对\n",
    "#         self.__dict__.update(kwargs)\n",
    "\n",
    "# # 创建 Args 对象\n",
    "\n",
    "# args = Args(\n",
    "#     model_type = \"TabNet\", # TabNet, TabTransformer, ExcelFormer, MLP, ResNet, Trompt, LightGBM, CatBoost, XGBoost\n",
    "#     task_type = \"binary_classification\", # binary_classification, multiclass_classification, regression\n",
    "#     scale = \"small\",\n",
    "#     idx = 0,\n",
    "# )\n",
    "\n",
    "model_type = \"MLP\"\n",
    "train_dataset = TrainDataSet\n",
    "test_dataset = TestDataSet\n",
    "val_dataset = ValDataSet\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRAIN_CONFIG_KEYS = [\"batch_size\", \"gamma_rate\", \"base_lr\"]\n",
    "task_type = \"binary_classification\"  # binary_classification, multiclass_classification, regression\n",
    "sacle = \"small\"  # small, medium, large\n",
    "epochs = 10\n",
    "num_trials = 3  # Number of Optuna-based hyper-parameter tuning.\n",
    "num_repeats = 5  # Number of repeated training and eval on the best config\n",
    "seed = 42\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "train_tensor_frame = train_dataset.tensor_frame\n",
    "val_tensor_frame = val_dataset.tensor_frame\n",
    "test_tensor_frame = test_dataset.tensor_frame\n",
    "\n",
    "if train_dataset.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "    out_channels = 1\n",
    "    loss_fun = BCEWithLogitsLoss()\n",
    "    metric_computer = AUROC(task=\"binary\").to(device)\n",
    "    higher_is_better = True\n",
    "elif train_dataset.task_type == TaskType.MULTICLASS_CLASSIFICATION:\n",
    "    out_channels = train_dataset.num_classes\n",
    "    loss_fun = CrossEntropyLoss()\n",
    "    metric_computer = Accuracy(\n",
    "        task=\"multiclass\", num_classes=train_dataset.num_classes\n",
    "    ).to(device)\n",
    "    higher_is_better = True\n",
    "elif train_dataset.task_type == TaskType.REGRESSION:\n",
    "    out_channels = 1\n",
    "    loss_fun = MSELoss()\n",
    "    metric_computer = MeanSquaredError(squared=False).to(device)\n",
    "    higher_is_better = False\n",
    "\n",
    "# To be set for each model\n",
    "model_cls = None\n",
    "col_stats = None\n",
    "\n",
    "# Set up model specific search space\n",
    "if model_type == \"TabNet\":\n",
    "    model_search_space = {\n",
    "        \"split_attn_channels\": [64, 128, 256],\n",
    "        \"split_feat_channels\": [64, 128, 256],\n",
    "        \"gamma\": [1.0, 1.2, 1.5],\n",
    "        \"num_layers\": [4, 6, 8],\n",
    "    }\n",
    "    train_search_space = {\n",
    "        \"batch_size\": [\n",
    "            2048,\n",
    "            4096,\n",
    "        ],  # Note if you have a small data, you may want to reduce it, also low gpu memory\n",
    "        # \"batch_size\": [128, 256],\n",
    "        \"base_lr\": [0.001, 0.01],\n",
    "        \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "    }\n",
    "    model_cls = TabNet\n",
    "    col_stats = train_dataset.col_stats\n",
    "elif model_type == \"FTTransformer\":\n",
    "    model_search_space = {\n",
    "        \"channels\": [64, 128, 256],\n",
    "        \"num_layers\": [4, 6, 8],\n",
    "    }\n",
    "    train_search_space = {\n",
    "        \"batch_size\": [256, 512],\n",
    "        \"base_lr\": [0.0001, 0.001],\n",
    "        \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "    }\n",
    "    model_cls = FTTransformer\n",
    "    col_stats = train_dataset.col_stats\n",
    "elif model_type == \"FTTransformerBucket\":\n",
    "    model_search_space = {\n",
    "        \"channels\": [64, 128, 256],\n",
    "        \"num_layers\": [4, 6, 8],\n",
    "    }\n",
    "    train_search_space = {\n",
    "        \"batch_size\": [256, 512],\n",
    "        \"base_lr\": [0.0001, 0.001],\n",
    "        \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "    }\n",
    "    model_cls = FTTransformer\n",
    "\n",
    "    col_stats = train_dataset.col_stats\n",
    "elif model_type == \"ResNet\":\n",
    "    model_search_space = {\n",
    "        \"channels\": [64, 128, 256],\n",
    "        \"num_layers\": [4, 6, 8],\n",
    "    }\n",
    "    train_search_space = {\n",
    "        \"batch_size\": [256, 512],\n",
    "        \"base_lr\": [0.0001, 0.001],\n",
    "        \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "    }\n",
    "    model_cls = ResNet\n",
    "    col_stats = train_dataset.col_stats\n",
    "elif model_type == \"MLP\":\n",
    "    model_search_space = {\n",
    "        \"channels\": [64, 128, 256],\n",
    "        \"num_layers\": [1, 2, 4],\n",
    "    }\n",
    "    train_search_space = {\n",
    "        \"batch_size\": [256, 512],\n",
    "        \"base_lr\": [0.0001, 0.001],\n",
    "        \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "    }\n",
    "    model_cls = MLP\n",
    "    col_stats = train_dataset.col_stats\n",
    "elif model_type == \"TabTransformer\":\n",
    "    model_search_space = {\n",
    "        \"channels\": [16, 32, 64, 128],\n",
    "        \"num_layers\": [4, 6, 8],\n",
    "        \"num_heads\": [4, 8],\n",
    "        \"encoder_pad_size\": [2, 4],\n",
    "        \"attn_dropout\": [0, 0.2],\n",
    "        \"ffn_dropout\": [0, 0.2],\n",
    "    }\n",
    "    train_search_space = {\n",
    "        \"batch_size\": [128, 256],\n",
    "        \"base_lr\": [0.0001, 0.001],\n",
    "        \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "    }\n",
    "    model_cls = TabTransformer\n",
    "    col_stats = train_dataset.col_stats\n",
    "elif model_type == \"Trompt\":\n",
    "    model_search_space = {\n",
    "        \"channels\": [64, 128, 192],\n",
    "        \"num_layers\": [4, 6, 8],\n",
    "        \"num_prompts\": [64, 128, 192],\n",
    "    }\n",
    "    train_search_space = {\n",
    "        \"batch_size\": [128, 256],\n",
    "        \"base_lr\": [0.01, 0.001],\n",
    "        \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "    }\n",
    "    if train_tensor_frame.num_cols > 20:\n",
    "        # Reducing the model size to avoid GPU OOM\n",
    "        model_search_space[\"channels\"] = [64, 128]\n",
    "        model_search_space[\"num_prompts\"] = [64, 128]\n",
    "    elif train_tensor_frame.num_cols > 50:\n",
    "        model_search_space[\"channels\"] = [64]\n",
    "        model_search_space[\"num_prompts\"] = [64]\n",
    "    model_cls = Trompt\n",
    "    col_stats = train_dataset.col_stats\n",
    "elif model_type == \"ExcelFormer\":\n",
    "    from torch_frame.transforms import (\n",
    "        CatToNumTransform,\n",
    "        MutualInformationSort,\n",
    "    )\n",
    "\n",
    "    categorical_transform = CatToNumTransform()\n",
    "    categorical_transform.fit(train_dataset.tensor_frame, train_dataset.col_stats)\n",
    "    train_tensor_frame = categorical_transform(train_tensor_frame)\n",
    "    # val_tensor_frame = categorical_transform(val_tensor_frame)\n",
    "    # test_tensor_frame = categorical_transform(test_tensor_frame)\n",
    "    col_stats = categorical_transform.transformed_stats\n",
    "\n",
    "    mutual_info_sort = MutualInformationSort(task_type=train_dataset.task_type)\n",
    "    mutual_info_sort.fit(train_tensor_frame, col_stats)\n",
    "    train_tensor_frame = mutual_info_sort(train_tensor_frame)\n",
    "    # val_tensor_frame = mutual_info_sort(val_tensor_frame)\n",
    "    # test_tensor_frame = mutual_info_sort(test_tensor_frame)\n",
    "\n",
    "    model_search_space = {\n",
    "        \"in_channels\": [128, 256],\n",
    "        \"num_heads\": [8, 16, 32],\n",
    "        \"num_layers\": [4, 6, 8],\n",
    "        \"diam_dropout\": [0, 0.2],\n",
    "        \"residual_dropout\": [0, 0.2],\n",
    "        \"aium_dropout\": [0, 0.2],\n",
    "        \"mixup\": [None, \"feature\", \"hidden\"],\n",
    "        \"beta\": [0.5],\n",
    "        \"num_cols\": [train_tensor_frame.num_cols],\n",
    "    }\n",
    "    train_search_space = {\n",
    "        \"batch_size\": [256, 512],\n",
    "        \"base_lr\": [0.001],\n",
    "        \"gamma_rate\": [0.9, 0.95, 1.0],\n",
    "    }\n",
    "    model_cls = ExcelFormer\n",
    "\n",
    "assert model_cls is not None\n",
    "assert col_stats is not None\n",
    "assert set(train_search_space.keys()) == set(TRAIN_CONFIG_KEYS)\n",
    "col_names_dict = train_tensor_frame.col_names_dict\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    loss_accum = total_count = 0\n",
    "\n",
    "    for tf in tqdm(loader, desc=f\"Epoch: {epoch}\"):\n",
    "        tf = tf.to(device)\n",
    "        y = tf.y\n",
    "        if isinstance(model, ExcelFormer):\n",
    "            # Train with FEAT-MIX or HIDDEN-MIX\n",
    "            pred, y = model(tf, mixup_encoded=True)\n",
    "        elif isinstance(model, Trompt):\n",
    "            # Trompt uses the layer-wise loss\n",
    "            pred = model(tf)\n",
    "            num_layers = pred.size(1)\n",
    "            # [batch_size * num_layers, num_classes]\n",
    "            pred = pred.view(-1, out_channels)\n",
    "            y = tf.y.repeat_interleave(num_layers)\n",
    "        else:\n",
    "            pred = model(tf)\n",
    "\n",
    "        if pred.size(1) == 1:\n",
    "            pred = pred.view(\n",
    "                -1,\n",
    "            )\n",
    "        if train_dataset.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "            y = y.to(torch.float)\n",
    "        loss = loss_fun(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_accum += float(loss) * len(tf.y)\n",
    "        total_count += len(tf.y)\n",
    "        optimizer.step()\n",
    "    return loss_accum / total_count\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(\n",
    "    model: Module,\n",
    "    loader: DataLoader,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    metric_computer.reset()\n",
    "    for tf in loader:\n",
    "        tf = tf.to(device)\n",
    "        pred = model(tf)\n",
    "        if isinstance(model, Trompt):\n",
    "            pred = pred.mean(dim=1)\n",
    "        if train_dataset.task_type == TaskType.MULTICLASS_CLASSIFICATION:\n",
    "            pred = pred.argmax(dim=-1)\n",
    "        elif train_dataset.task_type == TaskType.REGRESSION:\n",
    "            pred = pred.view(\n",
    "                -1,\n",
    "            )\n",
    "        metric_computer.update(pred, tf.y)\n",
    "    return metric_computer.compute().item()\n",
    "\n",
    "\n",
    "def train_and_eval_with_cfg(\n",
    "    model_cfg: dict[str, Any],\n",
    "    train_cfg: dict[str, Any],\n",
    "    trial: Optional[optuna.trial.Trial] = None,\n",
    ") -> tuple[float, float]:\n",
    "    # Use model_cfg to set up training procedure\n",
    "    if model_type == \"FTTransformerBucket\":\n",
    "        # Use LinearBucketEncoder instead\n",
    "        stype_encoder_dict = {\n",
    "            stype.categorical: EmbeddingEncoder(),\n",
    "            stype.numerical: LinearBucketEncoder(),\n",
    "        }\n",
    "        model_cfg[\"stype_encoder_dict\"] = stype_encoder_dict\n",
    "    model = model_cls(\n",
    "        **model_cfg,\n",
    "        out_channels=out_channels,\n",
    "        col_stats=col_stats,\n",
    "        col_names_dict=col_names_dict,\n",
    "    ).to(device)\n",
    "    model.reset_parameters()\n",
    "    # Use train_cfg to set up training procedure\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=train_cfg[\"base_lr\"])\n",
    "    lr_scheduler = ExponentialLR(optimizer, gamma=train_cfg[\"gamma_rate\"])\n",
    "    train_loader = DataLoader(\n",
    "        train_tensor_frame,\n",
    "        batch_size=train_cfg[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(val_tensor_frame, batch_size=train_cfg[\"batch_size\"])\n",
    "    test_loader = DataLoader(test_tensor_frame, batch_size=train_cfg[\"batch_size\"])\n",
    "\n",
    "    if higher_is_better:\n",
    "        best_val_metric = 0\n",
    "    else:\n",
    "        best_val_metric = math.inf\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, train_loader, optimizer, epoch)\n",
    "        val_metric = test(model, val_loader)\n",
    "\n",
    "        if higher_is_better:\n",
    "            if val_metric > best_val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                best_test_metric = test(model, test_loader)\n",
    "        else:\n",
    "            if val_metric < best_val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                best_test_metric = test(model, test_loader)\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val: {val_metric:.4f}\")\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(val_metric, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "    print(f\"Best val: {best_val_metric:.4f}, Best test: {best_test_metric:.4f}\")\n",
    "    return best_val_metric, best_test_metric\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    model_cfg = {}\n",
    "    for name, search_list in model_search_space.items():\n",
    "        model_cfg[name] = trial.suggest_categorical(name, search_list)\n",
    "    train_cfg = {}\n",
    "    for name, search_list in train_search_space.items():\n",
    "        train_cfg[name] = trial.suggest_categorical(name, search_list)\n",
    "\n",
    "    best_val_metric, _ = train_and_eval_with_cfg(\n",
    "        model_cfg=model_cfg, train_cfg=train_cfg, trial=trial\n",
    "    )\n",
    "    return best_val_metric\n",
    "\n",
    "\n",
    "def main_deep_models():\n",
    "    # Hyper-parameter optimization with Optuna\n",
    "    print(\"Hyper-parameter search via Optuna\")\n",
    "    start_time = time.time()\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(),\n",
    "        direction=\"maximize\" if higher_is_better else \"minimize\",\n",
    "    )\n",
    "    study.optimize(objective, n_trials=num_trials)\n",
    "    end_time = time.time()\n",
    "    search_time = end_time - start_time\n",
    "    print(\"Hyper-parameter search done. Found the best config.\")\n",
    "    params = study.best_params\n",
    "    best_train_cfg = {}\n",
    "    for train_cfg_key in TRAIN_CONFIG_KEYS:\n",
    "        best_train_cfg[train_cfg_key] = params.pop(train_cfg_key)\n",
    "    best_model_cfg = params\n",
    "\n",
    "    print(\n",
    "        f\"Repeat experiments {num_repeats} times with the best train \"\n",
    "        f\"config {best_train_cfg} and model config {best_model_cfg}.\"\n",
    "    )\n",
    "\n",
    "    # retrain model\n",
    "    if model_type == \"FTTransformerBucket\":\n",
    "        # Use LinearBucketEncoder instead\n",
    "        stype_encoder_dict = {\n",
    "            stype.categorical: EmbeddingEncoder(),\n",
    "            stype.numerical: LinearBucketEncoder(),\n",
    "        }\n",
    "        best_model_cfg[\"stype_encoder_dict\"] = stype_encoder_dict\n",
    "\n",
    "    model = model_cls(\n",
    "        **best_model_cfg,\n",
    "        out_channels=out_channels,\n",
    "        col_stats=col_stats,\n",
    "        col_names_dict=col_names_dict,\n",
    "    ).to(device)\n",
    "    model.reset_parameters()\n",
    "    # Use train_cfg to set up training procedure\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_train_cfg[\"base_lr\"])\n",
    "    lr_scheduler = ExponentialLR(optimizer, gamma=best_train_cfg[\"gamma_rate\"])\n",
    "    train_loader = DataLoader(\n",
    "        train_tensor_frame,\n",
    "        batch_size=best_train_cfg[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(val_tensor_frame, batch_size=best_train_cfg[\"batch_size\"])\n",
    "    test_loader = DataLoader(test_tensor_frame, batch_size=best_train_cfg[\"batch_size\"])\n",
    "\n",
    "    if higher_is_better:\n",
    "        best_val_metric = 0\n",
    "    else:\n",
    "        best_val_metric = math.inf\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, train_loader, optimizer, epoch)\n",
    "        val_metric = test(model, val_loader)\n",
    "\n",
    "        if higher_is_better:\n",
    "            if val_metric > best_val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                best_test_metric = test(model, test_loader)\n",
    "        else:\n",
    "            if val_metric < best_val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                best_test_metric = test(model, test_loader)\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val: {val_metric:.4f}\")\n",
    "\n",
    "    result_dict = {\n",
    "        # 'args': __dict__,\n",
    "        \"model\": model,\n",
    "        \"best_val_metric\": best_val_metric,\n",
    "        \"best_test_metric\": best_test_metric,\n",
    "        \"best_train_cfg\": best_train_cfg,\n",
    "        \"best_model_cfg\": best_model_cfg,\n",
    "        \"search_time\": search_time,\n",
    "    }\n",
    "    return result_dict\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_val_metrics = []\n",
    "    best_test_metrics = []\n",
    "    for _ in range(num_repeats):\n",
    "        best_val_metric, best_test_metric = train_and_eval_with_cfg(\n",
    "            best_model_cfg, best_train_cfg\n",
    "        )\n",
    "        best_val_metrics.append(best_val_metric)\n",
    "        best_test_metrics.append(best_test_metric)\n",
    "    # end_time = time.time()\n",
    "    # final_model_time = (end_time - start_time) / num_repeats\n",
    "    # best_val_metrics = np.array(best_val_metrics)\n",
    "    # best_test_metrics = np.array(best_test_metrics)\n",
    "\n",
    "    # result_dict = {\n",
    "    #     # 'args': __dict__,\n",
    "    #     \"best_val_metrics\": best_val_metrics,\n",
    "    #     \"best_test_metrics\": best_test_metrics,\n",
    "    #     \"best_val_metric\": best_val_metrics.mean(),\n",
    "    #     \"best_test_metric\": best_test_metrics.mean(),\n",
    "    #     \"best_train_cfg\": best_train_cfg,\n",
    "    #     \"best_model_cfg\": best_model_cfg,\n",
    "    #     \"search_time\": search_time,\n",
    "    #     \"final_model_time\": final_model_time,\n",
    "    #     \"total_time\": search_time + final_model_time,\n",
    "    # }\n",
    "    # print(result_dict)\n",
    "    # # Save results\n",
    "    # if result_path != \"\":\n",
    "    #     os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "    #     torch.save(result_dict, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_cls(\n",
    "#     **model_cfg,\n",
    "#     out_channels=out_channels,\n",
    "#     col_stats=col_stats,\n",
    "#     col_names_dict=col_names_dict,\n",
    "# )\n",
    "# model.to(device)\n",
    "# train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     optimizer,\n",
    "#     epoch,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 13:54:23,596] A new study created in memory with name: no-name-20fbe0fa-3088-409e-a763-1f3f3d3c0cd7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter search via Optuna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 198/198 [00:02<00:00, 95.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0695, Val: 0.7005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 198/198 [00:01<00:00, 147.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0370, Val: 0.7563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 198/198 [00:01<00:00, 147.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0347, Val: 0.8210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 198/198 [00:01<00:00, 140.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0324, Val: 0.8389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 198/198 [00:01<00:00, 125.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0314, Val: 0.8445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 198/198 [00:00<00:00, 202.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0313, Val: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 198/198 [00:00<00:00, 240.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0307, Val: 0.8492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 198/198 [00:01<00:00, 147.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0307, Val: 0.8511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 198/198 [00:00<00:00, 259.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0303, Val: 0.8526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 198/198 [00:01<00:00, 175.67it/s]\n",
      "[I 2025-03-24 13:54:48,946] Trial 0 finished with value: 0.8525776267051697 and parameters: {'channels': 64, 'num_layers': 4, 'batch_size': 512, 'base_lr': 0.001, 'gamma_rate': 0.9}. Best is trial 0 with value: 0.8525776267051697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0302, Val: 0.8525\n",
      "Best val: 0.8526, Best test: 0.8585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 397/397 [00:01<00:00, 372.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0423, Val: 0.7774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 397/397 [00:01<00:00, 394.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0325, Val: 0.7924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 397/397 [00:01<00:00, 353.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0319, Val: 0.7951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 397/397 [00:01<00:00, 355.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0316, Val: 0.8024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 397/397 [00:01<00:00, 349.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0311, Val: 0.8102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 397/397 [00:01<00:00, 354.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0311, Val: 0.8117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 397/397 [00:01<00:00, 359.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0305, Val: 0.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 397/397 [00:01<00:00, 367.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0304, Val: 0.8220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 397/397 [00:01<00:00, 370.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0302, Val: 0.8248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 397/397 [00:01<00:00, 356.14it/s]\n",
      "[I 2025-03-24 13:55:11,225] Trial 1 finished with value: 0.8301591873168945 and parameters: {'channels': 256, 'num_layers': 4, 'batch_size': 256, 'base_lr': 0.0001, 'gamma_rate': 0.9}. Best is trial 0 with value: 0.8525776267051697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0298, Val: 0.8302\n",
      "Best val: 0.8302, Best test: 0.8372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 198/198 [00:00<00:00, 411.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3212, Val: 0.6582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 198/198 [00:01<00:00, 182.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0498, Val: 0.7611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 198/198 [00:01<00:00, 169.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0353, Val: 0.8012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 198/198 [00:01<00:00, 172.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0331, Val: 0.8143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 198/198 [00:01<00:00, 162.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0323, Val: 0.8223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 198/198 [00:01<00:00, 169.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0317, Val: 0.8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 198/198 [00:01<00:00, 172.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0314, Val: 0.8334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 198/198 [00:01<00:00, 171.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0310, Val: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 198/198 [00:01<00:00, 187.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0308, Val: 0.8423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 198/198 [00:01<00:00, 185.94it/s]\n",
      "[I 2025-03-24 13:55:37,519] Trial 2 finished with value: 0.8456129431724548 and parameters: {'channels': 64, 'num_layers': 1, 'batch_size': 512, 'base_lr': 0.001, 'gamma_rate': 1.0}. Best is trial 0 with value: 0.8525776267051697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0305, Val: 0.8456\n",
      "Best val: 0.8456, Best test: 0.8411\n",
      "Hyper-parameter search done. Found the best config.\n",
      "Repeat experiments 5 times with the best train config {'batch_size': 512, 'gamma_rate': 0.9, 'base_lr': 0.001} and model config {'channels': 64, 'num_layers': 4}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 198/198 [00:01<00:00, 143.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0605, Val: 0.7547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 198/198 [00:00<00:00, 246.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0355, Val: 0.8017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 198/198 [00:01<00:00, 134.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0328, Val: 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 198/198 [00:01<00:00, 143.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0313, Val: 0.8466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 198/198 [00:01<00:00, 176.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0312, Val: 0.8521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 198/198 [00:00<00:00, 199.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0304, Val: 0.8555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 198/198 [00:01<00:00, 167.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0305, Val: 0.8560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 198/198 [00:00<00:00, 239.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0304, Val: 0.8567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 198/198 [00:01<00:00, 172.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0306, Val: 0.8587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 198/198 [00:01<00:00, 160.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0300, Val: 0.8586\n"
     ]
    }
   ],
   "source": [
    "res = main_deep_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predict\n",
    "model = res[\"model\"]\n",
    "model.eval()\n",
    "pred_array = []\n",
    "for tf in test_loader:\n",
    "    tf = tf.to(device)\n",
    "    pred = model(tf).cpu().detach().flatten().tolist()\n",
    "\n",
    "    # proba = torch.softmax(pred, dim=1)[:, 1].cpu().detach().numpy().tolist()\n",
    "    pred_array.extend(pred)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29921/1739847205.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.8585576386854802,\n",
       " 'ACC': 0.760648033468197,\n",
       " 'Macro_F1': 0.4506985981519423,\n",
       " 'Sensitivity': 0.8072642062097246,\n",
       " 'Specificity': 0.7603761113245223,\n",
       " 'APR': 0.05002365261877492}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestDataSet.df[\"pred\"] = pred_array\n",
    "from ppp_prediction.metrics import cal_binary_metrics\n",
    "\n",
    "cal_binary_metrics(TestDataSet.df[\"incident\"], TestDataSet.df[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "0.1027328372001648\n",
      "Epoch 1\n",
      "0.0993896871805191\n",
      "Epoch 2\n",
      "0.06829185783863068\n",
      "Epoch 3\n",
      "0.1154552698135376\n",
      "Epoch 4\n",
      "0.08367930352687836\n",
      "Epoch 5\n",
      "0.02228795550763607\n",
      "Epoch 6\n",
      "0.052397169172763824\n",
      "Epoch 7\n",
      "0.18704631924629211\n",
      "Epoch 8\n",
      "0.011632421053946018\n",
      "Epoch 9\n",
      "0.0033353206235915422\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ExampleTransformer(\n",
    "    channels=32,\n",
    "    out_channels=TrainDataSet.num_classes,\n",
    "    num_layers=2,\n",
    "    num_heads=8,\n",
    "    col_stats=TrainDataSet.col_stats,\n",
    "    col_names_dict=TrainDataSet.tensor_frame.col_names_dict,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for tf in train_loader:\n",
    "        tf = tf.to(device)\n",
    "        pred = model.forward(tf)\n",
    "        loss = F.cross_entropy(pred, tf.y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predict\n",
    "pred_array = []\n",
    "for tf in test_loader:\n",
    "    tf = tf.to(device)\n",
    "    pred = model(tf)\n",
    "\n",
    "    proba = torch.softmax(pred, dim=1)[:, 1].cpu().detach().numpy().tolist()\n",
    "    pred_array.extend(proba)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29921/995162998.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "TestDataSet.df[\"pred\"] = pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.99737962659679,\n",
       " 'ACC': 0.9736842105263158,\n",
       " 'Macro_F1': 0.9721203228173148,\n",
       " 'Sensitivity': 0.9859154929577465,\n",
       " 'Specificity': 0.9767441860465116,\n",
       " 'APR': 0.9983678405804648}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestDataSet.df[\"pred\"] = pred_array\n",
    "from ppp_prediction.metrics import cal_binary_metrics\n",
    "\n",
    "cal_binary_metrics(TestDataSet.df[\"target\"], TestDataSet.df[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_frame.datasets import Yandex\n",
    "from torch_frame.data import DataLoader\n",
    "\n",
    "train_dataset = Yandex(root=\"/tmp/adult\", name=\"adult\")\n",
    "train_dataset.materialize()\n",
    "train_dataset = train_dataset[:0.8]\n",
    "train_loader = DataLoader(train_dataset.tensor_frame, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target_col'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "from typing import Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, Module, MSELoss\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torchmetrics import AUROC, Accuracy, MeanSquaredError\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_frame import stype\n",
    "from torch_frame.data import DataLoader\n",
    "from torch_frame.datasets import DataFrameBenchmark\n",
    "from torch_frame.gbdt import CatBoost, LightGBM, XGBoost\n",
    "from torch_frame.nn.encoder import EmbeddingEncoder, LinearBucketEncoder\n",
    "from torch_frame.nn.models import (\n",
    "    MLP,\n",
    "    ExcelFormer,\n",
    "    FTTransformer,\n",
    "    ResNet,\n",
    "    TabNet,\n",
    "    TabTransformer,\n",
    "    Trompt,\n",
    ")\n",
    "from torch_frame.typing import TaskType\n",
    "\n",
    "TRAIN_CONFIG_KEYS = [\"batch_size\", \"gamma_rate\", \"base_lr\"]\n",
    "GBDT_MODELS = [\"XGBoost\", \"CatBoost\", \"LightGBM\"]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--task_type', type=str, choices=[\n",
    "        'binary_classification',\n",
    "        'multiclass_classification',\n",
    "        'regression',\n",
    "    ], default='binary_classification')\n",
    "parser.add_argument('--scale', type=str, choices=['small', 'medium', 'large'],\n",
    "                    default='small')\n",
    "parser.add_argument('--idx', type=int, default=0,\n",
    "                    help='The index of the dataset within DataFrameBenchmark')\n",
    "parser.add_argument('--epochs', type=int, default=50)\n",
    "parser.add_argument('--num_trials', type=int, default=20,\n",
    "                    help='Number of Optuna-based hyper-parameter tuning.')\n",
    "parser.add_argument(\n",
    "    '--num_repeats', type=int, default=5,\n",
    "    help='Number of repeated training and eval on the best config.')\n",
    "parser.add_argument(\n",
    "    '--model_type', type=str, default='TabNet', choices=[\n",
    "        'TabNet', 'FTTransformer', 'ResNet', 'MLP', 'TabTransformer', 'Trompt',\n",
    "        'ExcelFormer', 'FTTransformerBucket', 'XGBoost', 'CatBoost', 'LightGBM'\n",
    "    ])\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--result_path', type=str, default='')\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Prepare datasets\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data')\n",
    "train_dataset = DataFrameBenchmark(root=path, task_type=TaskType(args.task_type),\n",
    "                             scale=args.scale, idx=args.idx)\n",
    "train_dataset.materialize()\n",
    "train_dataset = train_dataset.shuffle()\n",
    "train_dataset, val_dataset, test_dataset = train_dataset.split()\n",
    "\n",
    "train_tensor_frame = train_dataset.tensor_frame\n",
    "val_tensor_frame = val_dataset.tensor_frame\n",
    "test_tensor_frame = test_dataset.tensor_frame\n",
    "\n",
    "if args.model_type in GBDT_MODELS:\n",
    "    gbdt_cls_dict = {\n",
    "        'XGBoost': XGBoost,\n",
    "        'CatBoost': CatBoost,\n",
    "        'LightGBM': LightGBM\n",
    "    }\n",
    "    model_cls = gbdt_cls_dict[args.model_type]\n",
    "else:\n",
    "    if train_dataset.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "        out_channels = 1\n",
    "        loss_fun = BCEWithLogitsLoss()\n",
    "        metric_computer = AUROC(task='binary').to(device)\n",
    "        higher_is_better = True\n",
    "    elif train_dataset.task_type == TaskType.MULTICLASS_CLASSIFICATION:\n",
    "        out_channels = train_dataset.num_classes\n",
    "        loss_fun = CrossEntropyLoss()\n",
    "        metric_computer = Accuracy(task='multiclass',\n",
    "                                   num_classes=train_dataset.num_classes).to(device)\n",
    "        higher_is_better = True\n",
    "    elif train_dataset.task_type == TaskType.REGRESSION:\n",
    "        out_channels = 1\n",
    "        loss_fun = MSELoss()\n",
    "        metric_computer = MeanSquaredError(squared=False).to(device)\n",
    "        higher_is_better = False\n",
    "\n",
    "    # To be set for each model\n",
    "    model_cls = None\n",
    "    col_stats = None\n",
    "\n",
    "    # Set up model specific search space\n",
    "    if args.model_type == 'TabNet':\n",
    "        model_search_space = {\n",
    "            'split_attn_channels': [64, 128, 256],\n",
    "            'split_feat_channels': [64, 128, 256],\n",
    "            'gamma': [1., 1.2, 1.5],\n",
    "            'num_layers': [4, 6, 8],\n",
    "        }\n",
    "        train_search_space = {\n",
    "            'batch_size': [2048, 4096],\n",
    "            'base_lr': [0.001, 0.01],\n",
    "            'gamma_rate': [0.9, 0.95, 1.],\n",
    "        }\n",
    "        model_cls = TabNet\n",
    "        col_stats = train_dataset.col_stats\n",
    "    elif args.model_type == 'FTTransformer':\n",
    "        model_search_space = {\n",
    "            'channels': [64, 128, 256],\n",
    "            'num_layers': [4, 6, 8],\n",
    "        }\n",
    "        train_search_space = {\n",
    "            'batch_size': [256, 512],\n",
    "            'base_lr': [0.0001, 0.001],\n",
    "            'gamma_rate': [0.9, 0.95, 1.],\n",
    "        }\n",
    "        model_cls = FTTransformer\n",
    "        col_stats = train_dataset.col_stats\n",
    "    elif args.model_type == 'FTTransformerBucket':\n",
    "        model_search_space = {\n",
    "            'channels': [64, 128, 256],\n",
    "            'num_layers': [4, 6, 8],\n",
    "        }\n",
    "        train_search_space = {\n",
    "            'batch_size': [256, 512],\n",
    "            'base_lr': [0.0001, 0.001],\n",
    "            'gamma_rate': [0.9, 0.95, 1.],\n",
    "        }\n",
    "        model_cls = FTTransformer\n",
    "\n",
    "        col_stats = train_dataset.col_stats\n",
    "    elif args.model_type == 'ResNet':\n",
    "        model_search_space = {\n",
    "            'channels': [64, 128, 256],\n",
    "            'num_layers': [4, 6, 8],\n",
    "        }\n",
    "        train_search_space = {\n",
    "            'batch_size': [256, 512],\n",
    "            'base_lr': [0.0001, 0.001],\n",
    "            'gamma_rate': [0.9, 0.95, 1.],\n",
    "        }\n",
    "        model_cls = ResNet\n",
    "        col_stats = train_dataset.col_stats\n",
    "    elif args.model_type == 'MLP':\n",
    "        model_search_space = {\n",
    "            'channels': [64, 128, 256],\n",
    "            'num_layers': [1, 2, 4],\n",
    "        }\n",
    "        train_search_space = {\n",
    "            'batch_size': [256, 512],\n",
    "            'base_lr': [0.0001, 0.001],\n",
    "            'gamma_rate': [0.9, 0.95, 1.],\n",
    "        }\n",
    "        model_cls = MLP\n",
    "        col_stats = train_dataset.col_stats\n",
    "    elif args.model_type == 'TabTransformer':\n",
    "        model_search_space = {\n",
    "            'channels': [16, 32, 64, 128],\n",
    "            'num_layers': [4, 6, 8],\n",
    "            'num_heads': [4, 8],\n",
    "            'encoder_pad_size': [2, 4],\n",
    "            'attn_dropout': [0, 0.2],\n",
    "            'ffn_dropout': [0, 0.2],\n",
    "        }\n",
    "        train_search_space = {\n",
    "            'batch_size': [128, 256],\n",
    "            'base_lr': [0.0001, 0.001],\n",
    "            'gamma_rate': [0.9, 0.95, 1.],\n",
    "        }\n",
    "        model_cls = TabTransformer\n",
    "        col_stats = train_dataset.col_stats\n",
    "    elif args.model_type == 'Trompt':\n",
    "        model_search_space = {\n",
    "            'channels': [64, 128, 192],\n",
    "            'num_layers': [4, 6, 8],\n",
    "            'num_prompts': [64, 128, 192],\n",
    "        }\n",
    "        train_search_space = {\n",
    "            'batch_size': [128, 256],\n",
    "            'base_lr': [0.01, 0.001],\n",
    "            'gamma_rate': [0.9, 0.95, 1.],\n",
    "        }\n",
    "        if train_tensor_frame.num_cols > 20:\n",
    "            # Reducing the model size to avoid GPU OOM\n",
    "            model_search_space['channels'] = [64, 128]\n",
    "            model_search_space['num_prompts'] = [64, 128]\n",
    "        elif train_tensor_frame.num_cols > 50:\n",
    "            model_search_space['channels'] = [64]\n",
    "            model_search_space['num_prompts'] = [64]\n",
    "        model_cls = Trompt\n",
    "        col_stats = train_dataset.col_stats\n",
    "    elif args.model_type == 'ExcelFormer':\n",
    "        from torch_frame.transforms import (\n",
    "            CatToNumTransform,\n",
    "            MutualInformationSort,\n",
    "        )\n",
    "\n",
    "        categorical_transform = CatToNumTransform()\n",
    "        categorical_transform.fit(train_dataset.tensor_frame,\n",
    "                                  train_dataset.col_stats)\n",
    "        train_tensor_frame = categorical_transform(train_tensor_frame)\n",
    "        val_tensor_frame = categorical_transform(val_tensor_frame)\n",
    "        test_tensor_frame = categorical_transform(test_tensor_frame)\n",
    "        col_stats = categorical_transform.transformed_stats\n",
    "\n",
    "        mutual_info_sort = MutualInformationSort(task_type=train_dataset.task_type)\n",
    "        mutual_info_sort.fit(train_tensor_frame, col_stats)\n",
    "        train_tensor_frame = mutual_info_sort(train_tensor_frame)\n",
    "        val_tensor_frame = mutual_info_sort(val_tensor_frame)\n",
    "        test_tensor_frame = mutual_info_sort(test_tensor_frame)\n",
    "\n",
    "        model_search_space = {\n",
    "            'in_channels': [128, 256],\n",
    "            'num_heads': [8, 16, 32],\n",
    "            'num_layers': [4, 6, 8],\n",
    "            'diam_dropout': [0, 0.2],\n",
    "            'residual_dropout': [0, 0.2],\n",
    "            'aium_dropout': [0, 0.2],\n",
    "            'mixup': [None, 'feature', 'hidden'],\n",
    "            'beta': [0.5],\n",
    "            'num_cols': [train_tensor_frame.num_cols],\n",
    "        }\n",
    "        train_search_space = {\n",
    "            'batch_size': [256, 512],\n",
    "            'base_lr': [0.001],\n",
    "            'gamma_rate': [0.9, 0.95, 1.],\n",
    "        }\n",
    "        model_cls = ExcelFormer\n",
    "\n",
    "    assert model_cls is not None\n",
    "    assert col_stats is not None\n",
    "    assert set(train_search_space.keys()) == set(TRAIN_CONFIG_KEYS)\n",
    "    col_names_dict = train_tensor_frame.col_names_dict\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    loss_accum = total_count = 0\n",
    "\n",
    "    for tf in tqdm(loader, desc=f'Epoch: {epoch}'):\n",
    "        tf = tf.to(device)\n",
    "        y = tf.y\n",
    "        if isinstance(model, ExcelFormer):\n",
    "            # Train with FEAT-MIX or HIDDEN-MIX\n",
    "            pred, y = model(tf, mixup_encoded=True)\n",
    "        elif isinstance(model, Trompt):\n",
    "            # Trompt uses the layer-wise loss\n",
    "            pred = model(tf)\n",
    "            num_layers = pred.size(1)\n",
    "            # [batch_size * num_layers, num_classes]\n",
    "            pred = pred.view(-1, out_channels)\n",
    "            y = tf.y.repeat_interleave(num_layers)\n",
    "        else:\n",
    "            pred = model(tf)\n",
    "\n",
    "        if pred.size(1) == 1:\n",
    "            pred = pred.view(-1, )\n",
    "        if train_dataset.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "            y = y.to(torch.float)\n",
    "        loss = loss_fun(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_accum += float(loss) * len(tf.y)\n",
    "        total_count += len(tf.y)\n",
    "        optimizer.step()\n",
    "    return loss_accum / total_count\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(\n",
    "    model: Module,\n",
    "    loader: DataLoader,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    metric_computer.reset()\n",
    "    for tf in loader:\n",
    "        tf = tf.to(device)\n",
    "        pred = model(tf)\n",
    "        if isinstance(model, Trompt):\n",
    "            pred = pred.mean(dim=1)\n",
    "        if train_dataset.task_type == TaskType.MULTICLASS_CLASSIFICATION:\n",
    "            pred = pred.argmax(dim=-1)\n",
    "        elif train_dataset.task_type == TaskType.REGRESSION:\n",
    "            pred = pred.view(-1, )\n",
    "        metric_computer.update(pred, tf.y)\n",
    "    return metric_computer.compute().item()\n",
    "\n",
    "\n",
    "def train_and_eval_with_cfg(\n",
    "    model_cfg: dict[str, Any],\n",
    "    train_cfg: dict[str, Any],\n",
    "    trial: Optional[optuna.trial.Trial] = None,\n",
    ") -> tuple[float, float]:\n",
    "    # Use model_cfg to set up training procedure\n",
    "    if args.model_type == 'FTTransformerBucket':\n",
    "        # Use LinearBucketEncoder instead\n",
    "        stype_encoder_dict = {\n",
    "            stype.categorical: EmbeddingEncoder(),\n",
    "            stype.numerical: LinearBucketEncoder(),\n",
    "        }\n",
    "        model_cfg['stype_encoder_dict'] = stype_encoder_dict\n",
    "    model = model_cls(\n",
    "        **model_cfg,\n",
    "        out_channels=out_channels,\n",
    "        col_stats=col_stats,\n",
    "        col_names_dict=col_names_dict,\n",
    "    ).to(device)\n",
    "    model.reset_parameters()\n",
    "    # Use train_cfg to set up training procedure\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=train_cfg['base_lr'])\n",
    "    lr_scheduler = ExponentialLR(optimizer, gamma=train_cfg['gamma_rate'])\n",
    "    train_loader = DataLoader(train_tensor_frame,\n",
    "                              batch_size=train_cfg['batch_size'], shuffle=True,\n",
    "                              drop_last=True)\n",
    "    val_loader = DataLoader(val_tensor_frame,\n",
    "                            batch_size=train_cfg['batch_size'])\n",
    "    test_loader = DataLoader(test_tensor_frame,\n",
    "                             batch_size=train_cfg['batch_size'])\n",
    "\n",
    "    if higher_is_better:\n",
    "        best_val_metric = 0\n",
    "    else:\n",
    "        best_val_metric = math.inf\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(model, train_loader, optimizer, epoch)\n",
    "        val_metric = test(model, val_loader)\n",
    "\n",
    "        if higher_is_better:\n",
    "            if val_metric > best_val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                best_test_metric = test(model, test_loader)\n",
    "        else:\n",
    "            if val_metric < best_val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                best_test_metric = test(model, test_loader)\n",
    "        lr_scheduler.step()\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val: {val_metric:.4f}')\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(val_metric, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "    print(\n",
    "        f'Best val: {best_val_metric:.4f}, Best test: {best_test_metric:.4f}')\n",
    "    return best_val_metric, best_test_metric\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    model_cfg = {}\n",
    "    for name, search_list in model_search_space.items():\n",
    "        model_cfg[name] = trial.suggest_categorical(name, search_list)\n",
    "    train_cfg = {}\n",
    "    for name, search_list in train_search_space.items():\n",
    "        train_cfg[name] = trial.suggest_categorical(name, search_list)\n",
    "\n",
    "    best_val_metric, _ = train_and_eval_with_cfg(model_cfg=model_cfg,\n",
    "                                                 train_cfg=train_cfg,\n",
    "                                                 trial=trial)\n",
    "    return best_val_metric\n",
    "\n",
    "\n",
    "def main_deep_models():\n",
    "    # Hyper-parameter optimization with Optuna\n",
    "    print(\"Hyper-parameter search via Optuna\")\n",
    "    start_time = time.time()\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(),\n",
    "        direction=\"maximize\" if higher_is_better else \"minimize\",\n",
    "    )\n",
    "    study.optimize(objective, n_trials=args.num_trials)\n",
    "    end_time = time.time()\n",
    "    search_time = end_time - start_time\n",
    "    print(\"Hyper-parameter search done. Found the best config.\")\n",
    "    params = study.best_params\n",
    "    best_train_cfg = {}\n",
    "    for train_cfg_key in TRAIN_CONFIG_KEYS:\n",
    "        best_train_cfg[train_cfg_key] = params.pop(train_cfg_key)\n",
    "    best_model_cfg = params\n",
    "\n",
    "    print(f\"Repeat experiments {args.num_repeats} times with the best train \"\n",
    "          f\"config {best_train_cfg} and model config {best_model_cfg}.\")\n",
    "    start_time = time.time()\n",
    "    best_val_metrics = []\n",
    "    best_test_metrics = []\n",
    "    for _ in range(args.num_repeats):\n",
    "        best_val_metric, best_test_metric = train_and_eval_with_cfg(\n",
    "            best_model_cfg, best_train_cfg)\n",
    "        best_val_metrics.append(best_val_metric)\n",
    "        best_test_metrics.append(best_test_metric)\n",
    "    end_time = time.time()\n",
    "    final_model_time = (end_time - start_time) / args.num_repeats\n",
    "    best_val_metrics = np.array(best_val_metrics)\n",
    "    best_test_metrics = np.array(best_test_metrics)\n",
    "\n",
    "    result_dict = {\n",
    "        'args': args.__dict__,\n",
    "        'best_val_metrics': best_val_metrics,\n",
    "        'best_test_metrics': best_test_metrics,\n",
    "        'best_val_metric': best_val_metrics.mean(),\n",
    "        'best_test_metric': best_test_metrics.mean(),\n",
    "        'best_train_cfg': best_train_cfg,\n",
    "        'best_model_cfg': best_model_cfg,\n",
    "        'search_time': search_time,\n",
    "        'final_model_time': final_model_time,\n",
    "        'total_time': search_time + final_model_time,\n",
    "    }\n",
    "    print(result_dict)\n",
    "    # Save results\n",
    "    if args.result_path != '':\n",
    "        os.makedirs(os.path.dirname(args.result_path), exist_ok=True)\n",
    "        torch.save(result_dict, args.result_path)\n",
    "\n",
    "\n",
    "def main_gbdt():\n",
    "    if train_dataset.task_type.is_classification:\n",
    "        num_classes = train_dataset.num_classes\n",
    "    else:\n",
    "        num_classes = None\n",
    "    model = model_cls(task_type=train_dataset.task_type, num_classes=num_classes)\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    model.tune(tf_train=train_dataset.tensor_frame,\n",
    "               tf_val=val_dataset.tensor_frame, num_trials=args.num_trials)\n",
    "    val_pred = model.predict(tf_test=val_dataset.tensor_frame)\n",
    "    val_metric = model.compute_metric(val_dataset.tensor_frame.y, val_pred)\n",
    "    test_pred = model.predict(tf_test=test_dataset.tensor_frame)\n",
    "    test_metric = model.compute_metric(test_dataset.tensor_frame.y, test_pred)\n",
    "    end_time = time.time()\n",
    "    result_dict = {\n",
    "        'args': args.__dict__,\n",
    "        'best_val_metric': val_metric,\n",
    "        'best_test_metric': test_metric,\n",
    "        'best_cfg': model.params,\n",
    "        'total_time': end_time - start_time,\n",
    "    }\n",
    "    print(result_dict)\n",
    "    # Save results\n",
    "    if args.result_path != '':\n",
    "        os.makedirs(os.path.dirname(args.result_path), exist_ok=True)\n",
    "        torch.save(result_dict, args.result_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(args)\n",
    "    if os.path.exists(args.result_path):\n",
    "        exit(-1)\n",
    "    if args.model_type in [\"XGBoost\", \"CatBoost\", \"LightGBM\"]:\n",
    "        main_gbdt()\n",
    "    else:\n",
    "        main_deep_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ExampleTransformer(\n",
    "    channels=32,\n",
    "    out_channels=train_dataset.num_classes,\n",
    "    num_layers=2,\n",
    "    num_heads=8,\n",
    "    col_stats=train_dataset.col_stats,\n",
    "    col_names_dict=train_dataset.tensor_frame.col_names_dict,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for tf in train_loader:\n",
    "        tf = tf.to(device)\n",
    "        pred = model.forward(tf)\n",
    "        loss = F.cross_entropy(pred, tf.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
